<!DOCTYPE html>
<html style="display: none;" lang="zh">
    <head>
    <meta charset="utf-8">
    <!--
        © Material Theme
        https://github.com/viosey/hexo-theme-material
        Version: 1.5.0 -->
    <script>
        window.materialVersion = "1.5.0"
        // Delete localstorage with these tags
        window.oldVersion = [
            'codestartv1',
            '1.3.4',
            '1.4.0',
            '1.4.0b1'
        ]
    </script>

    <!-- dns prefetch -->
    <meta http-equiv="x-dns-prefetch-control" content="on">














    <!-- Title -->
    
    <title>
        
            TensorFlow的使用 | 
        
        Azurery
    </title>

    <!-- Meta & Info -->
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="format-detection" content="telephone=no"/>
    <meta name="theme-color" content="#0097A7">
    <meta name="author" content="Magicmanoooo">
    <meta name="description" itemprop="description" content="蒟蒻一枚">
    <meta name="keywords" content=",ML">

    <!-- Import lsloader -->
    <script>(function(){window.lsloader={jsRunSequence:[],jsnamemap:{},cssnamemap:{}};lsloader.removeLS=function(key){try{localStorage.removeItem(key)}catch(e){}};lsloader.setLS=function(key,val){try{localStorage.setItem(key,val)}catch(e){}};lsloader.getLS=function(key){var val="";try{val=localStorage.getItem(key)}catch(e){val=""}return val};versionString="/*"+(window.materialVersion||"unknownVersion")+"*/";lsloader.clean=function(){try{var keys=[];for(var i=0;i<localStorage.length;i++){keys.push(localStorage.key(i))}keys.forEach(function(key){var data=lsloader.getLS(key);if(window.oldVersion){var remove=window.oldVersion.reduce(function(p,c){return p||data.indexOf("/*"+c+"*/")!==-1},false);if(remove){lsloader.removeLS(key)}}})}catch(e){}};lsloader.clean();lsloader.load=function(jsname,jspath,cssonload,isJs){if(typeof cssonload==="boolean"){isJs=cssonload;cssonload=undefined}isJs=isJs||false;cssonload=cssonload||function(){};var code;code=this.getLS(jsname);if(code&&code.indexOf(versionString)===-1){this.removeLS(jsname);this.requestResource(jsname,jspath,cssonload,isJs);return}if(code){var versionNumber=code.split(versionString)[0];if(versionNumber!=jspath){console.log("reload:"+jspath);this.removeLS(jsname);this.requestResource(jsname,jspath,cssonload,isJs);return}code=code.split(versionString)[1];if(isJs){this.jsRunSequence.push({name:jsname,code:code});this.runjs(jspath,jsname,code)}else{document.getElementById(jsname).appendChild(document.createTextNode(code));cssonload()}}else{this.requestResource(jsname,jspath,cssonload,isJs)}};lsloader.requestResource=function(name,path,cssonload,isJs){var that=this;if(isJs){this.iojs(path,name,function(path,name,code){that.setLS(name,path+versionString+code);that.runjs(path,name,code)})}else{this.iocss(path,name,function(code){document.getElementById(name).appendChild(document.createTextNode(code));that.setLS(name,path+versionString+code)},cssonload)}};lsloader.iojs=function(path,jsname,callback){var that=this;that.jsRunSequence.push({name:jsname,code:""});try{var xhr=new XMLHttpRequest;xhr.open("get",path,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){callback(path,jsname,xhr.response);return}}that.jsfallback(path,jsname)}};xhr.send(null)}catch(e){that.jsfallback(path,jsname)}};lsloader.iocss=function(path,jsname,callback,cssonload){var that=this;try{var xhr=new XMLHttpRequest;xhr.open("get",path,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){callback(xhr.response);cssonload();return}}that.cssfallback(path,jsname,cssonload)}};xhr.send(null)}catch(e){that.cssfallback(path,jsname,cssonload)}};lsloader.iofonts=function(path,jsname,callback,cssonload){var that=this;try{var xhr=new XMLHttpRequest;xhr.open("get",path,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){callback(xhr.response);cssonload();return}}that.cssfallback(path,jsname,cssonload)}};xhr.send(null)}catch(e){that.cssfallback(path,jsname,cssonload)}};lsloader.runjs=function(path,name,code){if(!!name&&!!code){for(var k in this.jsRunSequence){if(this.jsRunSequence[k].name==name){this.jsRunSequence[k].code=code}}}if(!!this.jsRunSequence[0]&&!!this.jsRunSequence[0].code&&this.jsRunSequence[0].status!="failed"){var script=document.createElement("script");script.appendChild(document.createTextNode(this.jsRunSequence[0].code));script.type="text/javascript";document.getElementsByTagName("head")[0].appendChild(script);this.jsRunSequence.shift();if(this.jsRunSequence.length>0){this.runjs()}}else if(!!this.jsRunSequence[0]&&this.jsRunSequence[0].status=="failed"){var that=this;var script=document.createElement("script");script.src=this.jsRunSequence[0].path;script.type="text/javascript";this.jsRunSequence[0].status="loading";script.onload=function(){that.jsRunSequence.shift();if(that.jsRunSequence.length>0){that.runjs()}};document.body.appendChild(script)}};lsloader.tagLoad=function(path,name){this.jsRunSequence.push({name:name,code:"",path:path,status:"failed"});this.runjs()};lsloader.jsfallback=function(path,name){if(!!this.jsnamemap[name]){return}else{this.jsnamemap[name]=name}for(var k in this.jsRunSequence){if(this.jsRunSequence[k].name==name){this.jsRunSequence[k].code="";this.jsRunSequence[k].status="failed";this.jsRunSequence[k].path=path}}this.runjs()};lsloader.cssfallback=function(path,name,cssonload){if(!!this.cssnamemap[name]){return}else{this.cssnamemap[name]=1}var link=document.createElement("link");link.type="text/css";link.href=path;link.rel="stylesheet";link.onload=link.onerror=cssonload;var root=document.getElementsByTagName("script")[0];root.parentNode.insertBefore(link,root)};lsloader.runInlineScript=function(scriptId,codeId){var code=document.getElementById(codeId).innerText;this.jsRunSequence.push({name:scriptId,code:code});this.runjs()};lsloader.loadCombo=function(jslist){var updateList="";var requestingModules={};for(var k in jslist){var LS=this.getLS(jslist[k].name);if(!!LS){var version=LS.split(versionString)[0];var code=LS.split(versionString)[1]}else{var version=""}if(version==jslist[k].path){this.jsRunSequence.push({name:jslist[k].name,code:code,path:jslist[k].path})}else{this.jsRunSequence.push({name:jslist[k].name,code:null,path:jslist[k].path,status:"comboloading"});requestingModules[jslist[k].name]=true;updateList+=(updateList==""?"":";")+jslist[k].path}}var that=this;if(!!updateList){var xhr=new XMLHttpRequest;xhr.open("get",combo+updateList,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){that.runCombo(xhr.response,requestingModules);return}}else{for(var i in that.jsRunSequence){if(requestingModules[that.jsRunSequence[i].name]){that.jsRunSequence[i].status="failed"}}that.runjs()}}};xhr.send(null)}this.runjs()};lsloader.runCombo=function(comboCode,requestingModules){comboCode=comboCode.split("/*combojs*/");comboCode.shift();for(var k in this.jsRunSequence){if(!!requestingModules[this.jsRunSequence[k].name]&&!!comboCode[0]){this.jsRunSequence[k].status="comboJS";this.jsRunSequence[k].code=comboCode[0];this.setLS(this.jsRunSequence[k].name,this.jsRunSequence[k].path+versionString+comboCode[0]);comboCode.shift()}}this.runjs()}})();</script>

    <!-- Import queue -->
    <script>function Queue(){this.dataStore=[];this.offer=b;this.poll=d;this.execNext=a;this.debug=false;this.startDebug=c;function b(e){if(this.debug){console.log("Offered a Queued Function.")}if(typeof e==="function"){this.dataStore.push(e)}else{console.log("You must offer a function.")}}function d(){if(this.debug){console.log("Polled a Queued Function.")}return this.dataStore.shift()}function a(){var e=this.poll();if(e!==undefined){if(this.debug){console.log("Run a Queued Function.")}e()}}function c(){this.debug=true}}var queue=new Queue();</script>

    <!-- Favicons -->
    <link rel="icon shortcut" type="image/ico" href="/img/favicon.png">
    <link rel="icon" sizes="192x192" href="/img/favicon.png">
    <link rel="apple-touch-icon" href="/img/favicon.png">

    <!--iOS -->
    <meta name="apple-mobile-web-app-title" content="Title">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="480">

    <!-- Add to homescreen for Chrome on Android -->
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Add to homescreen for Safari on iOS -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="Azurery">

    <!-- Site Verification -->
    
    

    <!-- RSS -->
    

    <!--[if lte IE 9]>
        <link rel="stylesheet" href="/css/ie-blocker.css">

        
            <script src="/js/ie-blocker.zhCN.js"></script>
        
    <![endif]-->

    <!-- Import CSS -->
    
        <style id="material_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("material_css","/css/material.min.css?Z7a72R1E4SxzBKR/WGctOA==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>
        <style id="style_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("style_css","/css/style.min.css?MKetZV3cUTfDxvMffaOezg==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>

        
            
                <style id="prettify_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("prettify_css","/css/prettify.min.css?zp8STOU9v89XWFEnN+6YmQ==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>
                <style id="prettify_theme"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("prettify_theme","/css/prettify/vibrant-ink.min.css?e5E/qqGcGveS7VTH4M896w==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>
            
        

    

    

    <!-- Config CSS -->

<!-- Other Styles -->
<style>
  body, html {
    font-family: Roboto, "Helvetica Neue", Helvetica, "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "微软雅黑", Arial, sans-serif;
    overflow-x: hidden !important;
  }
  
  code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  a {
    color: #00838F;
  }

  .mdl-card__media,
  #search-label,
  #search-form-label:after,
  #scheme-Paradox .hot_tags-count,
  #scheme-Paradox .sidebar_archives-count,
  #scheme-Paradox .sidebar-colored .sidebar-header,
  #scheme-Paradox .sidebar-colored .sidebar-badge{
    background-color: #0097A7 !important;
  }

  /* Sidebar User Drop Down Menu Text Color */
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:hover,
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:focus {
    color: #0097A7 !important;
  }

  #post_entry-right-info,
  .sidebar-colored .sidebar-nav li:hover > a,
  .sidebar-colored .sidebar-nav li:hover > a i,
  .sidebar-colored .sidebar-nav li > a:hover,
  .sidebar-colored .sidebar-nav li > a:hover i,
  .sidebar-colored .sidebar-nav li > a:focus i,
  .sidebar-colored .sidebar-nav > .open > a,
  .sidebar-colored .sidebar-nav > .open > a:hover,
  .sidebar-colored .sidebar-nav > .open > a:focus,
  #ds-reset #ds-ctx .ds-ctx-entry .ds-ctx-head a {
    color: #0097A7 !important;
  }

  .toTop {
    background: #757575 !important;
  }

  .material-layout .material-post>.material-nav,
  .material-layout .material-index>.material-nav,
  .material-nav a {
    color: #757575;
  }

  #scheme-Paradox .MD-burger-layer {
    background-color: #757575;
  }

  #scheme-Paradox #post-toc-trigger-btn {
    color: #757575;
  }

  .post-toc a:hover {
    color: #00838F;
    text-decoration: underline;
  }

</style>


<!-- Theme Background Related-->

    <style>
      body{
        background-image: url(/img/bg.png);
      }
    </style>




<!-- Fade Effect -->

    <style>
      .fade {
        transition: all 800ms linear;
        -webkit-transform: translate3d(0,0,0);
        -moz-transform: translate3d(0,0,0);
        -ms-transform: translate3d(0,0,0);
        -o-transform: translate3d(0,0,0);
        transform: translate3d(0,0,0);
        opacity: 1;
      }

      .fade.out{
        opacity: 0;
      }
    </style>


<!-- Import Font -->
<!-- Import Roboto -->

    <link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet">


<!-- Import Material Icon -->

    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">




    <!-- Import jQuery -->
    
        <script>lsloader.load("jq_js","/js/jquery.min.js?qcusAULNeBksqffqUM2+Ig==", true)</script>
    

    <!-- The Open Graph protocol -->
    <meta property="og:url" content="http://yoursite.com">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="TensorFlow的使用 | Azurery">
    <meta property="og:image" content="http://yoursite.com/img/favicon.png" />
    <meta property="og:description" content="蒟蒻一枚">
    <meta property="og:article:tag" content="ML"> 

    
        <meta property="article:published_time" content="Tue Dec 11 2018 19:06:59 GMT+0800" />
        <meta property="article:modified_time" content="Thu Jan 17 2019 18:24:25 GMT+0800" />
    

    <!-- The Twitter Card protocol -->
    <meta name="twitter:title" content="TensorFlow的使用 | Azurery">
    <meta name="twitter:description" content="蒟蒻一枚">
    <meta name="twitter:image" content="http://yoursite.com/img/favicon.png">
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:url" content="http://yoursite.com" />

    <!-- Add canonical link for SEO -->
    
        <link rel="canonical" href="http://yoursite.com/2018/12/11/TensorFlow的使用/index.html" />
    

    <!-- Structured-data for SEO -->
    
        


<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "mainEntityOfPage": "http://yoursite.com/2018/12/11/TensorFlow的使用/index.html",
    "headline": "TensorFlow的使用",
    "datePublished": "Tue Dec 11 2018 19:06:59 GMT+0800",
    "dateModified": "Thu Jan 17 2019 18:24:25 GMT+0800",
    "author": {
        "@type": "Person",
        "name": "Magicmanoooo",
        "image": {
            "@type": "ImageObject",
            "url": "/img/avatar.png"
        },
        "description": "秘境，探寻你的足迹"
    },
    "publisher": {
        "@type": "Organization",
        "name": "Azurery",
        "logo": {
            "@type":"ImageObject",
            "url": "/img/favicon.png"
        }
    },
    "keywords": ",ML",
    "description": "蒟蒻一枚",
}
</script>


    

    <!-- Analytics -->
    
    
    

    <!-- Custom Head -->
    

</head>


    
        <body id="scheme-Paradox" class="lazy">
            <div class="material-layout  mdl-js-layout has-drawer is-upgraded">
                

                <!-- Main Container -->
                <main class="material-layout__content" id="main">

                    <!-- Top Anchor -->
                    <div id="top"></div>

                    
                        <!-- Hamburger Button -->
                        <button class="MD-burger-icon sidebar-toggle">
                            <span class="MD-burger-layer"></span>
                        </button>
                    

                    <!-- Post TOC -->

    
    <!-- Back Button -->
    <!--
    <div class="material-back" id="backhome-div" tabindex="0">
        <a class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon"
           href="#" onclick="window.history.back();return false;"
           target="_self"
           role="button"
           data-upgraded=",MaterialButton,MaterialRipple">
            <i class="material-icons" role="presentation">arrow_back</i>
            <span class="mdl-button__ripple-container">
                <span class="mdl-ripple"></span>
            </span>
        </a>
    </div>
    -->


    <!-- Left aligned menu below button -->
    
    
    <button id="post-toc-trigger-btn"
        class="mdl-button mdl-js-button mdl-button--icon">
        <i class="material-icons">format_list_numbered</i>
    </button>

    <ul class="post-toc-wrap mdl-menu mdl-menu--bottom-left mdl-js-menu mdl-js-ripple-effect" for="post-toc-trigger-btn" style="max-height:80vh; overflow-y:scroll;">
        <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-get-variable"><span class="post-toc-number">1.</span> <span class="post-toc-text">tf.get_variable</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#TensorFlow-中的变量初始化函数："><span class="post-toc-number">1.0.1.</span> <span class="post-toc-text">TensorFlow 中的变量初始化函数：</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-Session-run"><span class="post-toc-number">2.</span> <span class="post-toc-text">tf.Session().run()</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-truncated-normal-shape-mean-stddev"><span class="post-toc-number">3.</span> <span class="post-toc-text">tf.truncated_normal(shape, mean, stddev)</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-name-scope-amp-tf-variable-scope"><span class="post-toc-number">4.</span> <span class="post-toc-text">tf.name_scope() &amp; tf.variable_scope()</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-tf-name-scope对tf-get-variable-的影响"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">1. tf.name_scope对tf.get_variable()的影响</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-nn-dropout"><span class="post-toc-number">5.</span> <span class="post-toc-text">tf.nn.dropout()</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-nn-conv2d"><span class="post-toc-number">6.</span> <span class="post-toc-text">tf.nn.conv2d</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-nn-separable-conv2d"><span class="post-toc-number">7.</span> <span class="post-toc-text">tf.nn.separable_conv2d</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#常规卷积操作"><span class="post-toc-number">7.1.</span> <span class="post-toc-text">常规卷积操作</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Depthwise-Convolution"><span class="post-toc-number">7.2.</span> <span class="post-toc-text">Depthwise Convolution</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Pointwise-Convolution"><span class="post-toc-number">7.3.</span> <span class="post-toc-text">Pointwise Convolution</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-nn-avg-pool"><span class="post-toc-number">8.</span> <span class="post-toc-text">tf.nn.avg_pool</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-initialize-all-variables-vs-tf-initialize-variables"><span class="post-toc-number">9.</span> <span class="post-toc-text">tf.initialize_all_variables() vs tf.initialize_variables</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-变量初始化"><span class="post-toc-number">9.1.</span> <span class="post-toc-text">1. 变量初始化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-识别未被初始化的变量"><span class="post-toc-number">9.2.</span> <span class="post-toc-text">2. 识别未被初始化的变量</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-变量的更新"><span class="post-toc-number">9.3.</span> <span class="post-toc-text">3. 变量的更新</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tensorflow-tf-python-io"><span class="post-toc-number">10.</span> <span class="post-toc-text">tensorflow tf.python_io</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#源码解析"><span class="post-toc-number">10.1.</span> <span class="post-toc-text">源码解析</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#数据读取的三种方式"><span class="post-toc-number">11.</span> <span class="post-toc-text">数据读取的三种方式</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-Preload-data-constant-预加载数据"><span class="post-toc-number">11.1.</span> <span class="post-toc-text">1. Preload data: constant 预加载数据</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-Feeding-placeholder，feed-dict"><span class="post-toc-number">11.2.</span> <span class="post-toc-text">2. Feeding: placeholder，feed_dict</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-Reading-From-File：直接从文件中读取"><span class="post-toc-number">11.3.</span> <span class="post-toc-text">3. Reading From File：直接从文件中读取</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#四种类型数据的读取流程"><span class="post-toc-number">12.</span> <span class="post-toc-text">四种类型数据的读取流程</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-文件队列构造"><span class="post-toc-number">12.1.</span> <span class="post-toc-text">1. 文件队列构造</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-文件阅读器"><span class="post-toc-number">12.2.</span> <span class="post-toc-text">2. 文件阅读器</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-文件内容解码器"><span class="post-toc-number">12.3.</span> <span class="post-toc-text">3. 文件内容解码器</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（1）CSV文件"><span class="post-toc-number">12.3.1.</span> <span class="post-toc-text">（1）CSV文件</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（2）二进制文件"><span class="post-toc-number">12.3.2.</span> <span class="post-toc-text">（2）二进制文件</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（3）图像文件"><span class="post-toc-number">12.3.3.</span> <span class="post-toc-text">（3）图像文件</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（4）TFRecords文件"><span class="post-toc-number">12.3.4.</span> <span class="post-toc-text">（4）TFRecords文件</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-批处理数据"><span class="post-toc-number">12.4.</span> <span class="post-toc-text">4. 批处理数据</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#TFRecords文件的存储与读取"><span class="post-toc-number">13.</span> <span class="post-toc-text">TFRecords文件的存储与读取</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#一-TFRecords存储"><span class="post-toc-number">13.1.</span> <span class="post-toc-text">一. TFRecords存储</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-建立TFRecord存储器"><span class="post-toc-number">13.1.1.</span> <span class="post-toc-text">1. 建立TFRecord存储器</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-构造每个样本的Example协议块"><span class="post-toc-number">13.1.2.</span> <span class="post-toc-text">2. 构造每个样本的Example协议块</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#（1）-tf-train-Example-features-None"><span class="post-toc-number">13.1.2.1.</span> <span class="post-toc-text">（1） tf.train.Example(features = None)</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#（2）-tf-train-Features-feature-None"><span class="post-toc-number">13.1.2.2.</span> <span class="post-toc-text">（2） tf.train.Features(feature = None)</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#（3）-tf-train-Feature-options"><span class="post-toc-number">13.1.2.3.</span> <span class="post-toc-text">（3） tf.train.Feature(**options)</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#二-TFRecords读取方法"><span class="post-toc-number">13.2.</span> <span class="post-toc-text">二. TFRecords读取方法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#解析TFRecords的Example协议内存块的步骤："><span class="post-toc-number">13.2.1.</span> <span class="post-toc-text">解析TFRecords的Example协议内存块的步骤：</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1-tf-parse-single-example-serialized-features-None-name-None"><span class="post-toc-number">13.2.1.1.</span> <span class="post-toc-text">1. tf.parse_single_example(serialized,features=None,name= None</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-tf-FixedLenFeature-shape-dtype"><span class="post-toc-number">13.2.1.2.</span> <span class="post-toc-text">2. tf.FixedLenFeature(shape,dtype)</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#总结（读取TFRecord文件的步骤）："><span class="post-toc-number">13.2.2.</span> <span class="post-toc-text">总结（读取TFRecord文件的步骤）：</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-cast"><span class="post-toc-number">14.</span> <span class="post-toc-text">tf.cast()</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-app-flags"><span class="post-toc-number">15.</span> <span class="post-toc-text">tf.app.flags()</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-app-run"><span class="post-toc-number">16.</span> <span class="post-toc-text">tf.app.run()</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Basis"><span class="post-toc-number">17.</span> <span class="post-toc-text">Basis</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#get-shape"><span class="post-toc-number">17.1.</span> <span class="post-toc-text">get_shape()</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#TensorFlow-随机数生成函数"><span class="post-toc-number">17.2.</span> <span class="post-toc-text">TensorFlow 随机数生成函数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#常数生成函数"><span class="post-toc-number">17.3.</span> <span class="post-toc-text">常数生成函数</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-ConfigProto-allow-soft-placement-True-log-device-placement-True"><span class="post-toc-number">18.</span> <span class="post-toc-text">tf.ConfigProto(allow soft_placement=True,log_device_placement=True)</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-clip-by-value"><span class="post-toc-number">19.</span> <span class="post-toc-text">tf.clip_by_value</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#反向传播算法（backpropagation）"><span class="post-toc-number">20.</span> <span class="post-toc-text">反向传播算法（backpropagation）</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-train-exponential-decay"><span class="post-toc-number">21.</span> <span class="post-toc-text">tf.train.exponential_decay</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#保存模型和加载模型"><span class="post-toc-number">22.</span> <span class="post-toc-text">保存模型和加载模型</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tf-gradients-与stop-gradient"><span class="post-toc-number">23.</span> <span class="post-toc-text">tf.gradients()与stop_gradient()</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#timeline模块"><span class="post-toc-number">24.</span> <span class="post-toc-text">timeline模块</span></a></li></ol>
    </ul>
    




<!-- Layouts -->

    <!-- Post Module -->
    <div class="material-post_container">

        <div class="material-post mdl-grid">
            <div class="mdl-card mdl-shadow--4dp mdl-cell mdl-cell--12-col">

                <!-- Post Header(Thumbnail & Title) -->
                
    <!-- Paradox Post Header -->
    
        
            <!-- Random Thumbnail -->
            <div class="post_thumbnail-random mdl-card__media mdl-color-text--grey-50">
            <script type="text/ls-javascript" id="post-thumbnail-script">
    var randomNum = Math.floor(Math.random() * 19 + 1);

    $('.post_thumbnail-random').attr('data-original', '/img/random/material-' + randomNum + '.png');
    $('.post_thumbnail-random').addClass('lazy');
</script>

        
    
            <p class="article-headline-p">
                TensorFlow的使用
            </p>
        </div>





                
                    <!-- Paradox Post Info -->
                    <div class="mdl-color-text--grey-700 mdl-card__supporting-text meta">

    <!-- Author Avatar -->
    <div id="author-avatar">
        <img src="/img/avatar.png" width="44px" height="44px" alt="Author Avatar"/>
    </div>
    <!-- Author Name & Date -->
    <div>
        <strong>Magicmanoooo</strong>
        <span>12月 11, 2018</span>
    </div>

    <div class="section-spacer"></div>

    <!-- Favorite -->
    <!--
        <button id="article-functions-like-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon btn-like">
            <i class="material-icons" role="presentation">favorite</i>
            <span class="visuallyhidden">favorites</span>
        </button>
    -->

    <!-- Qrcode -->
    
        <button id="article-functions-qrcode-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
    <i class="material-icons" role="presentation">devices other</i>
    <span class="visuallyhidden">devices other</span>
</button>
<ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-functions-qrcode-button">
    <li class="mdl-menu__item">在其它设备中阅读本文章</li>
    
        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACV0lEQVR42u3aQXLDIAwFUN//0u0BWif6ksFO5rHKZLDheYE0QsfPV48DDw8PDw8PDw/vYbyjPNKn/s4/+z1Z5Z+n8PDw8Lbw3hy14ZzKJv7OnK+Lh4eHt593tqHK79LCLzG9EIKHh4f3KbzXSXP6Ic7ejIeHh/d9vKtS7XoJAw8PD+9pvF6aW0+g69Tbai14eHh4ZV79yN7/e+v9Hh4eHl6BF7c0lT9BKf0tHP2jpis8PDy8Bbw0za0n3L3U+YL94OHh4S3m9Yqw84uuyVpBGRcPDw9vAS89yuvtp+nMXgk4iHt4eHh4l/LSwzq9Bqu84bIPh4eHh7eYVzmO68vXkZM9vEnN8fDw8Dby0sDQK1X0SrpBwQIPDw9vMW9+fZWWGOYtCEFgwMPDw9vC67VJpaFiXiYedUbg4eHhLeClyXSvBFwJUWl4wMPDw1vNS4NEmhb3mq7SPeDh4eHt56Ww3v+TMNBsusLDw8NbwEuv6nulit7VWpqU4+Hh4d3Lmx/i9eJFumJ8AYaHh4e3gFfZVuWgrxcUKu9JC754eHh49/LqBda0eJG2HcQXb3h4eHgbeSmp94Zrk3U8PDy8e3nXFh3SOWk7wun/eHh4eIt56Zgk071nL+iMwMPDw7uUd5THpH1qEnjw8PDwnslLG6R6y49KDOX5eHh4eDt59QN6XmJIr9aC9B0PDw/vkbxKCEmLF2nYwMPDw/tEXpoE11uv0jYFPDw8vLt46XGflm4rhYl6Eo+Hh4d3L69XAphcbr1un5oECTw8PLzVvO8beHh4eHh4eHh4Dxi/L+q5NakVqEAAAAAASUVORK5CYII=">
    
</ul>

    

    <!-- Tags (bookmark) -->
    
    <button id="article-functions-viewtags-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
        <i class="material-icons" role="presentation">bookmark</i>
        <span class="visuallyhidden">bookmark</span>
    </button>
    <ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-functions-viewtags-button">
        <li class="mdl-menu__item">
        <a class="post_tag-link" href="/tags/ML/">ML</a>
    </ul>
    

    <!-- Share -->
    <button id="article-fuctions-share-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
    <i class="material-icons" role="presentation">share</i>
    <span class="visuallyhidden">share</span>
</button>
<ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-fuctions-share-button">
    

    

    <!-- Share Weibo -->
    
        <a class="post_share-link" href="http://service.weibo.com/share/share.php?appkey=&title=TensorFlow的使用&url=http://yoursite.com/2018/12/11/TensorFlow的使用/index.html&pic=http://yoursite.com/img/favicon.png&searchPic=false&style=simple" target="_blank">
            <li class="mdl-menu__item">
                分享到微博
            </li>
        </a>
    

    <!-- Share Twitter -->
    
        <a class="post_share-link" href="https://twitter.com/intent/tweet?text=TensorFlow的使用&url=http://yoursite.com/2018/12/11/TensorFlow的使用/index.html&via=Magicmanoooo" target="_blank">
            <li class="mdl-menu__item">
                分享到 Twitter
            </li>
        </a>
    

    <!-- Share Facebook -->
    
        <a class="post_share-link" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2018/12/11/TensorFlow的使用/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 Facebook
            </li>
        </a>
    

    <!-- Share Google+ -->
    
        <a class="post_share-link" href="https://plus.google.com/share?url=http://yoursite.com/2018/12/11/TensorFlow的使用/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 Google+
            </li>
        </a>
    

    <!-- Share LinkedIn -->
    

    <!-- Share QQ -->
    
        <a class="post_share-link" href="http://connect.qq.com/widget/shareqq/index.html?site=Azurery&title=TensorFlow的使用&summary=蒟蒻一枚&pics=http://yoursite.com/img/favicon.png&url=http://yoursite.com/2018/12/11/TensorFlow的使用/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 QQ
            </li>
        </a>
    

    <!-- Share Telegram -->
    
</ul>

</div>

                

                <!-- Post Content -->
                <div id="post-content" class="mdl-color-text--grey-700 mdl-card__supporting-text fade out">
    
        <h1 id="tf-get-variable"><a href="#tf-get-variable" class="headerlink" title="tf.get_variable"></a><code>tf.get_variable</code></h1><p>当 <code>tf.get_variable</code> 用于创建变量时，它和 <code>tf.Variable</code> 的功能是基本等价的 。</p>
<p>例子：</p>
<pre><code class="python">＃ 下面这两个定义是等价的 。
v = t f .get variable (”v”, shape=[l] ,
initializer=tf . constant initializer(l.0))
v = tf.Variable(tf.constant(l.0 , shape=[l)), name=” v”)
</code></pre>
<h3 id="TensorFlow-中的变量初始化函数："><a href="#TensorFlow-中的变量初始化函数：" class="headerlink" title="TensorFlow 中的变量初始化函数："></a>TensorFlow 中的变量初始化函数：</h3><p><img src="https://upload-images.jianshu.io/upload_images/1351548-79772fe4a45b29f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><code>tf.get_variable</code> 函数与 <code>tf.Variable</code> 函数最大的区别在于指定变量名称的参数。对于 <code>tf.Variable</code> 函数，变量名称是一个可选的参数，通过 <code>name＝v”</code>的形式给出。但是对于 <code>tf.get_variable</code> 函数，变量名称是一个必填的参数。<code>tf.get_variable</code> 会根据这个名字去创建或者获取变量。 </p>
<p>如果需要通过 <code>tf.get_variable</code> 获取一个已经创建的变量，需要通过 <code>tf.variable_scope</code> 函数来生成一个上下文管理器，并明确指定在这个上下文管理器中， <code>tf.get_variable</code> 将直接获取己经生成的变量。</p>
<h1 id="tf-Session-run"><a href="#tf-Session-run" class="headerlink" title="tf.Session().run()"></a><code>tf.Session().run()</code></h1><pre><code class="python">run(fetches, feed_dict=None, options=None, run_metadata=None)
</code></pre>
<p><code>Session</code>实例中的方法对<code>fetches</code>中的张量进行评估和计算。</p>
<p>该方法进行Tensorflow计算的第一个步骤，是将<code>feed_dict</code>中的值替换为相应的输入值，通过运行必要的图形片段（necessary graph fragment）来执行每一个<code>Operation</code>，并计算<code>fetches</code>中的每一个张量。</p>
<ul>
<li><code>fetches</code>：可以是单个图元素（single graph element），也可以是任意嵌套的<code>list</code>，<code>tuple</code>，<code>namedtuple</code>，<code>dict</code>或包含图元素的<code>OrderedDict</code>。</li>
<li><code>feed_dict</code>：允许调用者覆盖图中张量的值，其中的每个<code>key</code>可以为：<ul>
<li><code>tf.Tensor</code>：则值<code>value</code>可能是python中的<code>scalar</code>、<code>string</code>、<code>list</code>、<code>numpy ndarray</code>，可以将其转化为<code>dtype</code>相同的张量</li>
<li><code>tf.placeholder</code>：则将检查值的形状是否与占位符兼容</li>
<li><code>tf.SparseTensor</code></li>
<li><code>tf.SparseTensorValue</code></li>
</ul>
</li>
<li><code>options</code>：需要一个<code>RunOptions</code>原型，允许控制该特定步骤的行为（例如，打开跟踪）。</li>
<li><code>run_metadata</code>参数需要一个<code>RunMetadata</code>原型。 适当时，将在那里收集此步骤的非Tensor输出。 例如，当用户在<code>options</code>中打开跟踪时，配置信息将被收集到此参数中并传回。</li>
</ul>
<h1 id="tf-truncated-normal-shape-mean-stddev"><a href="#tf-truncated-normal-shape-mean-stddev" class="headerlink" title="tf.truncated_normal(shape, mean, stddev)"></a><code>tf.truncated_normal(shape, mean, stddev)</code></h1><ul>
<li><code>shape</code>：生成张量的维度</li>
<li><code>mean</code>：均值</li>
<li><code>stddev</code>：标准差</li>
</ul>
<p>这个函数产生正态分布，均值和标准差自己设定。这是一个截断的产生正太分布的函数，就是说产生正太分布的值如果与均值的差值大于两倍的标准差，那就重新生成。和一般的正太分布的产生随机数据比起来，这个函数产生的随机数与均值的差距不会超过两倍的标准差，但是一般的别的函数是可能的。</p>
<h1 id="tf-name-scope-amp-tf-variable-scope"><a href="#tf-name-scope-amp-tf-variable-scope" class="headerlink" title="tf.name_scope() &amp; tf.variable_scope()"></a><code>tf.name_scope() &amp; tf.variable_scope()</code></h1><p>它们主要针对<code>name</code>进行范围定义。典型的TensorFlow可以有数以千计的节点，在构建各<code>op</code>的过程中，命名要做到不重复，在编写程序时就要特别注意。<code>name_scope</code>和<code>variable_scope</code>是从<code>name</code>上保证变量命名的唯一性。有了<code>scope</code>的存在，其中描述的变量和常量就会在机器当中自动给添加前缀描述，作为对各个变量、常量的区分。在这里的编程和以前使用C、C++不同之处就是，C/C++当中的变量名实际相当于这里的<code>XXX.name()</code>而不是<code>XXX</code>。</p>
<h2 id="1-tf-name-scope对tf-get-variable-的影响"><a href="#1-tf-name-scope对tf-get-variable-的影响" class="headerlink" title="1. tf.name_scope对tf.get_variable()的影响"></a>1. <code>tf.name_scope</code>对<code>tf.get_variable()</code>的影响</h2><p>例子：</p>
<pre><code class="python">with tf.name_scope(&quot;a_name_scope&quot;) as myscope:
    initializer = tf.constant_initializer(value=1)
    var1 = tf.get_variable(name=&#39;var1&#39;, shape=[1], dtype=tf.float32, initializer=initializer)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(var1.name)        # var1:0
    print(sess.run(var1))     # [ 1.]

# var1:0
# [ 1.]
</code></pre>
<h1 id="tf-nn-dropout"><a href="#tf-nn-dropout" class="headerlink" title="tf.nn.dropout()"></a><code>tf.nn.dropout()</code></h1><p>Dropout就是在不同的训练过程中随机扔掉一部分神经元。也就是让某个神经元的激活值以一定的概率<code>p</code>，让其停止工作，即此次训练过程中不更新权值，也不参加神经网络的计算。但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作。</p>
<pre><code class="python">def dropout(x, keep_prob, noise_shape=None, seed=None, name=None)
</code></pre>
<ul>
<li><code>x</code>：需要的训练、测试数据等</li>
<li><code>keep_prob</code>：dropout概率</li>
</ul>
<p>输出是：<code>A Tensor of the same shape of x</code>。</p>
<blockquote>
<p>一个神经元将以概率<code>keep_prob</code>决定是否被抑制。如果被抑制，该神经元的输出就为<code>0</code>；如果不被抑制，那么该神经元的输出值将被放大到原来的<code>1/keep_prob</code> 倍。个人觉得是随机抑制。</p>
</blockquote>
<p>除非是大型网络，才采用dropout，不然在一些小型网络上，训练结果不是很好</p>
<h1 id="tf-nn-conv2d"><a href="#tf-nn-conv2d" class="headerlink" title="tf.nn.conv2d"></a><code>tf.nn.conv2d</code></h1><pre><code class="python">tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)
</code></pre>
<ul>
<li><code>input</code>：指需要做卷积的输入图像，它要求是一个<code>Tensor</code>，具有<code>[batch, in_height, in_width, in_channels]</code>这样的<code>shape</code>，具体含义是<code>[训练时一个</code>batch<code>的图片数量, 图片高度, 图片宽度, 图像通道数]</code>。注意：它是一个<code>4</code>维的<code>Tensor</code>，要求类型为<code>float32</code>和<code>float64</code>其中之一。</li>
<li><code>filter</code>：相当于CNN中的卷积核，它要求是一个<code>Tensor</code>，具有<code>[filter_height, filter_width, in_channels, out_channels]</code>这样的<code>shape</code>，具体含义是<code>[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]</code>，要求类型与参数<code>input</code>相同。注意：第三维<code>in_channels</code>，就是参数<code>input的</code>第四维。</li>
<li><code>strides</code>：卷积时在图像每一维的步长，这是一个长度为<code>4</code>的一维张量。每一维度对应的是<code>input</code>中每一维的对应移动步数。</li>
<li><code>padding</code>：<code>string</code>类型，只能是<code>SAME</code>或<code>VALID</code>。这个值决定了不同的卷积方式。<ul>
<li>例如，当用<code>5x5</code> 的卷积核对<code>28x28</code>的图像进行卷积，采用<code>padding=VALID</code>模式，步长为<code>5</code>，那么最后每个方向会余下<code>3</code>个像素，而<code>VALID</code>模式中会直接舍弃，这样每个方向上的<code>5</code>个元素会变成一个元素，所以最终卷积后的图像大小变成了<code>5x5</code>。</li>
<li>当采用<code>padding=SAME</code>模式时，当每个方向差偶数个元素时首尾各补一半，差奇数个时前边补奇数个，后边补偶数个。当然具体差多少元素和选定的卷积核大小以及滑动步长密切相关。</li>
</ul>
</li>
<li><code>use_cudnn_on_gpu</code>：<code>bool</code>类型，是否使用<code>cudnn</code>加速，默认为<code>true</code>。</li>
<li><code>name</code>：用以指定该操作的<code>name</code>。</li>
</ul>
<p>结果：返回一个<code>Tensor</code>，这个输出就是常说的<code>feature map</code>，<code>shape</code>仍然是<code>[batch, height, width, channels]</code>这种形式。</p>
<h1 id="tf-nn-separable-conv2d"><a href="#tf-nn-separable-conv2d" class="headerlink" title="tf.nn.separable_conv2d"></a><code>tf.nn.separable_conv2d</code></h1><pre><code class="python">tf.nn.separable_conv2d (input, depthwise_filter, pointwise_filter, strides, padding, rate=None,
name=None, data_format=None)
</code></pre>
<ul>
<li><code>input</code>： 指需要做卷积的输入图像，要求是一个4维<code>Tensor</code>，具有<code>[batch, height, width, in_channels]</code>这样的<code>shape</code>，具体含义是<code>[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]</code></li>
<li><code>filter</code>：相当于CNN中的卷积核，要求是一个<code>4</code>维<code>Tensor</code>，具有<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>这样的<code>shape</code>，具体含义是<code>[卷积核的高度，卷积核的宽度，输入通道数，输出卷积乘子]</code>，同理这里第三维<code>in_channels</code>，就是参数<code>input</code>的第四维。</li>
<li><code>strides</code>：卷积的滑动步长。</li>
<li>padding： <code>string</code>类型的量，只能<code>SAME</code>或<code>VALID</code>，这个值决定了不同边缘填充方式。</li>
<li><code>rate</code>：要求是一个<code>int</code>型的正数，正常的卷积操作应该会有<code>stride</code>（即卷积核的滑动步长），但是空洞卷积是没有<code>stride</code>参数的。取而代之，它使用了新的<code>rate</code>参数，它定义为在输入图像上卷积时的采样间隔。可以理解为卷积核当中穿插了（rate-1）数量的<code>0</code>，把原来的卷积核插出了很多“洞洞”，这样做卷积时就相当于对原图像的采样间隔变大了.。可以易得出<code>rate=1</code>时，就没有<code>0</code>插入，此时这个函数就变成了普通卷积。</li>
</ul>
<p>结果返回一个<code>Tensor</code>，<code>shape</code>为<code>[batch, out_height, out_width, in_channels * channel_multiplier]</code>，注意这里输出通道变成了<code>in_channels * channel_multiplier</code>。</p>
<h2 id="常规卷积操作"><a href="#常规卷积操作" class="headerlink" title="常规卷积操作"></a>常规卷积操作</h2><p>对于一张<code>5×5</code>像素、三通道彩色输入图片（<code>shape</code>为<code>5×5×3</code>）。经过<code>3×3</code>卷积核的卷积层（假设输出通道数为<code>4</code>，则卷积核<code>shape</code>为<code>3×3×3×4</code>），最终输出<code>4</code>个<code>Feature Map</code>，如果有<code>same padding</code>，则尺寸与输入层相同（<code>5×5</code>），如果没有则为尺寸变为<code>3×3</code>。 </p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-d03e7ecf3dba787a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="Depthwise-Convolution"><a href="#Depthwise-Convolution" class="headerlink" title="Depthwise Convolution"></a>Depthwise Convolution</h2><p>不同于常规卷积操作，Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积。常规卷积每个卷积核是同时操作输入图片的每个通道。 同样是对于一张<code>5×5</code>像素、三通道彩色输入图片（<code>shape</code>为<code>5×5×3</code>），Depthwise Convolution首先经过第一次卷积运算，不同于上面的常规卷积，DW完全是在二维平面内进行。卷积核的数量与上一层的通道数相同（通道和卷积核一一对应）。所以一个三通道的图像经过运算后生成了<code>3</code>个<code>Feature map</code>(如果有<code>same padding</code>，则尺寸与输入层相同为<code>5×5</code>)。</p>
<p>Depthwise Convolution完成后的<code>Feature map</code>数量与输入层的通道数相同，无法扩展Feature map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-529584d7a3ab4443.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="Pointwise-Convolution"><a href="#Pointwise-Convolution" class="headerlink" title="Pointwise Convolution"></a>Pointwise Convolution</h2><p>Pointwise Convolution的运算与常规卷积运算非常相似，它的卷积核的尺寸为<code>1×1×M</code>，<code>M</code>为上一层的通道数。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的<code>Feature map</code>。有几个卷积核就有几个输出<code>Feature map</code>。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-8c009c7b659336bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="tf-nn-avg-pool"><a href="#tf-nn-avg-pool" class="headerlink" title="tf.nn.avg_pool"></a><code>tf.nn.avg_pool</code></h1><pre><code class="python">def avg_pool(value, ksize, strides, padding, data_format=&#39;NHWC&#39;, name=None)。
</code></pre>
<ul>
<li><code>value</code>：一个四维的张量。数据维度是<code>[batch, height, width, channels]</code>。</li>
<li><code>ksize</code>：一个长度不小于<code>4</code>的整型数组。每一位上的值对应于输入数据张量中每一维的窗口对应值。</li>
<li><code>data_format</code>：<code>NHWC</code>代表输入张量维度的顺序， <code>N</code> 为个数，<code>H</code>为高度，<code>W</code>为宽度，<code>C</code>为通道数（<code>RGB</code>三通道或者灰度单通道）。</li>
</ul>
<p>输出：一个张量。数据类型和<code>value</code>相同。计算输出维度的方法是： <code>shape(output) = (shape(value) - ksize + 1) / strides</code></p>
<h1 id="tf-initialize-all-variables-vs-tf-initialize-variables"><a href="#tf-initialize-all-variables-vs-tf-initialize-variables" class="headerlink" title="tf.initialize_all_variables() vs tf.initialize_variables"></a><code>tf.initialize_all_variables() vs tf.initialize_variables</code></h1><blockquote>
<p><code>tf.initialize_all_variables()</code>: THIS FUNCTION IS DEPRECATED. It will be removed after 2017-03-02. Instructions for updating: Use <code>tf.global_variables_initializer()</code> instead. </p>
</blockquote>
<h2 id="1-变量初始化"><a href="#1-变量初始化" class="headerlink" title="1. 变量初始化"></a>1. 变量初始化</h2><p>变量初始化的标准形式：</p>
<pre><code class="python">init = tf.initialize_all_variables() 
sess = tf.Session() 
sess.run(init)
</code></pre>
<p>也可简写为：</p>
<pre><code class="python">tf.Session().run(tf.initialize_all_variables())
</code></pre>
<p>如何有选择地初始化部分变量呢？可以使用<code>tf.initialize_variables()</code>。比如要初始化<code>v_6</code>，<code>v_7</code>，<code>v_8</code>三个变量：</p>
<pre><code class="python">init_new_vars_op = tf.initialize_variables([v_6, v_7, v_8]) sess.run(init_new_vars_op)
</code></pre>
<h2 id="2-识别未被初始化的变量"><a href="#2-识别未被初始化的变量" class="headerlink" title="2. 识别未被初始化的变量"></a>2. 识别未被初始化的变量</h2><p>用<code>try &amp; except</code>语句块捕获：</p>
<pre><code class="python">uninit_vars = [] for var in tf.all_variables(): 
    try: 
        sess.run(var) 
    except tf.errors.FailedPreconditionError:             
        uninit_vars.append(var) 
        init_new_vars_op = tf.initialize_variables(uninit_vars)
</code></pre>
<h2 id="3-变量的更新"><a href="#3-变量的更新" class="headerlink" title="3. 变量的更新"></a>3. 变量的更新</h2><pre><code class="python">state = tf.Variable(1, name=&#39;counter&#39;)
add_one = tf.add(state, tf.constant(1))
update = tf.assign(state, add_one)
with tf.Session() as sess:         
    sess.run(tf.gloabl_variables_initializer()) 
    sess.run(state) 
    for _ in range(3): 
        sess.run(update) 
        print(sess.run(state))
</code></pre>
<h1 id="tensorflow-tf-python-io"><a href="#tensorflow-tf-python-io" class="headerlink" title="tensorflow tf.python_io"></a><code>tensorflow tf.python_io</code></h1><p>该模块是tensorflow用来处理<code>tfrecords</code>文件的接口，定义在<code>tensorflow/python/lib/io/python_io.py</code>，主要包含了四个部分：</p>
<ul>
<li><code>class TFRecordCompressionType</code>：记录的压缩类型。 </li>
<li><code>class TFRecordOptions</code>：用于操作<code>TFRecord</code>文件的选项。</li>
<li><code>class TFRecordWriter</code>：将记录写入<code>TFRecords</code>文件的类。</li>
<li><code>tf_record_iterator(…)</code>：从TFRecords文件中读取记录的迭代器</li>
</ul>
<h2 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h2><pre><code class="python">from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from tensorflow.python import pywrap_tensorflow
from tensorflow.python.framework import errors
from tensorflow.python.util import compat

# 该类定义了tfrecords文件压缩类型：无，ZLIB，GZIP三种
class TFRecordCompressionType(object):
  &quot;&quot;&quot;The type of compression for the record.&quot;&quot;&quot;
  NONE = 0
  ZLIB = 1
  GZIP = 2


# 这个类会转换为proto格式，以便与C++接口对接
class TFRecordOptions(object):
  &quot;&quot;&quot;Options used for manipulating TFRecord files.&quot;&quot;&quot;
  compression_type_map = {
      TFRecordCompressionType.ZLIB: &quot;ZLIB&quot;,
      TFRecordCompressionType.GZIP: &quot;GZIP&quot;,
      TFRecordCompressionType.NONE: &quot;&quot;
  }

  def __init__(self, compression_type):
    self.compression_type = compression_type

  @classmethod
  def get_compression_type_string(cls, options):
    if not options:
      return &quot;&quot;
    return cls.compression_type_map[options.compression_type]

def tf_record_iterator(path, options=None):
  &quot;&quot;&quot;从tfrecords文件读取数据的迭代器.
  参数:
    path: TFRecords文件路径.
    options: 读取选项，主要是压缩类型，TFRecordOptions对象.
  yields:
    Strings.
  异常:
    IOError: 路径不正确是引发.
  &quot;&quot;&quot;
  compression_type = TFRecordOptions.get_compression_type_string(options)
  with errors.raise_exception_on_not_ok_status() as status:
    reader = pywrap_tensorflow.PyRecordReader_New(
        compat.as_bytes(path), 0, compat.as_bytes(compression_type), status)#读取器，pywarp_tensorflow包装所以的符号，这里定义了一个文件读取器对象

  if reader is None:
    raise IOError(&quot;Could not open %s.&quot; % path)
  while True:
    try:
      with errors.raise_exception_on_not_ok_status() as status:
        reader.GetNext(status)
    except errors.OutOfRangeError:
      break
    yield reader.record() #逐步读取文件
  reader.Close()


class TFRecordWriter(object):
  &quot;&quot;&quot;tfrecords文件写操作类，由于实施了__enter__和__exit__接口，根据Python的上下文管理机制，可以用with语句
  &quot;&quot;&quot;

  # TODO(josh11b): Support appending?
  def __init__(self, path, options=None):
    &quot;&quot;&quot;打开文件，并初始化写对象
    参数:
      path: 文件路径
      options: 选项，TFRecordOptions对象
    Raises:
      IOError: If `path` cannot be opened for writing.
    &quot;&quot;&quot;
    compression_type = TFRecordOptions.get_compression_type_string(options)#获取压缩类型

    with errors.raise_exception_on_not_ok_status() as status:
      self._writer = pywrap_tensorflow.PyRecordWriter_New(
          compat.as_bytes(path), compat.as_bytes(compression_type), status)#定义writer

  def __enter__(self):
    &quot;&quot;&quot;进入with语句块&quot;&quot;&quot;
    return self

  def __exit__(self, unused_type, unused_value, unused_traceback):
    &quot;&quot;&quot;退出with语句块，并关闭文件&quot;&quot;&quot;
    self.close()

  def write(self, record):
    &quot;&quot;&quot;想文件中写入一条记录.
    Args:
      record: str
    &quot;&quot;&quot;
    self._writer.WriteRecord(record)#实际是由writer实现

  def flush(self):
    &quot;&quot;&quot;刷新缓冲区内容到磁盘文件&quot;&quot;&quot;
    with errors.raise_exception_on_not_ok_status() as status:
      self._writer.Flush(status)

  def close(self):
    &quot;&quot;&quot;关闭文件&quot;&quot;&quot;
    with errors.raise_exception_on_not_ok_status() as status:
      self._writer.Close(status)
</code></pre>
<h1 id="数据读取的三种方式"><a href="#数据读取的三种方式" class="headerlink" title="数据读取的三种方式"></a>数据读取的三种方式</h1><p><img src="https://upload-images.jianshu.io/upload_images/1351548-8d41c0a358a472ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>TensorFlow的系统架构分为两个部分：</p>
<ul>
<li>前端系统：提供编程模型，负责构造计算图；</li>
<li>后端系统：提供运行时环境，负责执行计算图。</li>
</ul>
<p>在处理数据的过程当中，由于现在的硬件性能的极大提升，数值计算过程可以通过加强硬件的方式来改善，因此数据读取（即<code>IO</code>)往往会成为系统运行性能的瓶颈。在TensorFlow框架中提供了三种数据读取方式：</p>
<ul>
<li>Preloaded data：预加载数据</li>
<li>Feeding：<code>placeholder</code>、<code>feed_dict</code>，即由占位符代替数据，运行时填入数据</li>
<li>Reading from file: 从文件中直接读取</li>
</ul>
<blockquote>
<p>TF的核心是用C++写的，这样的好处是运行快，缺点是调用不灵活。而Python恰好相反，所以结合两种语言的优势。涉及计算的核心算子和运行框架是用C++写的，并提供API给Python。Python调用这些API，设计训练模型(<code>Graph</code>)，再将设计好的<code>Graph</code>给后端去执行。简而言之，Python的角色是Design，C++是Run。</p>
</blockquote>
<h2 id="1-Preload-data-constant-预加载数据"><a href="#1-Preload-data-constant-预加载数据" class="headerlink" title="1. Preload data: constant 预加载数据"></a>1. Preload data: <code>constant</code> 预加载数据</h2><p>特点：数据直接嵌入<code>graph</code>， 由<code>graph</code>传入<code>session</code>中运行.</p>
<h2 id="2-Feeding-placeholder，feed-dict"><a href="#2-Feeding-placeholder，feed-dict" class="headerlink" title="2. Feeding: placeholder，feed_dict"></a>2. Feeding: <code>placeholder</code>，<code>feed_dict</code></h2><p>特点：由占位符代替数据，运行时填入数据</p>
<h2 id="3-Reading-From-File：直接从文件中读取"><a href="#3-Reading-From-File：直接从文件中读取" class="headerlink" title="3. Reading From File：直接从文件中读取"></a>3. Reading From File：直接从文件中读取</h2><p>前两种方法很方便，但是遇到大型数据的时候就会很吃力，即使是<code>Feeding</code>，中间环节的增加也是不小的开销，比如数据类型转换等等。最优的方案就是在<code>Graph</code>定义好文件读取的方法，让TF自己去从文件中读取数据，并解码成可使用的样本集。</p>
<p>这种直接从文件中读取数据的方式需要设计成<code>Queue</code>的方式才能较好的解决<code>IO</code>瓶颈的问题。 <code>Queue</code>机制有如下三个特点：</p>
<ul>
<li>producer-consumer pattern(生产消费模式)</li>
<li>独立于主线程执行</li>
<li>异步IO: <code>reader.read(queue) tf.train.batch()</code></li>
</ul>
<p><img src="http://honggang.io/images/AnimatedFileQueues.gif" alt=""></p>
<p>最左边的<code>A</code>、<code>B</code>、<code>C</code>是存储于磁盘中文件，经过打乱文件之后（这里是默认的乱序读取，只是文件的顺序乱，但是文件内容不受影响），进入到文件队列中（Filename Queue）。文件队列当中的文件经过阅读器（<code>Reader</code>）处理，存储到内存当中。接下来对文件进行解码（<code>Decoder</code>），解码之后进入样本队列当中进行批处理，此时经过批处理之后就可以用于模型训练了。</p>
<h1 id="四种类型数据的读取流程"><a href="#四种类型数据的读取流程" class="headerlink" title="四种类型数据的读取流程"></a>四种类型数据的读取流程</h1><h2 id="1-文件队列构造"><a href="#1-文件队列构造" class="headerlink" title="1. 文件队列构造"></a>1. 文件队列构造</h2><pre><code class="python">tf.train.string_input_producer(string_tensor,num_epochs,shuffle=True)
</code></pre>
<p>将输出字符串（例如文件名）输入到管道队列。返回具有输出字符串的队列。</p>
<ul>
<li><code>string_tensor</code>：含有文件名的一阶张量，需要指定文件路径</li>
<li><code>num_epochs</code>：将全部数据循环的次数</li>
</ul>
<h2 id="2-文件阅读器"><a href="#2-文件阅读器" class="headerlink" title="2. 文件阅读器"></a>2. 文件阅读器</h2><p>此时需要根据文件的格式，选择对应的文件阅读器。它们的返回值都是对应文件格式的读取器实例。</p>
<ul>
<li><strong>文本文件</strong>：<code>tf.TextLineReader()</code><ul>
<li>读取文本文件，逗号分隔值（CSV)格式，默认按行读取</li>
</ul>
</li>
<li><strong>二进制文件</strong>：<code>tf.FixedLengthRecordReader(record_bytes)</code><ul>
<li>读取每个记录是固定数量字节的二进制文件</li>
<li><code>record_bytes</code>：整型，指定每次读取的字节数</li>
</ul>
</li>
<li><strong>图片文件</strong>：<code>tf.WholeReader()</code><ul>
<li>将文件的全部内容作为值输出，即一次读取一整个文件</li>
</ul>
</li>
<li><strong><code>TFRecords</code>文件</strong>：<code>tf.TFRecordReader()</code><ul>
<li>读取<code>TFRecords</code>文件</li>
</ul>
</li>
</ul>
<p><strong>【注】：</strong>这几种文件格式都有一个共同的读取方法：<code>read(file_queue)</code></p>
<ul>
<li>从队列中指定内容数量</li>
<li><code>file_name</code> : 文件队列</li>
</ul>
<p>返回一个<code>Tensor</code>元组：<code>(key, value)</code> </p>
<ul>
<li><code>key</code>：文件名</li>
<li><code>value</code>：每次读取的值（一行文本、一张图片或指定字节的值）</li>
</ul>
<h2 id="3-文件内容解码器"><a href="#3-文件内容解码器" class="headerlink" title="3. 文件内容解码器"></a>3. 文件内容解码器</h2><p>由于从文件中读取的是字符串，需要函数去解析这些字符串，最后变换成张量 。</p>
<h3 id="（1）CSV文件"><a href="#（1）CSV文件" class="headerlink" title="（1）CSV文件"></a>（1）CSV文件</h3><pre><code class="python">tf.decode_csv(records,record_defaults=None,field_delim=None,name=None)
</code></pre>
<p>将CSV文件转换成张量，需要<code>tf.TextLineReader()</code>搭配使用。</p>
<ul>
<li><code>records</code>：<code>tensor</code>型字符串，每个字符串是CSV中的记录行（即<code>value</code>值）</li>
<li><code>record_defaults</code>：此参数决定了所得张量的类型，并设置一个值，如果在输入字符串中缺少则使用默认值，如<code>[[1],[1]]</code>或者<code>[[“None”]，[“None”]]</code></li>
<li><code>field_dim</code>：默认分隔符为：<code>,</code></li>
</ul>
<h3 id="（2）二进制文件"><a href="#（2）二进制文件" class="headerlink" title="（2）二进制文件"></a>（2）二进制文件</h3><pre><code class="python">tf.decode_raw(bytes,out_type,little_endian=None,name=None)
</code></pre>
<p>将字节转换为一个数字向量表示，字节为以字符串类型的张量。与函数<code>tf.FixedLengthRecordReader</code>搭配使用。将二进制转换为<code>uint8</code>格式。</p>
<h3 id="（3）图像文件"><a href="#（3）图像文件" class="headerlink" title="（3）图像文件"></a>（3）图像文件</h3><ul>
<li><p><code>tf.image.decode_jpeg(contents)</code></p>
<ul>
<li>将JPEG编码的图像解码为<code>uint8</code>张量</li>
<li>返回<code>uint8</code>张量（3D形状<code>[height,width,channels]</code>）</li>
</ul>
</li>
<li><p><code>tf.image.decode_png(contents)</code></p>
<ul>
<li>将PNG编码的图像解码为<code>uint8</code>或者<code>uint16</code>编码</li>
<li>返回张量类型（3D形状<code>[height,width,channels]</code>）</li>
</ul>
</li>
</ul>
<h3 id="（4）TFRecords文件"><a href="#（4）TFRecords文件" class="headerlink" title="（4）TFRecords文件"></a>（4）<code>TFRecords</code>文件</h3><h2 id="4-批处理数据"><a href="#4-批处理数据" class="headerlink" title="4. 批处理数据"></a>4. 批处理数据</h2><p>对数据进行批处理需要在会话开启之前进行。</p>
<ul>
<li><p><code>tf.train.batch(tensors,batch_size,num_threads=1,capacity=32,name=None)</code></p>
<ul>
<li>用于读取指定大小（个数）的张量，返回<code>tensors</code>。<ul>
<li><code>tensor</code>：包含张量的列表</li>
<li><code>batch_size</code>：从队列中读取的批处理数据大小</li>
<li><code>num_threads</code>：进入队列的线程数</li>
<li><code>capacity</code>：整数，批处理队列中元素的最大数量</li>
</ul>
</li>
</ul>
</li>
<li><p><code>tf.train.shuffle_batch(tensors,batch_size,capacity,min_after_dequeue,num_threads=1,capacity=32,name=None)</code></p>
</li>
<li>乱序读取指定大小（数量）的张量<ul>
<li><code>min_after_dequeue</code>：留下队列里的张量个数，能够保持随机打乱</li>
</ul>
</li>
</ul>
<h1 id="TFRecords文件的存储与读取"><a href="#TFRecords文件的存储与读取" class="headerlink" title="TFRecords文件的存储与读取"></a><code>TFRecords</code>文件的存储与读取</h1><p><code>TFRecords</code>是TensorFlow中的设计的一种内置的文件格式，它是一种二进制文件，优点为：</p>
<ul>
<li>统一不同输入文件的框架</li>
<li>它更好地利用内存，更方便复制和移动（<code>TFRecord</code>压缩的二进制文件，<code>protocal buffer</code>序列化）</li>
<li>用于将二进制数据和标签（训练的类别标签）数据存储在同一个文件中</li>
</ul>
<h2 id="一-TFRecords存储"><a href="#一-TFRecords存储" class="headerlink" title="一. TFRecords存储"></a>一. <code>TFRecords</code>存储</h2><p>在将其他数据存储为<code>TFRecords</code>文件的时候，需要经过两个步骤：</p>
<ul>
<li>建立<code>TFRecord</code>存储器</li>
<li>构造每个样本的<code>Example</code>模块</li>
</ul>
<h3 id="1-建立TFRecord存储器"><a href="#1-建立TFRecord存储器" class="headerlink" title="1. 建立TFRecord存储器"></a>1. 建立<code>TFRecord</code>存储器</h3><pre><code class="python">tf.python_io.TFRecordWriter(path)
</code></pre>
<p>用于写入<code>TFRecords</code>文件。</p>
<ul>
<li><code>path</code>：<code>TFRecords</code>文件的路径</li>
<li>方法：<ul>
<li><code>write(record)</code>：向文件中写入一个字符串记录（即一个样本）。注：此处的字符串记录为一个序列化的<code>Example</code>，通过<code>Example.SerializeToString()</code>来实现，它的作用是将<code>Example</code>中的<code>map</code>压缩为二进制，节约大量空间。</li>
<li><code>close()</code>：关闭文件写入器</li>
</ul>
</li>
</ul>
<h3 id="2-构造每个样本的Example协议块"><a href="#2-构造每个样本的Example协议块" class="headerlink" title="2. 构造每个样本的Example协议块"></a>2. 构造每个样本的<code>Example</code>协议块</h3><p><code>Example</code>协议块的规则为（<code>feature.proto</code>）：</p>
<pre><code>message Example {
  Features features = 1;
};

message Features {
  map&lt;string, Feature&gt; feature = 1;
};

message Feature {
  oneof kind {
    BytesList bytes_list = 1;
    FloatList float_list = 2;
    Int64List int64_list = 3;
  }
};
</code></pre><h4 id="（1）-tf-train-Example-features-None"><a href="#（1）-tf-train-Example-features-None" class="headerlink" title="（1） tf.train.Example(features = None)"></a>（1） <code>tf.train.Example(features = None)</code></h4><p>用于写入<code>TFRecords</code>文件，返回<code>Example</code>协议格式块。</p>
<ul>
<li><code>features</code>：<code>tf.train.Features</code>类型的特征实例</li>
</ul>
<h4 id="（2）-tf-train-Features-feature-None"><a href="#（2）-tf-train-Features-feature-None" class="headerlink" title="（2） tf.train.Features(feature = None)"></a>（2） <code>tf.train.Features(feature = None)</code></h4><p>构造每个样本的信息键值对，返回<code>Features</code>类型。</p>
<ul>
<li><code>feature</code>：字典数据，<code>key</code>为要保存的名字，<code>value</code>为<code>tf.train.Feature</code>实例</li>
</ul>
<h4 id="（3）-tf-train-Feature-options"><a href="#（3）-tf-train-Feature-options" class="headerlink" title="（3） tf.train.Feature(**options)"></a>（3） <code>tf.train.Feature(**options)</code></h4><p><code>options</code>可以选择如下三种格式数据：</p>
<ul>
<li><code>bytes_list = tf.train.BytesList(value = [Bytes])</code></li>
<li><code>int64_list = tf.train.Int64List(value = [Value])</code></li>
<li><code>float_list = tf.trian.FloatList(value = [Value])</code></li>
</ul>
<p>将图片数据转化为<code>TFRecords</code>的例子（对每一个样本，都做如下的处理）：</p>
<pre><code class="python">example = tf.train.Example(feature = tf.train.Features(feature = {                               &quot;image&quot;:tf.train.Feature(bytes_list=tf.train.BytesList(value=[image(bytes)]))              &quot;label&quot;:tf.train.Feature(int64_list=tf.train.Int64List(value=[label(int)]))
    }))
</code></pre>
<h2 id="二-TFRecords读取方法"><a href="#二-TFRecords读取方法" class="headerlink" title="二. TFRecords读取方法"></a>二. <code>TFRecords</code>读取方法</h2><p>和文件阅读器的流程基本相同（需要先使用<code>TFRecordReader()</code>创建一个文件阅读器），只是中间多了一步解析过程。</p>
<h3 id="解析TFRecords的Example协议内存块的步骤："><a href="#解析TFRecords的Example协议内存块的步骤：" class="headerlink" title="解析TFRecords的Example协议内存块的步骤："></a>解析<code>TFRecords</code>的<code>Example</code>协议内存块的步骤：</h3><h4 id="1-tf-parse-single-example-serialized-features-None-name-None"><a href="#1-tf-parse-single-example-serialized-features-None-name-None" class="headerlink" title="1. tf.parse_single_example(serialized,features=None,name= None"></a>1. <code>tf.parse_single_example(serialized,features=None,name= None</code></h4><ul>
<li>用于解析一个单一的<code>Example</code>原型，返回 一个键值对组成的字典，键为读取的名字</li>
<li><code>serialized</code>：标量字符串的<code>Tensor</code>，一个序列化的<code>Example</code>，文件经过文件阅读器之后的<code>value</code></li>
</ul>
<h4 id="2-tf-FixedLenFeature-shape-dtype"><a href="#2-tf-FixedLenFeature-shape-dtype" class="headerlink" title="2. tf.FixedLenFeature(shape,dtype)"></a>2. <code>tf.FixedLenFeature(shape,dtype)</code></h4><ul>
<li><code>shape</code>：输入数据的形状，一般不指定，为空列表</li>
<li><code>dtype</code>：输入数据类型，与存储进文件的类型要一致，类型只能是<code>float32</code>，<code>int64</code>，<code>string</code></li>
<li>返回<code>Tensor</code> (即使有零的部分也存储）</li>
</ul>
<p><strong>【注】：</strong><code>1</code>中<code>features</code>中的<code>value</code>还可以为<code>tf.VarLenFeature()</code>，但是这种方式用的比较少，它返回的是<code>SparseTensor</code>数据，这是一种只存储非零部分的数据格式。</p>
<h3 id="总结（读取TFRecord文件的步骤）："><a href="#总结（读取TFRecord文件的步骤）：" class="headerlink" title="总结（读取TFRecord文件的步骤）："></a>总结（读取TFRecord文件的步骤）：</h3><ul>
<li>生成<code>tfrecord</code>文件</li>
<li>定义<code>recordreader</code>解析<code>tfrecord</code>文件</li>
<li>构造一个批生成器（<code>batcher</code>）</li>
<li>构建其他的操作</li>
<li>初始化所有的操作</li>
<li>启动<code>QueueRunner</code></li>
</ul>
<p><code>TFRecords</code>可以允许将任意的数据转换为TensorFlow所支持的格式， 这种方法可以使TensorFlow的数据集更容易与网络应用架构相匹配。这种建议的方法就是使用<code>TFRecord</code>s文件，<code>TFRecords</code>文件包含了<code>tf.train.Example</code>协议内存块（protocol buffer），协议内存块包含了字段<code>Features</code>。可以将获取的数据填入到<code>Example</code>协议内存块（protocol buffer），将协议内存块序列化为一个字符串， 并且通过<code>tf.python_io.TFRecordWriter</code>写入到<code>TFRecords</code>文件。</p>
<p><code>TFRecords</code>文件格式在图像识别中有很好的使用，其<strong>可以将二进制数据和标签数据（训练的类别标签）数据存储在同一个文件中。它可以在模型进行训练之前通过预处理步骤，将图像转换为<code>TFRecords</code>格式。此格式最大的优点是可以将每幅输入图像和与之关联的标签放在同一个文件中。<code>TFRecords</code>文件是一种二进制文件，其不对数据进行压缩，所以可以被快速加载到内存中。格式不支持随机访问，因此它适合于大量的数据流，但不适用于快速分片或其他非连续存取。</strong></p>
<h1 id="tf-cast"><a href="#tf-cast" class="headerlink" title="tf.cast()"></a><code>tf.cast()</code></h1><pre><code class="python">cast(
    x,
    dtype,
    name=None )
</code></pre>
<p>将<code>x</code>的数据格式转化成<code>dtype</code>。例如，原来<code>x</code>的数据格式是<code>bool</code>，那么将其转化成<code>float</code>以后，就能够将其转化成<code>0</code>和<code>1</code>的序列。</p>
<h1 id="tf-app-flags"><a href="#tf-app-flags" class="headerlink" title="tf.app.flags()"></a><code>tf.app.flags()</code></h1><p><code>tf.app.flags</code>主要用于处理命令行参数的解析工作，其实可以理解为一个封装好了的<code>argparse</code>包（<code>argparse</code>是一种结构化的数据存储格式，类似于Json、XML）。</p>
<p>首先需要导入<code>argparse</code>包，使用<code>argparse</code>的第一步就是创建一个解析器对象，告诉它将会有些什么参数。当程序运行时，该解析器可以用于处理命令行参数（只能解析参数、获取参数、设置已有参数的默认值等操作）。<code>argparse</code>中的解析器类是<code>ArgumentParser</code>。</p>
<pre><code class="python">from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse as _argparse

from tensorflow.python.platform import tf_logging as _logging
from tensorflow.python.util.all_util import remove_undocumented

_global_parser = _argparse.ArgumentParser()
</code></pre>
<p>定义了<code>_FlagValues</code>类，如前面所说，要处理命令行参数，就要用解析器类<code>_global_parser</code>里的方法来解析，这里使用了<code>parse_known_args()</code>函数，其实同<code>parse_args()</code>函数差不多（注：这里说的<code>parse_args()</code>函数和此处<code>_FlagValues</code>类中定义的<code>_parse_args()</code>函数不一样，前者也是<code>argparse</code>中一种解析参数的函数），<strong>只是这个函数在接受到多余的命令行参数时不会报错，会原封不动的以一个<code>list</code>形式将其返回</strong>。所以此函数返回的”result“是参数解析完的数据，而”unparsed“是那些未被解析的参数<code>list</code>。</p>
<p>将命令行传入的命令和数据解析出来以字典的形式放到<code>__dict__</code>的<code>[&#39;_flags&#39;]</code>这个字典中，这么做也是为了方便后续直接访问命令行输入的命令，因为可以直接通过字典调用（在tensorflow中其实是通过<code>tf.app.flag.Flags</code>来实现实例化这个类，然后再调用里面解析得到的参数即可）。</p>
<pre><code class="python">class _FlagValues(object):
  &quot;&quot;&quot;Global container and accessor for flags and their values.&quot;&quot;&quot;

  def __init__(self):
    self.__dict__[&#39;__flags&#39;] = {}
    self.__dict__[&#39;__parsed&#39;] = False
    self.__dict__[&#39;__required_flags&#39;] = set()

  def _parse_flags(self, args=None):
    result, unparsed = _global_parser.parse_known_args(args=args)
    for flag_name, val in vars(result).items():
      self.__dict__[&#39;__flags&#39;][flag_name] = val
    self.__dict__[&#39;__parsed&#39;] = True
    self._assert_all_required()
    return unparsed
</code></pre>
<p>初始化完了之后，可以看到源码里是一些<code>setattr/getattr</code>的方法，也就是一些设置和获得解析的命令行参数的方法。要注意的是，在获得参数的时候（<code>getattr</code>），首先要通过解析字典中的’parsed’来检验参数是否已经被解析过，因为在<code>_parse_flags</code>方法中，只要解析过参数（也即是运行过该函数），那么<code>self.__dict__[‘__parsed’]</code>就会为<code>True</code>（表明解析过参数）。因为这里是获取参数，所以除了要判断参数是否在字典里的基本要求外，还要判断有没有解析过参数，没有就运行<code>_parse_flags</code>解析参数。</p>
<pre><code class="python">  def __getattr__(self, name):
    &quot;&quot;&quot;Retrieves the &#39;value&#39; attribute of the flag --name.&quot;&quot;&quot;
    try:
      parsed = self.__dict__[&#39;__parsed&#39;]
    except KeyError:
      # May happen during pickle.load or copy.copy
      raise AttributeError(name)
    if not parsed:
      self._parse_flags()
    if name not in self.__dict__[&#39;__flags&#39;]:
      raise AttributeError(name)
    return self.__dict__[&#39;__flags&#39;][name]

  def __setattr__(self, name, value):
    &quot;&quot;&quot;Sets the &#39;value&#39; attribute of the flag --name.&quot;&quot;&quot;
    if not self.__dict__[&#39;__parsed&#39;]:
      self._parse_flags()
    self.__dict__[&#39;__flags&#39;][name] = value
    self._assert_required(name)

  def _add_required_flag(self, item):
    self.__dict__[&#39;__required_flags&#39;].add(item)

  def _assert_required(self, flag_name):
    if (flag_name not in self.__dict__[&#39;__flags&#39;] or
        self.__dict__[&#39;__flags&#39;][flag_name] is None):
      raise AttributeError(&#39;Flag --%s must be specified.&#39; % flag_name)

  def _assert_all_required(self):
    for flag_name in self.__dict__[&#39;__required_flags&#39;]:
      self._assert_required(flag_name)
</code></pre>
<p>上述整个<code>_FlagValues</code>类进行实例化，这样就方便进行访问操作。当需要访问命令行输入的命令时，就可以直接从这个实例里操作（注意：从这里开始都是在类外定义的方法，所以要调用就只能通过<code>tf.app.flags.XXX</code>来实现。</p>
<pre><code class="python">FLAGS = _FlagValues()
</code></pre>
<p><code>_define_helper</code>函数中调用了<code>_global_parser.add_argument</code>完成对命令行参数的添加（传入<code>flag_name</code>，<code>default_value</code>，<code>docstring</code>，<code>flagtype</code>参数）。</p>
<p>可以看到，添加参数使用的是解析器类<code>_global_parser</code>的方法。</p>
<ul>
<li><code>&#39;--&#39; + flag_name</code>：它表示定义的命令行参数在使用时必须以<code>--</code>开头，比如<code>--flag_int9</code></li>
<li><code>default_value</code>：是参数的默认值</li>
<li><code>docstring</code>：保存帮助信息（命令行中输入<code>-h</code>激活该参数）</li>
<li><code>flagtype</code>：限定了命令行参数数据的类型。</li>
</ul>
<pre><code class="python">def _define_helper(flag_name, default_value, docstring, flagtype):
  &quot;&quot;&quot;Registers &#39;flag_name&#39; with &#39;default_value&#39; and &#39;docstring&#39;.&quot;&quot;&quot;
  _global_parser.add_argument(&#39;--&#39; + flag_name,
                              default=default_value,
                              help=docstring,
                              type=flagtype)
</code></pre>
<p>使用<code>_define_helper</code>参数即可以添加命令行参数。源码又将其封装为针对<code>string/int/float/bool</code>类型参数的特定添加方法。</p>
<p>对于<code>DEFINE_string()</code>，由于<code>_define_helper()</code>的最后一个<code>flagtype</code>参数是<code>str</code>。</p>
<pre><code class="python">def DEFINE_string(flag_name, default_value, docstring):
     _define_helper(flag_name, default_value, docstring, str)

def DEFINE_integer(flag_name, default_value, docstring):
     _define_helper(flag_name, default_value, docstring, int)

def DEFINE_boolean(flag_name, default_value, docstring):
    # Register a custom function for &#39;bool&#39; so --flag=True works.
    def str2bool(v):
        return v.lower() in (&#39;true&#39;, &#39;t&#39;, &#39;1&#39;)
      _global_parser.add_argument(&#39;--&#39; + flag_name,
                              nargs=&#39;?&#39;,
                              const=True,
                              help=docstring,
                              default=default_value,
                              type=str2bool)

    # Add negated version, stay consistent with argparse with regard to
      # dashes in flag names.
      _global_parser.add_argument(&#39;--no&#39; + flag_name,
                              action=&#39;store_false&#39;,
                              dest=flag_name.replace(&#39;-&#39;, &#39;_&#39;))

    # The internal google library defines the following alias, so we match
    # the API for consistency.
    DEFINE_bool = DEFINE_boolean  # pylint: disable=invalid-name

def DEFINE_float(flag_name, default_value, docstring):
    _define_helper(flag_name, default_value, docstring, float)
</code></pre>
<p>源码中最后介绍的方法是：在程序运行前，先将某些命令行参数加入到”必备参数“（<code>__required_flags</code>）的字典中，以判断解析完的参数是否满足这些必备要求。因为<code>mark_flags_as_required</code>方法会调用<code>mark_flag_as_required</code>方法，来将当前传入的参数加入到<code>__required_flags</code>字典中（<code>_add_required_flag</code>方法），在最上面解析参数的方法<code>_parse_flags</code>中，解析完参数会通过<code>_assert_all_required</code>方法判断解析到的参数是否都在<code>_required_flags</code>字典中。</p>
<pre><code class="python">def mark_flag_as_required(flag_name):
  &quot;&quot;&quot;Ensures that flag is not None during program execution.

  It is recommended to call this method like this:

    if __name__ == &#39;__main__&#39;:
      tf.flags.mark_flag_as_required(&#39;your_flag_name&#39;)
      tf.app.run()

  Args:
    flag_name: string, name of the flag to mark as required.

  Raises:
    AttributeError: if flag_name is not registered as a valid flag name.
      NOTE: The exception raised will change in the future. 
  &quot;&quot;&quot;
  if _global_parser.get_default(flag_name) is not None:
    _logging.warn(
        &#39;Flag %s has a non-None default value; therefore, &#39;
        &#39;mark_flag_as_required will pass even if flag is not specified in the &#39;
        &#39;command line!&#39; % flag_name)
  FLAGS._add_required_flag(flag_name)


def mark_flags_as_required(flag_names):
  &quot;&quot;&quot;Ensures that flags are not None during program execution.

  Recommended usage:

    if __name__ == &#39;__main__&#39;:
      tf.flags.mark_flags_as_required([&#39;flag1&#39;, &#39;flag2&#39;, &#39;flag3&#39;])
      tf.app.run()

  Args:
    flag_names: a list/tuple of flag names to mark as required.
  Raises:
    AttributeError: If any of flag name has not already been defined as a flag.
      NOTE: The exception raised will change in the future.
  &quot;&quot;&quot;
  for flag_name in flag_names:
    mark_flag_as_required(flag_name)
</code></pre>
<p>在tensorflow中该怎么使用呢？首先通过 <code>tf.app.flags</code> 来调用这个<code>flags.py</code>文件，这样就可以  <code>flags.DEFINE_interger/float()</code> 来添加命令行参数，而 <code>FLAGS=flags.FLAGS</code> 可以实例化这个解析参数的类从对应的命令行参数取出参数。</p>
<p>例如，新建 <code>test.py</code> 文件，并输入如下代码，代码的功能是创建几个命令行参数，然后把命令行参数输出显示：</p>
<pre><code class="python">import tensorflow as tf  

flags = tf.app.flags
flags.DEFINE_string(&#39;data_dir&#39;, &#39;/tmp/mnist&#39;, &#39;Directory with the MNIST data.&#39;)
flags.DEFINE_integer(&#39;batch_size&#39;, 5, &#39;Batch size.&#39;)
flags.DEFINE_integer(&#39;num_evals&#39;, 1000, &#39;Number of batches to evaluate.&#39;)
FLAGS = flags.FLAGS

print(FLAGS.data_dir, FLAGS.batch_size, FLAGS.num_evals)
</code></pre>
<ul>
<li>在命令行中输入 <code>test.py -h</code> 就可以查看帮助信息，也就是 <code>Directory with the MNIST data.</code>，<code>Batch size</code> 和 <code>Number of batches to evaluate</code> 这样的消息。</li>
<li>在命令行中输入 <code>test.py --batchsize 10</code> 就可以将 <code>batch_size</code> 的值修改为 <code>10</code>。</li>
</ul>
<h1 id="tf-app-run"><a href="#tf-app-run" class="headerlink" title="tf.app.run()"></a><code>tf.app.run()</code></h1><p>该函数一般都是出现在这种代码中：</p>
<pre><code class="python">if __name__ == &#39;__main__&#39;:
    tf.app.run()
</code></pre>
<p>上述第一行代码表示如果当前是从其它模块调用的该模块程序，则不会运行 <code>main</code> 函数！而如果就是直接运行的该模块程序，则会运行 <code>main</code> 函数。</p>
<p><code>flags_passthrough = f._parse_flags(args = args)</code>中的 <code>parse_flags</code> 就是 <code>tf.app.flags</code>源码中用来解析命令行参数的函数。所以这一行就是解析参数的功能；下面两行代码也就是 <code>tf.app.run</code> 的核心意思：执行程序中 <code>main</code> 函数，并解析命令行参数。</p>
<p>源码如下：</p>
<pre><code class="python">def run(main=None, argv=None):
  &quot;&quot;&quot;Runs the program with an optional &#39;main&#39; function and &#39;argv&#39; list.&quot;&quot;&quot;
  f = flags.FLAGS

  # Extract the args from the optional `argv` list.
  args = argv[1:] if argv else None

  # Parse the known flags from that list, or from the command
  # line otherwise.
  # pylint: disable=protected-access
  flags_passthrough = f._parse_flags(args=args)
  # pylint: enable=protected-access

  main = main or _sys.modules[&#39;__main__&#39;].main

  # Call the main function, passing through any arguments
  # to the final program.
  _sys.exit(main(_sys.argv[:1] + flags_passthrough))
</code></pre>
<h1 id="Basis"><a href="#Basis" class="headerlink" title="Basis"></a>Basis</h1><p>在张量中并没有真正保存数字，它保存的是如何得到这些数字的计算过程。一个张量中主要保存了三个属性 ： 名字（ <code>name</code> ）、维度（<code>shape</code>）和类型（<code>type</code>）。在 TensorFlow 中，变量 （ <code>tf.Variable</code> ）的作用就是保存和更新神经网络中的参数 。 </p>
<p>计算神经网络的前向传播结果需要三部分信息：</p>
<ul>
<li>神经网络的输入，这个输入就是从实体中提取的特征向量。</li>
<li>神经网络的连接结构。神经网络是由神经元构成的 ，神经网络的结构给出不同神经元之间输入输出的连接关系。神经网络中的神经元也可以称之为节点 </li>
<li>每个神经元中的参数。</li>
</ul>
<p>通过 <code>tf.global_variables_initializer()</code> 函数，就不需要将变量一个一个初始化了（不需要对每一个<code>Variable</code> 调用<code>initializer</code>） 。 这个函数也会自动处理变量之间的依赖关系。</p>
<p>在 TensorFlow 中，变量的声明函数 <code>tf.Variable</code> 是一个运算。这个运算的输出结果就是一个张量。所以变量只是一种特殊的张量。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-cc3a42af45680ad4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="神经网络反向传播优化流程图"></p>
<p>反向传播算法实现了 一个法代的过程。在每次迭代的开始，首先需要选取一小部分训练数据 ， 这一小部分数据叫做一个 <code>batch</code>。然后，这个 <code>batch</code> 的样例会通过前向传播算法得到神经网络模型的预测结果。因为训练数据都是有正确答案标注的 ，所以可以计算出当前神经网络模型的预测答案与正确答案之间的差距。最后，基于预测值和真实值之间的差距，反向传播算法会相应更新神经网络参数的取值，使得在这个 <code>batch</code> 上神经网络模型的预测结果和真实答案更加接近 。</p>
<p>如果每轮迭代中选取的数据都要通过常量来表示，那么 TensorFlow 的计算图将会太大。因为每生成一个常量 ， TensorFlow 都会在计算图中增加一个节点。<code>placeholder</code> 相当于定义了一个位置，这个位置中的数据在程序运行时再指定 。这样在程序中就不需要生成大量常量来提供输入数据，而只需要将数据通过  <code>placeholder</code> 传入TensorFlow 计算图 。在 <code>placeholder</code> 定义时，这个位置上的数据类型是需要指定的。和其他张量一样， <code>placeholder</code> 的类型也是不可以改变的。 <code>placeholder</code> 中数据的维度信息可以根据提供的数据推导得出，所以不一定要给出。</p>
<p><code>feed_dict</code> 是一个字典（ <code>map</code> ），在字典中需要给出每个用到的 <code>placeholder</code> 的取值。如果某个需要的 <code>placeholder</code> 没有被指定取值，那么程序在运行时将会报错。</p>
<p>如果将每一个神经元（也就是神经网络中的节点）的输出通过一个非线性函数，那么整个神经网络的模型也就不再是线性的了。这个非线性函数就是激活函数。</p>
<h2 id="get-shape"><a href="#get-shape" class="headerlink" title="get_shape()"></a><code>get_shape()</code></h2><p><code>a.get_shape()</code>中<code>a</code>的数据类型只能是<code>tensor</code>，且返回的是一个元组。</p>
<p>会话拥有并管理 TensorFlow 程序运行时的所有资源。所有计算完成之后需要关闭会话来帮助系统回收资源，否则就可能出现资源泄漏的问题。</p>
<h2 id="TensorFlow-随机数生成函数"><a href="#TensorFlow-随机数生成函数" class="headerlink" title="TensorFlow 随机数生成函数"></a>TensorFlow 随机数生成函数</h2><p><img src="https://upload-images.jianshu.io/upload_images/1351548-bf87f711bec3f1b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="常数生成函数"><a href="#常数生成函数" class="headerlink" title="常数生成函数"></a>常数生成函数</h2><p><img src="https://upload-images.jianshu.io/upload_images/1351548-e42fbd934f63d67f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="tf-ConfigProto-allow-soft-placement-True-log-device-placement-True"><a href="#tf-ConfigProto-allow-soft-placement-True-log-device-placement-True" class="headerlink" title="tf.ConfigProto(allow soft_placement=True,log_device_placement=True)"></a><code>tf.ConfigProto(allow soft_placement=True,log_device_placement=True)</code></h1><p>通过<code>ConfigProto</code>可以配置类似并行的线程数、 GPU 分配策略、运算超时时间等参数。在这些参数中，最常使用的有两个。</p>
<ul>
<li><code>allow_soft_placement</code>，这是一个布尔型的参数，当它为<code>True</code>时， 在以下任意一个条件成立时， GPU 上的运算可 以放到 CPU 上进行 ：<ol>
<li>运算无法在 GPU 上执行 。</li>
<li>没有 GPU 资源（比如运算被指定在第 二个 GPU 上运行 ，但是机器只有一个 GPU ） 。</li>
<li>运算输入包含对 CPU 计算结果的引用 。<br>这个参数的默认值为 <code>False</code>，但是为了使得代码的可移植性更强，在有 GPU 的环境下这个参数一般会被设置为 <code>True</code>。不同的 GPU 驱动版本可能对计算的支持有略微的区别，通过将 <code>allow_soft_placement</code> 参数设为 <code>True</code>，当某些运算无法被当前 GPU 支持时，可 以自动调整到 CPU 上，而不是报错。类似地，通过将这个参数设置为 True，可以让程序在拥有不同数量的 GPU 机器上顺利运行。</li>
</ol>
</li>
<li><code>log_device_placement</code>：这也是一个布尔型的参数，当它为 <code>True</code> 时日志中将会记录每个节点被安排在哪个设备上以方便调试。而在生产环境中将这个参数设置为 <code>False</code> 可以减少日志量。</li>
</ul>
<h1 id="tf-clip-by-value"><a href="#tf-clip-by-value" class="headerlink" title="tf.clip_by_value"></a><code>tf.clip_by_value</code></h1><pre><code class="python">tf.clip_by_value(A, min, max)
</code></pre>
<p>输入一个张量 <code>A</code>，把 <code>A</code> 中的每一个元素的值都压缩在 <code>min</code> 和 <code>max</code> 之间。小于 <code>min</code> 的让它等于 <code>min</code>；大于 <code>max</code> 的元素的值等于 <code>max</code>。</p>
<p>通过 <code>tf.clip_by_value</code> 函数可以将一个张量中的数值限制在一个范围之内，这样可以避免一些运算错误（比如 <code>log0</code> 是无效的）。</p>
<h1 id="反向传播算法（backpropagation）"><a href="#反向传播算法（backpropagation）" class="headerlink" title="反向传播算法（backpropagation）"></a>反向传播算法（backpropagation）</h1><p><strong>梯度下降算法主要用于优化单个参数的取值，反向传播算法给出了一个高效的方式在所有参数上使用梯度下降算法，从而使神经网络模型在训练数据上的损失函数尽可能小</strong>。</p>
<p>反向传播算法是训练神经网络的核心算法，它可以根据定义好的损失函数优化神经网络中参数的取值，从而使神经网络模型在训练数据集上的损失函数达到一个较小值。神经网络模型中参数的优化过程直接决定了模型的质量 ，是使用神经网络时非常重要的一步。</p>
<p>假设用<img src="https://upload-images.jianshu.io/upload_images/1351548-c74f9e90d23584fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">表示神经网络中的参数，<img src="https://upload-images.jianshu.io/upload_images/1351548-ecef3c81353a38bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">表示在给定的参数取值下，训练数据集上损失函数的大小，那么整个优化过程可以抽象为寻找一个参数<img src="https://upload-images.jianshu.io/upload_images/1351548-c74f9e90d23584fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>，使得<img src="https://upload-images.jianshu.io/upload_images/1351548-ecef3c81353a38bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">最小 。 梯度下降算法会法代式更新参数<img src="https://upload-images.jianshu.io/upload_images/1351548-c74f9e90d23584fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""> ，不断沿着梯度的反方向让参数朝着总损失更小的方向更新 。</p>
<p>参数的梯度可以通过求偏导的方式计算，对于参数<img src="https://upload-images.jianshu.io/upload_images/1351548-c74f9e90d23584fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">，其梯度为<img src="https://upload-images.jianshu.io/upload_images/1351548-d31400db109f6bfa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">。有了梯度，还需要定义一个学习率<img src="https://upload-images.jianshu.io/upload_images/1351548-391d4a6382331ae9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">（ learning rate ）来定义每次参数更新的幅度。从直观上理解，可以认为学习率定义的就是每次参数移动的幅度（即，控制参数更新的速度。学习率决定了参数每次更新的幅度。如果幅度过大，那么可能导致参数在极优值的两侧来回移动。）。通过参数的梯度和学习率，参数更新的公式为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-cb3e1f86cac21c37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>神经网络的优化过程可以分为两个阶段，第一个阶段先通过前向传播算法计算得到预测值，并将预测值和真实值做对比得出两者之间的差距 。 然后在第二个阶段通过反向传播算法计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数。</p>
<p><strong>【注意】：</strong>梯度下降算法并不能保证被优化的函数达到全局最优解。只有当损失函数为凸函数时，梯度下降算法才能保证达到全局最优解。除了不一定能达到全局最优，梯度下降算法的另外一个问题就是计算时间太长。因为要在全部训练数据上最小化损失，所以损失函数<img src="https://upload-images.jianshu.io/upload_images/1351548-ecef3c81353a38bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">是在所有训练数据上的损失和。这样在每一轮迭代中都需要计算在全部训练数据上的损失函数。在海量训练数据下，要计算所有训练数据的损失函数是非常消耗时间的。</p>
<p>为了加速训练过程，可以使用随机梯度下降的算法（SGD）。这个算法优化的不是在全部训练数据上的损失函数，而是在每一轮法代中，随机优化某一条训练数据上的损失函数。这样每一轮参数更新的速度就大大加快了。因为随机梯度下降算法每次优化的只是某一条数据上的损失函数，所以它的问题也非常明显：在某一条数据上损失函数更小并不代表在全部数据上损失函数更小，于是使用随机梯度下降优化得到的神经网络甚至可能无法达到局部最优 。</p>
<p>为了综合梯度下降算法和随机梯度下降算法的优缺点，在实际应用中一般采用这两个算法的折中：每次计算一小部分训练数据的损失函数 。 这一小部分数据被称之为一个 <code>batch</code>。通过矩阵运算，每次在一个 <code>batch</code> 上优化神经网络的参数并不会比单个数据慢太多。另一方面，每次使用一个 <code>batch</code> 可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。</p>
<h1 id="tf-train-exponential-decay"><a href="#tf-train-exponential-decay" class="headerlink" title="tf.train.exponential_decay"></a><code>tf.train.exponential_decay</code></h1><p>实现指数衰减学习率。</p>
<p>步骤： </p>
<ol>
<li>首先使用较大学习率（目的：为快速得到一个比较优的解)）</li>
<li>然后通过迭代逐步减小学习率（目的：为使模型在训练后期更加稳定）</li>
</ol>
<pre><code class="python">tf.train.exponential_decay(
    learning_rate,        # 初始学习率
    global_step,      # 当前迭代次数
    decay_steps,        # 衰减速度（在迭代到该次数时学习率衰减为earning_rate * decay_rate）
    decay_rate,            # 学习率衰减系数，通常介于0-1之间。
    staircase=False,    # (默认值为False,当为True时，（global_step/decay_steps）则被转化为整数) ,
                                        # 选择不同的衰减方式。
    name=None
)
</code></pre>
<p>在神经网络的训练过程中，学习率（learning rate）控制着参数的更新速度。学习率会按照以下公式变化：</p>
<pre><code>decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)
</code></pre><p>直观解释：假设给定初始学习率 learning_rate 为 <code>0.1</code>，学习率衰减率为 <code>0.1</code>，<code>decay_steps</code> 为 <code>10000</code>。则随着迭代次数从 <code>1</code> 到 <code>10000</code>，当前的学习率 <code>decayed_learning_rate</code> 慢慢的从 <code>0.1</code> 降低为  <code>0.1*0.1 = 0.01</code>，当迭代次数到 <code>20000</code>，当前的学习率慢慢的从 <code>0.01</code> 降低为<img src="https://upload-images.jianshu.io/upload_images/1351548-1bd7275ad60cee5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">，以此类推。 也就是说每 <code>10000</code> 次迭代，学习率衰减为前 <code>10000</code> 次的十分之一，该衰减是连续的，这是在 <code>staircase</code> 为 <code>False</code> 的情况下。</p>
<p>如果 <code>staircase</code> 为 <code>True</code>，则 <code>global_step / decay_steps</code>始终取整数，也就是说衰减是突变的，每 <code>decay_steps</code> 次变化一次，变化曲线是阶梯状。</p>
<p><code>decay_steps</code> 通常代表了完整的使用一遍训练数据所需要的迭代轮数。这个迭代轮数也就是总训练样本数除以每一个 <code>batch</code> 中的训练样本数 。这种设置的常用场景是每完整地过完一遍训练数据，学习率就减小一次。这可以使得训练数据集中的所有数据对模型训练有相等的作用。当使用连续的指数衰减学习率时，不同的训练数据有不同的学习率，而当学习率减小时，对应的训练数据对模型训练结果的影响也就小了。</p>
<h1 id="保存模型和加载模型"><a href="#保存模型和加载模型" class="headerlink" title="保存模型和加载模型"></a>保存模型和加载模型</h1><p>利用 TensorFlow 搭建模型并保存时，保存模型的方法为：</p>
<pre><code class="python">saver = tf.train.Saver();
saver.save(sess, model_path + model_name)
</code></pre>
<p>这样，在 <code>model_path</code> 路径下会得到3个名为 <code>model_name</code> 的文件和一个 <code>checkpoint</code> 文件。</p>
<p>其中， <code>.data-00000-of-00001</code> 和 <code>.index</code> 文件属于 <code>ckpt</code> 文件，<strong>保存了所有的 <code>weights</code> 、<code>biases</code> 、<code>gradients</code> 等变量</strong>。</p>
<p><code>.meta</code> 主要保存图结构，它是 <code>pb（protocol buffer）</code> 格式文件，包含变量、<code>op</code>、集合等。</p>
<p><code>checkpoint</code> 文件<strong>记录了保存的最新的 <code>checkpoint</code> 文件以及其他 <code>checkpoint</code> 文件列表</strong>。在 <code>inference</code> 时，可以通过修改这个文件来指定使用哪个model。</p>
<p>在实际训练中，我们可能会在每迭代 <code>1000</code> 次后保存一次模型数据，但有图是不变的，没必要每次都去保存，可以通过如下方式指定不保存图：</p>
<pre><code class="python">saver.saver(sess, model_path, global_step=step, write_meta_graph=False)
</code></pre>
<p>另一种比较实用的是，如果希望每 <code>2</code> 个小时保存一次模型，并且只保存最近的 <code>5</code> 个模型文件：</p>
<pre><code class="python">tf.train.Saver(max_to_keep=5, keep_checkpoint_every_n_hours=2)
</code></pre>
<p><strong>【注意】：</strong> TensorFlow 默认只会保存最近的 <code>5</code> 个模型文件，如果希望保存更多的文件，可以通过 <code>max_to_keep</code> 来进行指定。</p>
<p>如果我们不对 <code>tf.train.Saver</code> 指定任何参数，默认会保存所有变量。如果不想保存所有变量，而只保存一部分变量，可以通过指定 <code>variables/collections</code>。在创建 <code>tf.train.Saver</code> 实例时，通过将需要保存的变量构造 <code>list</code> 或者 <code>dictionary</code>，传入到 <code>Saver</code> 中：</p>
<pre><code class="python">w1 = tf.Variable(tf.random_normal(shape=[2]), name=&#39;w1&#39;)
w2 = tf.Variable(tf.random_normal(shape=[5]), name=&#39;w2&#39;)
saver = tf.train.Saver([w1,w2])
sess = tf.Session()
sess.run(tf.global_variables_initializer())
saver.save(sess, &#39;./checkpoint_dir/MyModel&#39;,global_step=1000)
</code></pre>
<p>由于 TensorFlow 将图和变量数据分开保存为不同的文件。因此，在导入模型时，也要分为两步：</p>
<ol>
<li>构造网络图</li>
<li>加载参数</li>
</ol>
<p>在进行构造网络图时，一种比较笨的办法是：直接再将之前的代码再敲一遍，实现跟模型一模一样的图结构。其实，由于之前已经保存了图，就没有必要再去手写一遍图结构代码，而是可以通过以下方式就可以将图加载进来：</p>
<pre><code class="python">saver = tf.train.import_meta_graph(meta_path)
</code></pre>
<p>仅仅将图加载进来并没有用，还需要将之前训练好的模型参数（即 <code>weights</code> 、<code>biases</code>等）。由于变量值需要依赖于 <code>Session</code>，因此在加载参数时，需要先要构造好 <code>Session</code> ：</p>
<pre><code class="python">with tf.Session() as sess:
    new_saver = tf.train.import_meta_graph(meta_path)
    new_saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))
</code></pre>
<p>此时，<code>w1</code> 和 <code>w2</code> 就被加载进了图中，并且可以被访问：</p>
<pre><code class="python">with tf.Session() as sess:
    saver = tf.train.import_meta_graph(model_path+meta_name)
    saver.restore(sess, tf.train.latest_checkpoint(model_path))
    print(sess.run(&#39;w1:0&#39;))
# [ 0.51480412 -0.56989086]
</code></pre>
<p>理解了如何保存和恢复模型之后，在实际应用中，通常是希望使用一些已经训练好的模型，如 <code>prediction</code>、<code>fine-tuning</code> 以及进一步训练等。这时，可能需要获取训练好的模型中的一些中间结果值，可以通过 <code>graph.get_tensor_by_name(&#39;w1:0&#39;)</code> 来进行获取（其中，<code>w1:0</code> 是 tensor 的 name）。</p>
<p>例子：</p>
<pre><code class="python">w1 = tf.Placeholder(&#39;float&#39;, name=&#39;w1&#39;)
w2 = tf.Placeholder(&#39;float&#39;, name=&#39;w2&#39;)
b1 = tf.Variable(2.0, name=&#39;bias&#39;)

# 定义一个op，用于后面恢复
w3 = tf.add(w1, w2)
w4 = tf.multiply(w3, b1, name=&#39;op_to_restore&#39;)
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 创建一个Saver对象，用于保存多有的变量
saver = tf.train.Saver()

# 通过传入数据，执行op
print(sess.run(w4, feed_dict={w1:4, w2:8}))

# 现在保存模型
saver.save(sess, model_path+model_name, global_step=1000)

# 通过
sess = tf.Session()
# 先加载图和参数变量
saver = tf.train.import_meta_graph(model_path+meta_name)
saver.restore(sess, tf.train.latest_checkpoint(model_path))

# 访问Placeholder变量，并且创建feed-dict来作为Placeholder的新值
graph = tf.get_default_graph()
w1 = graph.get_tensor_by_name(&#39;w1:0&#39;)
w2 = graph.get_tensor_by_name(&#39;w2:0&#39;)
feed_dict = {w1:13.0, w2:17.0}

# 访问想要执行的op
op_to_restore = graph.get_tensor_by_name(&#39;op_to_restore:0&#39;)
print(sess.run(op_to_restore, feed_dict))
# 结果为60.0
</code></pre>
<p><strong>注意</strong>：保存模型时，只会保存变量的值，<code>Placeholder</code> 里面的值不会被保存。</p>
<p>如果不仅仅是用训练好的模型，还要加入一些 <code>op</code>，或者说加入一些 <code>layers</code>，并训练新的模型：</p>
<pre><code class="python">sess = tf.Session()

# 加载图和变量
saver = tf.train.import_meta_graph(meta_name)
saver.restore(sess, tf.train.latest_checkpoint(model_path))

# 访问Placeholder变量，并且创建feed-dict来作为Placeholder的新值
graph = tf.train.get_default_graph()
w1 = graph.get_tensor_by_name(&#39;w1:0&#39;)
w2 = graph.get_tensor_by_name(&#39;w2:0&#39;)
feed_dict = {w1:13.0, w2:17.0}

# 访问想要执行的op
op_to_restore = graph.get_tensor_by_name(&#39;op_to_restore:0&#39;)

# 在当前图中加入op
add_on_op = tf.multiply(op_to_restore, 2)
print(sess.run(add_on_op, feed_dict))
</code></pre>
<p>如果只想恢复图的一部分，并且再加入其它的 <code>op</code> 用于 <code>fine-tuning</code>。只需通过 <code>graph.get_tensor_by_name()</code> 方法获取需要的 <code>op</code>，并且在此基础上建立图。假设我们需要在训练好的VGG网络使用图，并且修改最后一层，将输出改为 <code>2</code>，用于 <code>fine-tuning</code> 新数据：</p>
<pre><code class="python">saver = tf.train.import_meta_graph(meta_name)
graph = tf.get_default_graph()

# 访问用于fine-tuning的output
fc7 = graph.get_tensor_by_name(&#39;fc7:0&#39;)

# 如果想要修改最后一层梯度，需要如下操作
fc7 = tf.stop_gradient(fc7)
fc7_shape = fc7.get_shape().as_list()

new_outputs = 2
weights = tf.Variable(tf.truncated_normal([fc7_shape[3], num_ouputs], stddev=0.05))
biases = tf.Variable(tf.constant(0.05, shape=[num_outputs]))
output = tf.matmul(fc7, weights) + biased
pred = tf.nn.softmax(output)
</code></pre>
<h1 id="tf-gradients-与stop-gradient"><a href="#tf-gradients-与stop-gradient" class="headerlink" title="tf.gradients()与stop_gradient()"></a><code>tf.gradients()与stop_gradient()</code></h1><p>TensorFlow 中有一个计算梯度的函数 <code>tf.gradients(ys, xs)</code>。要注意的是，<code>xs</code> 中的 <code>x</code> 必须要与 <code>ys</code> 相关，不相关的话，会报错。</p>
<p>例如，下述代码中定义了两个变量 <code>w1</code>， <code>w2</code>， 但 <code>ret</code> 只与 <code>w1</code> 相关。</p>
<pre><code class="python">w1 = tf.Variable([[1,2]])
w2 = tf.Variable([[3,4]])
ret = tf.matmul(w1, [[2,1]])
grad = tf.gradients(ret, [w1,w2])

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    print(sess.run(grad))
# TypeError: Fetch argument None has invalid type
</code></pre>
<p>正确做法为：</p>
<pre><code class="python">w1 = tf.Variable([[1,2]])
w2 = tf.Variable([[3,4]])

ret = tf.matmul(w1, [[2],[1]])

grad = tf.gradients(ret,[w1])

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    print(sess.run(grad))
#  [array([[2, 1]], dtype=int32)]
</code></pre>
<p>对求导函数而言，其主要功能即求导：<br>$$<br>\frac{\partial y}{\partial x}<br>$$</p>
<p>。在tensorflow中，yy和xx都是tensor。</p>
<p>更进一步，tf.gradients()接受求导值ys和xs不仅可以是tensor，还可以是list，形如[tensor1, tensor2, …, tensorn]。当ys和xs都是list时，它们的求导关系为：</p>
<p>gradients() adds ops to the graph to output the derivatives of ys with respect to xs. It returns a list of Tensor of length len(xs) where each tensor is the sum(dy/dx) for y in ys.</p>
<p>意思是：</p>
<p>tf.gradients()实现ys对xs求导<br>求导返回值是一个list，list的长度等于len(xs)</p>
<p>假设返回值是[grad1, grad2, grad3]，ys=[y1, y2]，xs=[x1, x2, x3]。则，真实的计算过程为: </p>
<p><code>tf.gradients()</code> 的原型为：</p>
<pre><code class="python">tf.gradients(ys, xs, 
             grad_ys=None, 
             name=&#39;gradients&#39;,
             colocate_gradients_with_ops=False,
             gate_gradients=False,
             aggregation_method=None,
             stop_gradients=None)
</code></pre>
<h1 id="timeline模块"><a href="#timeline模块" class="headerlink" title="timeline模块"></a><code>timeline</code>模块</h1>
        
    

    
</div>


                

                <!-- Post Comments -->
                
                    
                
            </div>

            <!-- Post Prev & Next Nav -->
            <nav class="material-nav mdl-color-text--grey-50 mdl-cell mdl-cell--12-col">
    <!-- Prev Nav -->
    

    <!-- Section Spacer -->
    <div class="section-spacer"></div>

    <!-- Next Nav -->
    
        <a href="/2018/11/17/KCF/" id="post_nav-older" class="next-content">
            旧篇
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation">
                <i class="material-icons">arrow_forward</i>
            </button>
        </a>
    
</nav>

        </div>
    </div>



                    
                        <!-- Overlay For Active Sidebar -->
<div class="sidebar-overlay"></div>

<!-- Material sidebar -->
<aside id="sidebar" class="sidebar sidebar-colored sidebar-fixed-left" role="navigation">
    <div id="sidebar-main">
        <!-- Sidebar Header -->
        <div class="sidebar-header header-cover" style="background-image: url(/img/sidebar_header.png);">
    <!-- Top bar -->
    <div class="top-bar"></div>

    <!-- Sidebar toggle button -->
    <button type="button" class="sidebar-toggle mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon" style="display: initial;" data-upgraded=",MaterialButton,MaterialRipple">
        <i class="material-icons">clear_all</i>
        <span class="mdl-button__ripple-container">
            <span class="mdl-ripple">
            </span>
        </span>
    </button>

    <!-- Sidebar Avatar -->
    <div class="sidebar-image">
        <img src="/img/avatar.png" alt="Magicmanoooo's avatar">
    </div>

    <!-- Sidebar Email -->
    <a data-toggle="dropdown" class="sidebar-brand" href="#settings-dropdown">
        838713968@qq.com
        <b class="caret"></b>
    </a>
</div>


        <!-- Sidebar Navigation  -->
        <ul class="nav sidebar-nav">
    <!-- User dropdown  -->
    <li class="dropdown">
        <ul id="settings-dropdown" class="dropdown-menu">
            
                <li>
                    <a href="mailto: 838713968@qq.com" target="_blank" title="Email Me">
                        
                            <i class="material-icons sidebar-material-icons sidebar-indent-left1pc-element">email</i>
                        
                        Email Me
                    </a>
                </li>
            
        </ul>
    </li>

    <!-- Homepage -->
    
        <li id="sidebar-first-li">
            <a href="/">
                
                    <i class="material-icons sidebar-material-icons">home</i>
                
                主页
            </a>
        </li>
        
    

    <!-- Archives  -->
    
        <li class="dropdown">
            <a href="#" class="ripple-effect dropdown-toggle" data-toggle="dropdown">
                
                    <i class="material-icons sidebar-material-icons">inbox</i>
                
                    归档
                <b class="caret"></b>
            </a>
            <ul class="dropdown-menu">
            <li>
                <a class="sidebar_archives-link" href="/archives/2018/12/">十二月 2018<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/11/">十一月 2018<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/10/">十月 2018<span class="sidebar_archives-count">5</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/09/">九月 2018<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/08/">八月 2018<span class="sidebar_archives-count">4</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/07/">七月 2018<span class="sidebar_archives-count">5</span></a>
            </ul>
        </li>
        
    

    <!-- Categories  -->
    
        <li class="dropdown">
            <a href="#" class="ripple-effect dropdown-toggle" data-toggle="dropdown">
                
                    <i class="material-icons sidebar-material-icons">chrome_reader_mode</i>
                
                分类
                <b class="caret"></b>
            </a>
            <ul class="dropdown-menu">
                <li>
                
            </ul>
        </li>
        
    

    <!-- Pages  -->
    

    <!-- Article Number  -->
    
        <li>
            <a href="/archives">
                文章总数
                <span class="sidebar-badge">21</span>
            </a>
        </li>
        
    
</ul>


        <!-- Sidebar Footer -->
        <!--
I'm glad you use this theme, the development is no so easy, I hope you can keep the copyright, I will thank you so much.
If you still want to delete the copyrights, could you still retain the first one? Which namely "Theme Material"
It will not impact the appearance and can give developers a lot of support :)

很高兴您使用并喜欢该主题，开发不易 十分谢谢与希望您可以保留一下版权声明。
如果您仍然想删除的话 能否只保留第一项呢？即 "Theme Material"
它不会影响美观并可以给开发者很大的支持和动力。 :)
-->

<!-- Sidebar Divider -->

    <div class="sidebar-divider"></div>


<!-- Theme Material -->


<!-- Help & Support -->
<!--

    <a href="mailto:hiviosey@gmail.com" class="sidebar-footer-text-a">
        <div class="sidebar-text mdl-button mdl-js-button mdl-js-ripple-effect sidebar-footer-text-div" data-upgraded=",MaterialButton,MaterialRipple">
        sidebar.help
        <span class="mdl-button__ripple-container">
          <span class="mdl-ripple"></span>
        </span>
      </div>
    </a>

-->

<!-- Feedback -->
<!--

    <a href="https://github.com/viosey/hexo-theme-material/issues" target="_blank" class="sidebar-footer-text-a">
        <div class="sidebar-text mdl-button mdl-js-button mdl-js-ripple-effect sidebar-footer-text-div" data-upgraded=",MaterialButton,MaterialRipple">
        sidebar.feedback
        <span class="mdl-button__ripple-container"><span class="mdl-ripple"></span></span></div>
    </a>

-->

<!-- About Theme -->
<!--

    <a href="https://blog.viosey.com/index.php/Material.html" target="_blank" class="sidebar-footer-text-a">
        <div class="sidebar-text mdl-button mdl-js-button mdl-js-ripple-effect sidebar-footer-text-div" data-upgraded=",MaterialButton,MaterialRipple">
             sidebar.about_theme
            <span class="mdl-button__ripple-container"><span class="mdl-ripple"></span></span></div>
    </a>

-->

    </div>

    <!-- Sidebar Image -->
    

</aside>

                    

                    
                        <!-- Footer Top Button -->
                        <div id="back-to-top" class="toTop-wrap">
    <a href="#top" class="toTop">
        <i class="material-icons footer_top-i">expand_less</i>
    </a>
</div>

                    

                    <!--Footer-->
<footer class="mdl-mini-footer" id="bottom">
    
        <!-- Paradox Footer Left Section -->
        <div class="mdl-mini-footer--left-section sns-list">
    <!-- Twitter -->
    

    <!-- Facebook -->
    

    <!-- Google + -->
    

    <!-- Weibo -->
    
        <a href="https://weibo.com/5345088988/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo&amp;is_all=1" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-weibo">
                <span class="visuallyhidden">Weibo</span>
            </button><!--
     --></a>
    

    <!-- Instagram -->
    

    <!-- Tumblr -->
    

    <!-- Github -->
    
        <a href="https://github.com/Azurery" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-github">
                <span class="visuallyhidden">Github</span>
            </button><!--
     --></a>
    

    <!-- LinkedIn -->
    

    <!-- Zhihu -->
    
        <a href="https://www.zhihu.com/people/zhang-tao-60-41/activities" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-zhihu">
                <span class="visuallyhidden">Zhihu</span>
            </button><!--
     --></a>
    

    <!-- Bilibili -->
    
        <a href="https://space.bilibili.com/94222521/#/" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-bilibili">
                <span class="visuallyhidden">Bilibili</span>
            </button><!--
     --></a>
    

    <!-- Telegram -->
    
    
    <!-- V2EX -->
    
</div>


        <!--Copyright-->
        <div id="copyright">
            Copyright&nbsp;©&nbsp;2017&nbsp;-<script type="text/javascript">var fd = new Date();document.write("&nbsp;" + fd.getFullYear() + "&nbsp;");</script>Azurery
            
                <br>
                
                    只有永不遏止的奋斗，才能使青春之花，即使是凋谢，也是壮丽地凋谢
                
            
        </div>

        <!-- Paradox Footer Right Section -->

        <!--
        I am glad you use this theme, the development is no so easy, I hope you can keep the copyright.
        It will not impact the appearance and can give developers a lot of support :)

        很高兴您使用该主题，开发不易，希望您可以保留一下版权声明。
        它不会影响美观并可以给开发者很大的支持。 :)
        -->

        <div class="mdl-mini-footer--right-section">
            <div>
                <div class="footer-develop-div">Powered by <a href="https://hexo.io" target="_blank" class="footer-develop-a">Hexo</a></div>
                <div class="footer-develop-div">Theme - <a href="https://github.com/viosey/hexo-theme-material" target="_blank" class="footer-develop-a">Material</a></div>
            </div>
        </div>
    
</footer>


                    <!-- Import JS File -->

    <script>lsloader.load("lazyload_js","/js/lazyload.min.js?1BcfzuNXqV+ntF6gq+5X3Q==", true)</script>



    <script>lsloader.load("js_js","/js/js.min.js?V/53wGualMuiPM3xoetD5Q==", true)</script>



    <script>lsloader.load("np_js","/js/nprogress.js?pl3Qhb9lvqR1FlyLUna1Yw==", true)</script>


<script type="text/ls-javascript" id="NProgress-script">
    NProgress.configure({
        showSpinner: true
    });
    NProgress.start();
    $('#nprogress .bar').css({
        'background': '#29d'
    });
    $('#nprogress .peg').css({
        'box-shadow': '0 0 10px #29d, 0 0 15px #29d'
    });
    $('#nprogress .spinner-icon').css({
        'border-top-color': '#29d',
        'border-left-color': '#29d'
    });
    setTimeout(function() {
        NProgress.done();
        $('.fade').removeClass('out');
    }, 800);
</script>













<!-- UC Browser Compatible -->
<script>
	var agent = navigator.userAgent.toLowerCase();
	if(agent.indexOf('ucbrowser')>0) {
		document.write('<link rel="stylesheet" href="/css/uc.css">');
	   alert('由于 UC 浏览器使用极旧的内核，而本网站使用了一些新的特性。\n为了您能更好的浏览，推荐使用 Chrome 或 Firefox 浏览器。');
	}
</script>

<!-- Import prettify js  -->

    
        
            <script>lsloader.load("prettify_js","/js/prettify.min.js?WN07fivHQSMKWy7BmHBB6w==", true)</script>
        
    



<!-- Window Load -->
<!-- add class for prettify -->
<script type="text/ls-javascript" id="window-load">
    $(window).on('load', function() {
        // Post_Toc parent position fixed
        $('.post-toc-wrap').parent('.mdl-menu__container').css('position', 'fixed');
    });

    
        
            $(function() {
                $('pre').addClass('prettyprint linenums').attr('style', 'overflow:auto;');
                prettyPrint();
                })
        
    
    
</script>

<!-- MathJax Load-->


<!-- Bing Background -->


<script type="text/ls-javascript" id="lazy-load">
    // Offer LazyLoad
    queue.offer(function(){
        $('.lazy').lazyload({
            effect : 'show'
        });
    });

    // Start Queue
    $(document).ready(function(){
        setInterval(function(){
            queue.execNext();
        },200);
    });
</script>

<!-- Custom Footer -->



<script>
    (function(){
        var scriptList = document.querySelectorAll('script[type="text/ls-javascript"]')

        for (var i = 0; i < scriptList.length; ++i) {
            var item = scriptList[i];
            lsloader.runInlineScript(item.id,item.id);
        }
    })()
console.log('\n %c © Material Theme | Version: 1.5.0 | https://github.com/viosey/hexo-theme-material %c \n', 'color:#455a64;background:#e0e0e0;padding:5px 0;border-top-left-radius:5px;border-bottom-left-radius:5px;', 'color:#455a64;background:#e0e0e0;padding:5px 0;border-top-right-radius:5px;border-bottom-right-radius:5px;');
</script>

                </main>
            </div>
        </body>
    
</html>
