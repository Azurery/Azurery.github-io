<!DOCTYPE html>
<html style="display: none;" lang="zh">
    <head>
    <meta charset="utf-8">
    <!--
        © Material Theme
        https://github.com/viosey/hexo-theme-material
        Version: 1.5.0 -->
    <script>
        window.materialVersion = "1.5.0"
        // Delete localstorage with these tags
        window.oldVersion = [
            'codestartv1',
            '1.3.4',
            '1.4.0',
            '1.4.0b1'
        ]
    </script>

    <!-- dns prefetch -->
    <meta http-equiv="x-dns-prefetch-control" content="on">














    <!-- Title -->
    
    <title>
        
        Azurery
    </title>

    <!-- Meta & Info -->
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="format-detection" content="telephone=no"/>
    <meta name="theme-color" content="#0097A7">
    <meta name="author" content="Magicmanoooo">
    <meta name="description" itemprop="description" content="蒟蒻一枚">
    <meta name="keywords" content="">

    <!-- Import lsloader -->
    <script>(function(){window.lsloader={jsRunSequence:[],jsnamemap:{},cssnamemap:{}};lsloader.removeLS=function(key){try{localStorage.removeItem(key)}catch(e){}};lsloader.setLS=function(key,val){try{localStorage.setItem(key,val)}catch(e){}};lsloader.getLS=function(key){var val="";try{val=localStorage.getItem(key)}catch(e){val=""}return val};versionString="/*"+(window.materialVersion||"unknownVersion")+"*/";lsloader.clean=function(){try{var keys=[];for(var i=0;i<localStorage.length;i++){keys.push(localStorage.key(i))}keys.forEach(function(key){var data=lsloader.getLS(key);if(window.oldVersion){var remove=window.oldVersion.reduce(function(p,c){return p||data.indexOf("/*"+c+"*/")!==-1},false);if(remove){lsloader.removeLS(key)}}})}catch(e){}};lsloader.clean();lsloader.load=function(jsname,jspath,cssonload,isJs){if(typeof cssonload==="boolean"){isJs=cssonload;cssonload=undefined}isJs=isJs||false;cssonload=cssonload||function(){};var code;code=this.getLS(jsname);if(code&&code.indexOf(versionString)===-1){this.removeLS(jsname);this.requestResource(jsname,jspath,cssonload,isJs);return}if(code){var versionNumber=code.split(versionString)[0];if(versionNumber!=jspath){console.log("reload:"+jspath);this.removeLS(jsname);this.requestResource(jsname,jspath,cssonload,isJs);return}code=code.split(versionString)[1];if(isJs){this.jsRunSequence.push({name:jsname,code:code});this.runjs(jspath,jsname,code)}else{document.getElementById(jsname).appendChild(document.createTextNode(code));cssonload()}}else{this.requestResource(jsname,jspath,cssonload,isJs)}};lsloader.requestResource=function(name,path,cssonload,isJs){var that=this;if(isJs){this.iojs(path,name,function(path,name,code){that.setLS(name,path+versionString+code);that.runjs(path,name,code)})}else{this.iocss(path,name,function(code){document.getElementById(name).appendChild(document.createTextNode(code));that.setLS(name,path+versionString+code)},cssonload)}};lsloader.iojs=function(path,jsname,callback){var that=this;that.jsRunSequence.push({name:jsname,code:""});try{var xhr=new XMLHttpRequest;xhr.open("get",path,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){callback(path,jsname,xhr.response);return}}that.jsfallback(path,jsname)}};xhr.send(null)}catch(e){that.jsfallback(path,jsname)}};lsloader.iocss=function(path,jsname,callback,cssonload){var that=this;try{var xhr=new XMLHttpRequest;xhr.open("get",path,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){callback(xhr.response);cssonload();return}}that.cssfallback(path,jsname,cssonload)}};xhr.send(null)}catch(e){that.cssfallback(path,jsname,cssonload)}};lsloader.iofonts=function(path,jsname,callback,cssonload){var that=this;try{var xhr=new XMLHttpRequest;xhr.open("get",path,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){callback(xhr.response);cssonload();return}}that.cssfallback(path,jsname,cssonload)}};xhr.send(null)}catch(e){that.cssfallback(path,jsname,cssonload)}};lsloader.runjs=function(path,name,code){if(!!name&&!!code){for(var k in this.jsRunSequence){if(this.jsRunSequence[k].name==name){this.jsRunSequence[k].code=code}}}if(!!this.jsRunSequence[0]&&!!this.jsRunSequence[0].code&&this.jsRunSequence[0].status!="failed"){var script=document.createElement("script");script.appendChild(document.createTextNode(this.jsRunSequence[0].code));script.type="text/javascript";document.getElementsByTagName("head")[0].appendChild(script);this.jsRunSequence.shift();if(this.jsRunSequence.length>0){this.runjs()}}else if(!!this.jsRunSequence[0]&&this.jsRunSequence[0].status=="failed"){var that=this;var script=document.createElement("script");script.src=this.jsRunSequence[0].path;script.type="text/javascript";this.jsRunSequence[0].status="loading";script.onload=function(){that.jsRunSequence.shift();if(that.jsRunSequence.length>0){that.runjs()}};document.body.appendChild(script)}};lsloader.tagLoad=function(path,name){this.jsRunSequence.push({name:name,code:"",path:path,status:"failed"});this.runjs()};lsloader.jsfallback=function(path,name){if(!!this.jsnamemap[name]){return}else{this.jsnamemap[name]=name}for(var k in this.jsRunSequence){if(this.jsRunSequence[k].name==name){this.jsRunSequence[k].code="";this.jsRunSequence[k].status="failed";this.jsRunSequence[k].path=path}}this.runjs()};lsloader.cssfallback=function(path,name,cssonload){if(!!this.cssnamemap[name]){return}else{this.cssnamemap[name]=1}var link=document.createElement("link");link.type="text/css";link.href=path;link.rel="stylesheet";link.onload=link.onerror=cssonload;var root=document.getElementsByTagName("script")[0];root.parentNode.insertBefore(link,root)};lsloader.runInlineScript=function(scriptId,codeId){var code=document.getElementById(codeId).innerText;this.jsRunSequence.push({name:scriptId,code:code});this.runjs()};lsloader.loadCombo=function(jslist){var updateList="";var requestingModules={};for(var k in jslist){var LS=this.getLS(jslist[k].name);if(!!LS){var version=LS.split(versionString)[0];var code=LS.split(versionString)[1]}else{var version=""}if(version==jslist[k].path){this.jsRunSequence.push({name:jslist[k].name,code:code,path:jslist[k].path})}else{this.jsRunSequence.push({name:jslist[k].name,code:null,path:jslist[k].path,status:"comboloading"});requestingModules[jslist[k].name]=true;updateList+=(updateList==""?"":";")+jslist[k].path}}var that=this;if(!!updateList){var xhr=new XMLHttpRequest;xhr.open("get",combo+updateList,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){that.runCombo(xhr.response,requestingModules);return}}else{for(var i in that.jsRunSequence){if(requestingModules[that.jsRunSequence[i].name]){that.jsRunSequence[i].status="failed"}}that.runjs()}}};xhr.send(null)}this.runjs()};lsloader.runCombo=function(comboCode,requestingModules){comboCode=comboCode.split("/*combojs*/");comboCode.shift();for(var k in this.jsRunSequence){if(!!requestingModules[this.jsRunSequence[k].name]&&!!comboCode[0]){this.jsRunSequence[k].status="comboJS";this.jsRunSequence[k].code=comboCode[0];this.setLS(this.jsRunSequence[k].name,this.jsRunSequence[k].path+versionString+comboCode[0]);comboCode.shift()}}this.runjs()}})();</script>

    <!-- Import queue -->
    <script>function Queue(){this.dataStore=[];this.offer=b;this.poll=d;this.execNext=a;this.debug=false;this.startDebug=c;function b(e){if(this.debug){console.log("Offered a Queued Function.")}if(typeof e==="function"){this.dataStore.push(e)}else{console.log("You must offer a function.")}}function d(){if(this.debug){console.log("Polled a Queued Function.")}return this.dataStore.shift()}function a(){var e=this.poll();if(e!==undefined){if(this.debug){console.log("Run a Queued Function.")}e()}}function c(){this.debug=true}}var queue=new Queue();</script>

    <!-- Favicons -->
    <link rel="icon shortcut" type="image/ico" href="/img/favicon.png">
    <link rel="icon" sizes="192x192" href="/img/favicon.png">
    <link rel="apple-touch-icon" href="/img/favicon.png">

    <!--iOS -->
    <meta name="apple-mobile-web-app-title" content="Title">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="480">

    <!-- Add to homescreen for Chrome on Android -->
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Add to homescreen for Safari on iOS -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="Azurery">

    <!-- Site Verification -->
    
    

    <!-- RSS -->
    

    <!--[if lte IE 9]>
        <link rel="stylesheet" href="/css/ie-blocker.css">

        
            <script src="/js/ie-blocker.zhCN.js"></script>
        
    <![endif]-->

    <!-- Import CSS -->
    
        <style id="material_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("material_css","/css/material.min.css?Z7a72R1E4SxzBKR/WGctOA==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>
        <style id="style_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("style_css","/css/style.min.css?MKetZV3cUTfDxvMffaOezg==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>

        
            
                <style id="prettify_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("prettify_css","/css/prettify.min.css?zp8STOU9v89XWFEnN+6YmQ==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>
                <style id="prettify_theme"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("prettify_theme","/css/prettify/vibrant-ink.min.css?e5E/qqGcGveS7VTH4M896w==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>
            
        

    

    

    <!-- Config CSS -->

<!-- Other Styles -->
<style>
  body, html {
    font-family: Roboto, "Helvetica Neue", Helvetica, "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "微软雅黑", Arial, sans-serif;
    overflow-x: hidden !important;
  }
  
  code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  a {
    color: #00838F;
  }

  .mdl-card__media,
  #search-label,
  #search-form-label:after,
  #scheme-Paradox .hot_tags-count,
  #scheme-Paradox .sidebar_archives-count,
  #scheme-Paradox .sidebar-colored .sidebar-header,
  #scheme-Paradox .sidebar-colored .sidebar-badge{
    background-color: #0097A7 !important;
  }

  /* Sidebar User Drop Down Menu Text Color */
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:hover,
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:focus {
    color: #0097A7 !important;
  }

  #post_entry-right-info,
  .sidebar-colored .sidebar-nav li:hover > a,
  .sidebar-colored .sidebar-nav li:hover > a i,
  .sidebar-colored .sidebar-nav li > a:hover,
  .sidebar-colored .sidebar-nav li > a:hover i,
  .sidebar-colored .sidebar-nav li > a:focus i,
  .sidebar-colored .sidebar-nav > .open > a,
  .sidebar-colored .sidebar-nav > .open > a:hover,
  .sidebar-colored .sidebar-nav > .open > a:focus,
  #ds-reset #ds-ctx .ds-ctx-entry .ds-ctx-head a {
    color: #0097A7 !important;
  }

  .toTop {
    background: #757575 !important;
  }

  .material-layout .material-post>.material-nav,
  .material-layout .material-index>.material-nav,
  .material-nav a {
    color: #757575;
  }

  #scheme-Paradox .MD-burger-layer {
    background-color: #757575;
  }

  #scheme-Paradox #post-toc-trigger-btn {
    color: #757575;
  }

  .post-toc a:hover {
    color: #00838F;
    text-decoration: underline;
  }

</style>


<!-- Theme Background Related-->

    <style>
      body{
        background-image: url(/img/bg.png);
      }
    </style>




<!-- Fade Effect -->

    <style>
      .fade {
        transition: all 800ms linear;
        -webkit-transform: translate3d(0,0,0);
        -moz-transform: translate3d(0,0,0);
        -ms-transform: translate3d(0,0,0);
        -o-transform: translate3d(0,0,0);
        transform: translate3d(0,0,0);
        opacity: 1;
      }

      .fade.out{
        opacity: 0;
      }
    </style>


<!-- Import Font -->
<!-- Import Roboto -->

    <link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet">


<!-- Import Material Icon -->

    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">




    <!-- Import jQuery -->
    
        <script>lsloader.load("jq_js","/js/jquery.min.js?qcusAULNeBksqffqUM2+Ig==", true)</script>
    

    <!-- The Open Graph protocol -->
    <meta property="og:url" content="http://yoursite.com">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="Azurery">
    <meta property="og:image" content="http://yoursite.com/img/favicon.png" />
    <meta property="og:description" content="蒟蒻一枚">
    

    
        <meta property="article:published_time" content="Sat Nov 10 2018 21:22:36 GMT+0800" />
        <meta property="article:modified_time" content="Wed Feb 27 2019 17:35:24 GMT+0800" />
    

    <!-- The Twitter Card protocol -->
    <meta name="twitter:title" content="Azurery">
    <meta name="twitter:description" content="蒟蒻一枚">
    <meta name="twitter:image" content="http://yoursite.com/img/favicon.png">
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:url" content="http://yoursite.com" />

    <!-- Add canonical link for SEO -->
    
        <link rel="canonical" href="http://yoursite.com/2018/11/10/CNN/index.html" />
    

    <!-- Structured-data for SEO -->
    
        


<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "mainEntityOfPage": "http://yoursite.com/2018/11/10/CNN/index.html",
    "headline": "",
    "datePublished": "Sat Nov 10 2018 21:22:36 GMT+0800",
    "dateModified": "Wed Feb 27 2019 17:35:24 GMT+0800",
    "author": {
        "@type": "Person",
        "name": "Magicmanoooo",
        "image": {
            "@type": "ImageObject",
            "url": "/img/avatar.png"
        },
        "description": "秘境，探寻你的足迹"
    },
    "publisher": {
        "@type": "Organization",
        "name": "Azurery",
        "logo": {
            "@type":"ImageObject",
            "url": "/img/favicon.png"
        }
    },
    "keywords": "",
    "description": "蒟蒻一枚",
}
</script>


    

    <!-- Analytics -->
    
    
    

    <!-- Custom Head -->
    

</head>


    
        <body id="scheme-Paradox" class="lazy">
            <div class="material-layout  mdl-js-layout has-drawer is-upgraded">
                

                <!-- Main Container -->
                <main class="material-layout__content" id="main">

                    <!-- Top Anchor -->
                    <div id="top"></div>

                    
                        <!-- Hamburger Button -->
                        <button class="MD-burger-icon sidebar-toggle">
                            <span class="MD-burger-layer"></span>
                        </button>
                    

                    <!-- Post TOC -->

    
    <!-- Back Button -->
    <!--
    <div class="material-back" id="backhome-div" tabindex="0">
        <a class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon"
           href="#" onclick="window.history.back();return false;"
           target="_self"
           role="button"
           data-upgraded=",MaterialButton,MaterialRipple">
            <i class="material-icons" role="presentation">arrow_back</i>
            <span class="mdl-button__ripple-container">
                <span class="mdl-ripple"></span>
            </span>
        </a>
    </div>
    -->


    <!-- Left aligned menu below button -->
    
    
    <button id="post-toc-trigger-btn"
        class="mdl-button mdl-js-button mdl-button--icon">
        <i class="material-icons">format_list_numbered</i>
    </button>

    <ul class="post-toc-wrap mdl-menu mdl-menu--bottom-left mdl-js-menu mdl-js-ripple-effect" for="post-toc-trigger-btn" style="max-height:80vh; overflow-y:scroll;">
        <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#神经网络基础概念"><span class="post-toc-number">1.</span> <span class="post-toc-text">神经网络基础概念</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#神经网络的工作原理"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">神经网络的工作原理</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#损失函数（loss-function）"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">损失函数（loss function）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#优化器（optimizer）"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">优化器（optimizer）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#神经网络的组件"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">神经网络的组件</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#神经网络的数据表示"><span class="post-toc-number">1.5.</span> <span class="post-toc-text">神经网络的数据表示</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#张量的关键属性有以下三个关键属性来定义："><span class="post-toc-number">1.5.0.1.</span> <span class="post-toc-text">张量的关键属性有以下三个关键属性来定义：</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#梯度"><span class="post-toc-number">1.5.1.</span> <span class="post-toc-text">梯度</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#训练网络"><span class="post-toc-number">1.6.</span> <span class="post-toc-text">训练网络</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#为什么需要激活函数？"><span class="post-toc-number">1.7.</span> <span class="post-toc-text">为什么需要激活函数？</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#线性组合只能是直线"><span class="post-toc-number">1.7.1.</span> <span class="post-toc-text">线性组合只能是直线</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#交叉熵"><span class="post-toc-number">2.</span> <span class="post-toc-text">交叉熵</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#全连接层的作用"><span class="post-toc-number">3.</span> <span class="post-toc-text">全连接层的作用</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Dropout"><span class="post-toc-number">4.</span> <span class="post-toc-text">Dropout</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-Dropout-出现的缘由"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">1. Dropout 出现的缘由</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-Dropout-的概念"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">2. Dropout 的概念</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-Dropout-具体工作流程"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">3. Dropout 具体工作流程</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-Dropout-的使用"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">4. Dropout 的使用</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1-在训练模型阶段"><span class="post-toc-number">4.4.0.1.</span> <span class="post-toc-text">1. 在训练模型阶段</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-在测试模型阶段"><span class="post-toc-number">4.4.0.2.</span> <span class="post-toc-text">2. 在测试模型阶段</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-为什么Dropout-可以解决过拟合？"><span class="post-toc-number">4.5.</span> <span class="post-toc-text">5. 为什么Dropout 可以解决过拟合？</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-Dropout-在-Keras-中的源码分析"><span class="post-toc-number">4.6.</span> <span class="post-toc-text">6. Dropout 在 Keras 中的源码分析</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#基础知识"><span class="post-toc-number">5.</span> <span class="post-toc-text">基础知识</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#无监督预训练（Unsupervised-pre-training）"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">无监督预训练（Unsupervised pre-training）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#有监督预训练（Supervised-pre-training）"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">有监督预训练（Supervised pre-training）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Ground-Truth"><span class="post-toc-number">5.3.</span> <span class="post-toc-text">Ground Truth</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1-错误的数据"><span class="post-toc-number">5.3.0.1.</span> <span class="post-toc-text">1.  错误的数据</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-正确的数据"><span class="post-toc-number">5.3.0.2.</span> <span class="post-toc-text">2.  正确的数据</span></a></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#top-1-amp-top-5"><span class="post-toc-number">6.</span> <span class="post-toc-text">top-1 &amp; top-5</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#epoch"><span class="post-toc-number">7.</span> <span class="post-toc-text">epoch</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#感受野（receptive-field）"><span class="post-toc-number">8.</span> <span class="post-toc-text">感受野（receptive field）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#定义"><span class="post-toc-number">8.0.1.</span> <span class="post-toc-text">定义</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#感受野的计算和可视化"><span class="post-toc-number">8.1.</span> <span class="post-toc-text">感受野的计算和可视化</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#感受野的计算（Receptive-Field-Arithmetic）"><span class="post-toc-number">8.1.1.</span> <span class="post-toc-text">感受野的计算（Receptive Field Arithmetic）</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#例子"><span class="post-toc-number">8.2.</span> <span class="post-toc-text">例子</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#IoU"><span class="post-toc-number">8.3.</span> <span class="post-toc-text">IoU</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#非极大值抑制"><span class="post-toc-number">8.4.</span> <span class="post-toc-text">非极大值抑制</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#对梯度幅值进行非极大值抑制"><span class="post-toc-number">8.5.</span> <span class="post-toc-text">对梯度幅值进行非极大值抑制</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#非极大值抑制的工作原理："><span class="post-toc-number">8.5.0.1.</span> <span class="post-toc-text">非极大值抑制的工作原理：</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#物体检测-VS-图片分类"><span class="post-toc-number">8.6.</span> <span class="post-toc-text">物体检测 VS 图片分类</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Selective-Search-for-Object-Recognition"><span class="post-toc-number">9.</span> <span class="post-toc-text">Selective Search for Object Recognition</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#目标检测-VS-目标识别"><span class="post-toc-number">9.1.</span> <span class="post-toc-text">目标检测 VS 目标识别</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Sliding-Window-Algorithm"><span class="post-toc-number">9.2.</span> <span class="post-toc-text">Sliding Window Algorithm</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Region-Proposal-Algorithms"><span class="post-toc-number">9.3.</span> <span class="post-toc-text">Region Proposal Algorithms</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Selective-Search"><span class="post-toc-number">9.4.</span> <span class="post-toc-text">Selective Search</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#算法流程："><span class="post-toc-number">9.4.1.</span> <span class="post-toc-text">算法流程：</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#相似度计算"><span class="post-toc-number">9.4.2.</span> <span class="post-toc-text">相似度计算</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#颜色相似度（color-similarity）"><span class="post-toc-number">9.4.2.1.</span> <span class="post-toc-text">颜色相似度（color similarity）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#纹理相似度（texture-similarity）"><span class="post-toc-number">9.4.2.2.</span> <span class="post-toc-text">纹理相似度（texture similarity）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#尺寸相似度（size-similarity）"><span class="post-toc-number">9.4.2.3.</span> <span class="post-toc-text">尺寸相似度（size similarity）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#交叠相似度（shape-compatibility-measure）"><span class="post-toc-number">9.4.2.4.</span> <span class="post-toc-text">交叠相似度（shape compatibility measure）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#最终的相似度"><span class="post-toc-number">9.4.2.5.</span> <span class="post-toc-text">最终的相似度</span></a></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#池化层"><span class="post-toc-number">10.</span> <span class="post-toc-text">池化层</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#LeNet"><span class="post-toc-number">11.</span> <span class="post-toc-text">LeNet</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#第一层：卷积层"><span class="post-toc-number">11.1.</span> <span class="post-toc-text">第一层：卷积层</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#第二层：池化层"><span class="post-toc-number">11.2.</span> <span class="post-toc-text">第二层：池化层</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#第三层：卷积层"><span class="post-toc-number">11.3.</span> <span class="post-toc-text">第三层：卷积层</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#第四层：池化层"><span class="post-toc-number">11.4.</span> <span class="post-toc-text">第四层：池化层</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#第五层：全连接层"><span class="post-toc-number">11.5.</span> <span class="post-toc-text">第五层：全连接层</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#第六层：全连接层"><span class="post-toc-number">11.6.</span> <span class="post-toc-text">第六层：全连接层</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#第七层：全连接层"><span class="post-toc-number">11.7.</span> <span class="post-toc-text">第七层：全连接层</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#RCNN"><span class="post-toc-number">12.</span> <span class="post-toc-text">RCNN</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#整体过程："><span class="post-toc-number">12.1.</span> <span class="post-toc-text">整体过程：</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-算法的整体思路"><span class="post-toc-number">12.2.</span> <span class="post-toc-text">1. 算法的整体思路</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-候选框的搜索"><span class="post-toc-number">12.3.</span> <span class="post-toc-text">2. 候选框的搜索</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-各向异性缩放"><span class="post-toc-number">12.3.1.</span> <span class="post-toc-text">1. 各向异性缩放</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-各向同性缩放"><span class="post-toc-number">12.3.2.</span> <span class="post-toc-text">2. 各向同性缩放</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#非极大值抑制的具体操作"><span class="post-toc-number">12.3.3.</span> <span class="post-toc-text">非极大值抑制的具体操作</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#步骤："><span class="post-toc-number">12.3.3.1.</span> <span class="post-toc-text">步骤：</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-CNN特征提取"><span class="post-toc-number">12.4.</span> <span class="post-toc-text">3. CNN特征提取</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-网络结构设计"><span class="post-toc-number">12.4.1.</span> <span class="post-toc-text">1. 网络结构设计</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-有监督预训练"><span class="post-toc-number">12.4.2.</span> <span class="post-toc-text">2. 有监督预训练</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-fine-tuning-训练"><span class="post-toc-number">12.4.3.</span> <span class="post-toc-text">3. fine-tuning 训练</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#疑问"><span class="post-toc-number">12.4.4.</span> <span class="post-toc-text">疑问</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1-既然-CNN-都是用于提取特征，那么直接用-Alexnet-做特征提取，省去-fine-tuning-阶段可以吗？"><span class="post-toc-number">12.4.4.1.</span> <span class="post-toc-text">1. 既然 CNN 都是用于提取特征，那么直接用 Alexnet 做特征提取，省去  fine-tuning 阶段可以吗？</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-没有-fine-tuning-的时候，要选择哪一层的特征作为-CNN-提取到的特征呢？由于可以选择-p5、f6、f7，这三层的神经元个数分别是-9216、4096、4096。从-p5-到-p6-这层的参数个数是：4096-9216，从-f6-到-f7-的参数是4096-4096。那么具体是选择-p5、f6-还是-f7-呢？"><span class="post-toc-number">12.4.4.2.</span> <span class="post-toc-text">2. 没有 fine-tuning 的时候，要选择哪一层的特征作为 CNN 提取到的特征呢？由于可以选择 p5、f6、f7，这三层的神经元个数分别是 9216、4096、4096。从 p5 到 p6 这层的参数个数是：4096*9216，从 f6 到 f7 的参数是4096*4096。那么具体是选择 p5、f6 还是 f7 呢？</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-CNN-在进行训练的时候，本来就是对-bounding-box-的物体进行识别分类训练，是一个端到端的任务。在训练的最后一层-softmax-就是分类层，那么为什么要先用-CNN-做特征提取（提取-fc7层数据），然后再把提取的特征用于训练-SVM-分类器？"><span class="post-toc-number">12.4.4.3.</span> <span class="post-toc-text">3. CNN 在进行训练的时候，本来就是对 bounding box 的物体进行识别分类训练，是一个端到端的任务。在训练的最后一层 softmax 就是分类层，那么为什么要先用 CNN 做特征提取（提取 fc7层数据），然后再把提取的特征用于训练 SVM 分类器？</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#4-为什么需要回归器？"><span class="post-toc-number">12.4.4.4.</span> <span class="post-toc-text">4. 为什么需要回归器？</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#如何设计回归器（Bounding-box-regression）？"><span class="post-toc-number">12.4.4.5.</span> <span class="post-toc-text">如何设计回归器（Bounding-box regression）？</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-SVM训练"><span class="post-toc-number">12.5.</span> <span class="post-toc-text">4. SVM训练</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Faster-RCNN"><span class="post-toc-number">13.</span> <span class="post-toc-text">Faster RCNN</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-Conv-layers"><span class="post-toc-number">13.1.</span> <span class="post-toc-text">1. Conv layers</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-Region-Proposal-Networks（RPN）"><span class="post-toc-number">13.2.</span> <span class="post-toc-text">2.  Region Proposal Networks（RPN）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-1-多通道图像卷积基础知识介绍"><span class="post-toc-number">13.2.1.</span> <span class="post-toc-text">2.1 多通道图像卷积基础知识介绍</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-2-anchors"><span class="post-toc-number">13.2.2.</span> <span class="post-toc-text">2.2  anchors</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-3-softmax-判定-foreground-与-background"><span class="post-toc-number">13.2.3.</span> <span class="post-toc-text">2.3  softmax 判定 foreground 与 background</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-4-bounding-box-regression-原理"><span class="post-toc-number">13.2.4.</span> <span class="post-toc-text">2.4  bounding box regression 原理</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#目标检测中-region-proposal-的作用？"><span class="post-toc-number">14.</span> <span class="post-toc-text">目标检测中 region proposal 的作用？</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-理由一"><span class="post-toc-number">14.1.</span> <span class="post-toc-text">1.  理由一</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-理由二"><span class="post-toc-number">14.2.</span> <span class="post-toc-text">2.  理由二</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#总结"><span class="post-toc-number">14.2.1.</span> <span class="post-toc-text">总结</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#SPP-Net"><span class="post-toc-number">15.</span> <span class="post-toc-text">SPP Net</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#RCNN-的弊端"><span class="post-toc-number">15.0.1.</span> <span class="post-toc-text">RCNN 的弊端</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#为什么要固定输入图片的大小？"><span class="post-toc-number">15.0.1.1.</span> <span class="post-toc-text">为什么要固定输入图片的大小？</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#网络细节"><span class="post-toc-number">15.0.2.</span> <span class="post-toc-text">网络细节</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1-卷积层特征图"><span class="post-toc-number">15.0.2.1.</span> <span class="post-toc-text">1.  卷积层特征图</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-空间金字塔池化（Spatial-Pyramid-Pooling）"><span class="post-toc-number">15.0.2.2.</span> <span class="post-toc-text">2.  空间金字塔池化（Spatial Pyramid Pooling）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-SPP-Net应用于图像分类"><span class="post-toc-number">15.0.2.3.</span> <span class="post-toc-text">3.  SPP Net应用于图像分类</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#4-SPP-Net-应用于目标检测"><span class="post-toc-number">15.0.2.4.</span> <span class="post-toc-text">4.  SPP Net 应用于目标检测</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#5-如何从一个-region-proposal-映射到-feature-map-的位置？"><span class="post-toc-number">15.0.2.5.</span> <span class="post-toc-text">5.  如何从一个 region proposal 映射到 feature map 的位置？</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#金字塔池化的意义"><span class="post-toc-number">15.0.2.6.</span> <span class="post-toc-text">金字塔池化的意义</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#SPP-Net-amp-RCNN"><span class="post-toc-number">15.0.2.7.</span> <span class="post-toc-text">SPP Net &amp; RCNN</span></a></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#AlexNet"><span class="post-toc-number">16.</span> <span class="post-toc-text">AlexNet</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-模型结构"><span class="post-toc-number">16.1.</span> <span class="post-toc-text">1. 模型结构</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-具体结构"><span class="post-toc-number">16.2.</span> <span class="post-toc-text">2. 具体结构</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#第一层：卷积层（conv1）"><span class="post-toc-number">16.2.0.1.</span> <span class="post-toc-text">第一层：卷积层（conv1）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#第二层：卷积层（conv2）"><span class="post-toc-number">16.2.0.2.</span> <span class="post-toc-text">第二层：卷积层（conv2）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#第三层：卷积层（conv3）"><span class="post-toc-number">16.2.0.3.</span> <span class="post-toc-text">第三层：卷积层（conv3）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#第四层：卷积层（conv4）"><span class="post-toc-number">16.2.0.4.</span> <span class="post-toc-text">第四层：卷积层（conv4）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#第五层：卷积层（conv5）"><span class="post-toc-number">16.2.0.5.</span> <span class="post-toc-text">第五层：卷积层（conv5）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#第六层：全连接层（fc6）"><span class="post-toc-number">16.2.0.6.</span> <span class="post-toc-text">第六层：全连接层（fc6）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#第七层：全连接层（fc7）"><span class="post-toc-number">16.2.0.7.</span> <span class="post-toc-text">第七层：全连接层（fc7）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#第八层：全连接层（fc8）"><span class="post-toc-number">16.2.0.8.</span> <span class="post-toc-text">第八层：全连接层（fc8）</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-创新点"><span class="post-toc-number">16.3.</span> <span class="post-toc-text">3. 创新点</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1-ReLU-Nonlinearity"><span class="post-toc-number">16.3.0.1.</span> <span class="post-toc-text">1. ReLU Nonlinearity</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-双-GPU-并行运行"><span class="post-toc-number">16.3.0.2.</span> <span class="post-toc-text">2. 双 GPU 并行运行</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-LRN-局部响应归一化"><span class="post-toc-number">16.3.0.3.</span> <span class="post-toc-text">3. LRN 局部响应归一化</span></a></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#ZF-Net"><span class="post-toc-number">17.</span> <span class="post-toc-text">ZF Net</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-概述"><span class="post-toc-number">17.1.</span> <span class="post-toc-text">1.  概述</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-可视化结构"><span class="post-toc-number">17.2.</span> <span class="post-toc-text">2.  可视化结构</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-1-Unpooling"><span class="post-toc-number">17.2.1.</span> <span class="post-toc-text">2.1  Unpooling</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-2-Rectification"><span class="post-toc-number">17.2.2.</span> <span class="post-toc-text">2.2  Rectification</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-3-Filtering"><span class="post-toc-number">17.2.3.</span> <span class="post-toc-text">2.3  Filtering</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-Feature-Visualization"><span class="post-toc-number">17.3.</span> <span class="post-toc-text">3.  Feature Visualization</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-Feature-Evolution-during-Training"><span class="post-toc-number">17.4.</span> <span class="post-toc-text">4.  Feature Evolution during Training</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-Feature-Invariance"><span class="post-toc-number">17.5.</span> <span class="post-toc-text">5.  Feature Invariance</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-ZF-Net"><span class="post-toc-number">17.6.</span> <span class="post-toc-text">6.  ZF Net</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7-实验"><span class="post-toc-number">17.7.</span> <span class="post-toc-text">7.  实验</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#总结-1"><span class="post-toc-number">17.8.</span> <span class="post-toc-text">总结</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#VGG-Net"><span class="post-toc-number">18.</span> <span class="post-toc-text">VGG Net</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-概括"><span class="post-toc-number">18.1.</span> <span class="post-toc-text">1.  概括</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-创新点"><span class="post-toc-number">18.2.</span> <span class="post-toc-text">2.  创新点</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-VGG-网路结构："><span class="post-toc-number">18.3.</span> <span class="post-toc-text">3.  VGG 网路结构：</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-训练"><span class="post-toc-number">18.4.</span> <span class="post-toc-text">4. 训练</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-1-训练初始化参数"><span class="post-toc-number">18.4.1.</span> <span class="post-toc-text">4.1 训练初始化参数</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-2-训练输入"><span class="post-toc-number">18.4.2.</span> <span class="post-toc-text">4.2 训练输入</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-实验效果（CLASSIFICATION-EXPERIMENTS-）"><span class="post-toc-number">18.5.</span> <span class="post-toc-text">5. 实验效果（CLASSIFICATION EXPERIMENTS ）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#5-1-单尺度（SINGLE-SCALE-EVALUATION-）"><span class="post-toc-number">18.5.1.</span> <span class="post-toc-text">5.1 单尺度（SINGLE SCALE EVALUATION ）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#5-2-多尺度（MULTI-SCALE-EVALUATION-）"><span class="post-toc-number">18.5.2.</span> <span class="post-toc-text">5.2 多尺度（MULTI-SCALE EVALUATION ）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#5-3-多尺度裁剪（MULTI-CROP-EVALUATION-）"><span class="post-toc-number">18.5.3.</span> <span class="post-toc-text">5.3 多尺度裁剪（MULTI-CROP EVALUATION ）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#5-4-网络融合（CONVNET-FUSION）"><span class="post-toc-number">18.5.4.</span> <span class="post-toc-text">5.4 网络融合（CONVNET FUSION）</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-Q-amp-A"><span class="post-toc-number">18.6.</span> <span class="post-toc-text">6. Q&amp;A</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Q1-为什么-3-个-3x3-的卷积可以代替-7x7-的卷积？"><span class="post-toc-number">18.6.0.1.</span> <span class="post-toc-text">Q1: 为什么 3 个 3x3 的卷积可以代替 7x7 的卷积？</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Q2：2-个-3x3-卷积核可以来代替-5x5-卷积核？"><span class="post-toc-number">18.6.0.2.</span> <span class="post-toc-text">Q2：2 个 3x3 卷积核可以来代替 5x5 卷积核？</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Q3-1x1-卷积核的作用？"><span class="post-toc-number">18.6.0.3.</span> <span class="post-toc-text">Q3: 1x1 卷积核的作用？</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Q4-网络深度对结果的影响（同年-google-也独立发布了深度为-22-层的网络-GoogleNet）"><span class="post-toc-number">18.6.0.4.</span> <span class="post-toc-text">Q4: 网络深度对结果的影响（同年 google 也独立发布了深度为 22 层的网络 GoogleNet）</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7-VGG-TensorFlow-实现"><span class="post-toc-number">18.7.</span> <span class="post-toc-text">7. VGG TensorFlow 实现</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#7-1-自己实现"><span class="post-toc-number">18.7.1.</span> <span class="post-toc-text">7.1 自己实现</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#7-2-Google-官方-slim-实现"><span class="post-toc-number">18.7.2.</span> <span class="post-toc-text">7.2 Google 官方 slim 实现</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#8-模型参数计算"><span class="post-toc-number">19.</span> <span class="post-toc-text">8. 模型参数计算</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Inception-v1（GoogLeNet）"><span class="post-toc-number">20.</span> <span class="post-toc-text">Inception v1（GoogLeNet）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-简介"><span class="post-toc-number">20.1.</span> <span class="post-toc-text">1. 简介</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-研究动机"><span class="post-toc-number">20.2.</span> <span class="post-toc-text">2. 研究动机</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-网络结构"><span class="post-toc-number">20.3.</span> <span class="post-toc-text">3. 网络结构</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-核心思想"><span class="post-toc-number">20.4.</span> <span class="post-toc-text">4. 核心思想</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-1-1x1-的卷积"><span class="post-toc-number">20.4.1.</span> <span class="post-toc-text">4.1 1x1 的卷积</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-2-多个尺寸上进行卷积再聚合"><span class="post-toc-number">20.4.2.</span> <span class="post-toc-text">4.2  多个尺寸上进行卷积再聚合</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-论文关键点解析"><span class="post-toc-number">20.5.</span> <span class="post-toc-text">5. 论文关键点解析</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-论文的亮点"><span class="post-toc-number">20.6.</span> <span class="post-toc-text">6. 论文的亮点</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7-GoogLeNet-slim-实现"><span class="post-toc-number">20.7.</span> <span class="post-toc-text">7. GoogLeNet slim 实现</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Inception-v2（Batch-Normalization）"><span class="post-toc-number">21.</span> <span class="post-toc-text">Inception v2（Batch Normalization）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-提出背景"><span class="post-toc-number">21.1.</span> <span class="post-toc-text">1. 提出背景</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-1-炼丹的困扰"><span class="post-toc-number">21.1.1.</span> <span class="post-toc-text">1.1 炼丹的困扰</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-2-什么是-Internal-Covariate-Shift"><span class="post-toc-number">21.1.2.</span> <span class="post-toc-text">1.2 什么是 Internal Covariate Shift</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-3-Internal-Covariate-Shift-会带来什么问题？"><span class="post-toc-number">21.1.3.</span> <span class="post-toc-text">1.3 Internal Covariate Shift 会带来什么问题？</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-4-如何减缓-Internal-Covariate-Shift？"><span class="post-toc-number">21.1.4.</span> <span class="post-toc-text">1.4 如何减缓 Internal Covariate Shift？</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-Batch-Normalization"><span class="post-toc-number">21.2.</span> <span class="post-toc-text">2. Batch Normalization</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-1-思路"><span class="post-toc-number">21.2.1.</span> <span class="post-toc-text">2.1 思路</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-2-算法"><span class="post-toc-number">21.2.2.</span> <span class="post-toc-text">2.2 算法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-2-1-参数定义"><span class="post-toc-number">21.2.2.1.</span> <span class="post-toc-text">2.2.1 参数定义</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-2-2-算法步骤"><span class="post-toc-number">21.2.2.2.</span> <span class="post-toc-text">2.2.2 算法步骤</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-2-3-公式"><span class="post-toc-number">21.2.2.3.</span> <span class="post-toc-text">2.2.3 公式</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-测试阶段如何使用-Batch-Normalization？"><span class="post-toc-number">21.3.</span> <span class="post-toc-text">3. 测试阶段如何使用 Batch Normalization？</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-Batch-Normalization-的优势"><span class="post-toc-number">21.4.</span> <span class="post-toc-text">4. Batch Normalization 的优势</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-实验"><span class="post-toc-number">21.5.</span> <span class="post-toc-text">5. 实验</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#为什么卷积层可以“代替”全连接层？"><span class="post-toc-number">22.</span> <span class="post-toc-text">为什么卷积层可以“代替”全连接层？</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#为什么不能用卷积层代替全连接层的方式，使得模型处理不同大小的输入？"><span class="post-toc-number">23.</span> <span class="post-toc-text">为什么不能用卷积层代替全连接层的方式，使得模型处理不同大小的输入？</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#全卷积神经网络Fully-Convolutional-Network-FCN"><span class="post-toc-number">24.</span> <span class="post-toc-text">全卷积神经网络Fully Convolutional Network (FCN)</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Yolo-v1"><span class="post-toc-number">25.</span> <span class="post-toc-text">Yolo v1</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-滑动窗口与-CNN"><span class="post-toc-number">25.1.</span> <span class="post-toc-text">1. 滑动窗口与 CNN</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-设计理念"><span class="post-toc-number">25.2.</span> <span class="post-toc-text">2.  设计理念</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-网络设计"><span class="post-toc-number">25.3.</span> <span class="post-toc-text">3.  网络设计</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#DL-中的-GPU-和显存分析"><span class="post-toc-number">26.</span> <span class="post-toc-text">DL 中的 GPU 和显存分析</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-基础知识"><span class="post-toc-number">26.1.</span> <span class="post-toc-text">1. 基础知识</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-显存分析"><span class="post-toc-number">26.2.</span> <span class="post-toc-text">2. 显存分析</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-1-存储指标"><span class="post-toc-number">26.2.1.</span> <span class="post-toc-text">2.1 存储指标</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-2-神经网络显存占用"><span class="post-toc-number">26.2.2.</span> <span class="post-toc-text">2.2 神经网络显存占用</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-2-1-参数的显存占用"><span class="post-toc-number">26.2.2.1.</span> <span class="post-toc-text">2.2.1 参数的显存占用</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-2-2-梯度与动量的显存占用"><span class="post-toc-number">26.2.2.2.</span> <span class="post-toc-text">2.2.2 梯度与动量的显存占用</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-2-3-输入输出的显存占用"><span class="post-toc-number">26.2.2.3.</span> <span class="post-toc-text">2.2.3 输入输出的显存占用</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-3-节约显存的方法"><span class="post-toc-number">26.2.3.</span> <span class="post-toc-text">2.3 节约显存的方法</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-计算量分析"><span class="post-toc-number">26.3.</span> <span class="post-toc-text">3. 计算量分析</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-1-常用操作的计算量"><span class="post-toc-number">26.3.1.</span> <span class="post-toc-text">3.1 常用操作的计算量</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-2-AlexNet-的显存占用"><span class="post-toc-number">26.3.2.</span> <span class="post-toc-text">3.2 AlexNet 的显存占用</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-3-减少卷积层的计算量"><span class="post-toc-number">26.3.3.</span> <span class="post-toc-text">3.3 减少卷积层的计算量</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-4-常用模型的比较（显存-计算复杂度-准确率）"><span class="post-toc-number">26.3.4.</span> <span class="post-toc-text">3.4 常用模型的比较（显存/计算复杂度/准确率）</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-总结"><span class="post-toc-number">26.4.</span> <span class="post-toc-text">4. 总结</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-1-建议"><span class="post-toc-number">26.4.1.</span> <span class="post-toc-text">4.1 建议</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-2-显卡的选购"><span class="post-toc-number">26.4.2.</span> <span class="post-toc-text">4.2 显卡的选购</span></a></li></ol></li></ol></li></ol>
    </ul>
    




<!-- Layouts -->

    <!-- Post Module -->
    <div class="material-post_container">

        <div class="material-post mdl-grid">
            <div class="mdl-card mdl-shadow--4dp mdl-cell mdl-cell--12-col">

                <!-- Post Header(Thumbnail & Title) -->
                
    <!-- Paradox Post Header -->
    
        
            <!-- Random Thumbnail -->
            <div class="post_thumbnail-random mdl-card__media mdl-color-text--grey-50">
            <script type="text/ls-javascript" id="post-thumbnail-script">
    var randomNum = Math.floor(Math.random() * 19 + 1);

    $('.post_thumbnail-random').attr('data-original', '/img/random/material-' + randomNum + '.png');
    $('.post_thumbnail-random').addClass('lazy');
</script>

        
    
            <p class="article-headline-p">
                
            </p>
        </div>





                
                    <!-- Paradox Post Info -->
                    <div class="mdl-color-text--grey-700 mdl-card__supporting-text meta">

    <!-- Author Avatar -->
    <div id="author-avatar">
        <img src="/img/avatar.png" width="44px" height="44px" alt="Author Avatar"/>
    </div>
    <!-- Author Name & Date -->
    <div>
        <strong>Magicmanoooo</strong>
        <span>11月 10, 2018</span>
    </div>

    <div class="section-spacer"></div>

    <!-- Favorite -->
    <!--
        <button id="article-functions-like-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon btn-like">
            <i class="material-icons" role="presentation">favorite</i>
            <span class="visuallyhidden">favorites</span>
        </button>
    -->

    <!-- Qrcode -->
    
        <button id="article-functions-qrcode-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
    <i class="material-icons" role="presentation">devices other</i>
    <span class="visuallyhidden">devices other</span>
</button>
<ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-functions-qrcode-button">
    <li class="mdl-menu__item">在其它设备中阅读本文章</li>
    
        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACaUlEQVR42u3aQYrDMAwF0N7/0jPbgSHulxS7obysSgmxnxe2+dLr56ufFx4eHh4eHh4e3sN4r/j5//7Vd/7+czmJ5Si9ueHh4eGd5L3ZaoOJJsj1klXHvXoHDw8P7ySvOnx1018fM1cHQ3VueHh4eE/jVbf1q6mvL+h4eHh438pbb+X5P8mbeHh4eM/k5XFDMnwy3cdlLXh4eHgxrxofnPz9gfoeHh4e3pJXbmkqRrq9SY9miIeHh7eZ14tQJ81SvWOpEH/g4eHhHeflLVbrf96Era3WqxvaVfHw8PDGvGSwfKK9GLfaavBmOfDw8PA28/a1Q62PloTaC0fw8PDwTvKSyCA/DKrgvBjWjHHx8PDwbuL1qNVrd3KETK7ReHh4eCd5eXNVHhbcFeP2mgzw8PDwzvOSDbrXXlD9Qh6O4OHh4Z3nTa7C1VC497XCFRwPDw/vCG8ev84XorfQUQEMDw8P71beZFufBLvVC3dhCfDw8PA28/LyUrXAf1djwXo+5aYrPDw8vA28efw6bwJIlgMPDw/vCbxqwakaH+RFr1FrFx4eHt5m3jyWrW7f1e9U2wjw8PDwTvKSy24+/OTiPmpTwMPDw9vM623B+eR671Tbs/Dw8PBO8nplp+qBUY2DRxduPDw8vM286qbcK5JVY9885iife3h4eHg38eaHQb7R5+WuajMBHh4e3nleXsivtk/lyzePJPDw8PCexksOgySkmB8Y5Ss1Hh4e3kd5veGrU7+tvoeHh4e3jTe5FvcuxL2AGA8PD+8JvGoBrHdRnrRYJYcTHh4e3hne9z14eHh4eHh4eHgPeH4BZRK/g8ZDPWwAAAAASUVORK5CYII=">
    
</ul>

    

    <!-- Tags (bookmark) -->
    

    <!-- Share -->
    <button id="article-fuctions-share-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
    <i class="material-icons" role="presentation">share</i>
    <span class="visuallyhidden">share</span>
</button>
<ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-fuctions-share-button">
    

    

    <!-- Share Weibo -->
    
        <a class="post_share-link" href="http://service.weibo.com/share/share.php?appkey=&title=&url=http://yoursite.com/2018/11/10/CNN/index.html&pic=http://yoursite.com/img/favicon.png&searchPic=false&style=simple" target="_blank">
            <li class="mdl-menu__item">
                分享到微博
            </li>
        </a>
    

    <!-- Share Twitter -->
    
        <a class="post_share-link" href="https://twitter.com/intent/tweet?text=&url=http://yoursite.com/2018/11/10/CNN/index.html&via=Magicmanoooo" target="_blank">
            <li class="mdl-menu__item">
                分享到 Twitter
            </li>
        </a>
    

    <!-- Share Facebook -->
    
        <a class="post_share-link" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2018/11/10/CNN/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 Facebook
            </li>
        </a>
    

    <!-- Share Google+ -->
    
        <a class="post_share-link" href="https://plus.google.com/share?url=http://yoursite.com/2018/11/10/CNN/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 Google+
            </li>
        </a>
    

    <!-- Share LinkedIn -->
    

    <!-- Share QQ -->
    
        <a class="post_share-link" href="http://connect.qq.com/widget/shareqq/index.html?site=Azurery&title=&summary=蒟蒻一枚&pics=http://yoursite.com/img/favicon.png&url=http://yoursite.com/2018/11/10/CNN/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 QQ
            </li>
        </a>
    

    <!-- Share Telegram -->
    
</ul>

</div>

                

                <!-- Post Content -->
                <div id="post-content" class="mdl-color-text--grey-700 mdl-card__supporting-text fade out">
    
        <p>title: CNN<br>tags: DL</p>
<p>深度学习以数据的原始形态（raw data）作为算法的输入，经过算法的层层抽象，将原始数据逐层抽象为自身任务所需的最终特种表示，最后以特征到任务的目标的映射（mapping）作为结束，从原始数据到最终任务目标，一气呵成而中间并无夹杂任何人为操作。相比于传统的 ML 算法仅学得模型这一单一任务模块而言，DL 除了模型学习，还有特征学习、特征抽象等任务模块的参与，借助多层任务模块完成最终的学习任务。</p>
<h1 id="神经网络基础概念"><a href="#神经网络基础概念" class="headerlink" title="神经网络基础概念"></a>神经网络基础概念</h1><h2 id="神经网络的工作原理"><a href="#神经网络的工作原理" class="headerlink" title="神经网络的工作原理"></a>神经网络的工作原理</h2><p>神经网络中每层对输入数据所做的具体操作保存在该层的权重（ weight）中，其本质是一串数字。用术语来说，每层实现的变换由其权重来参数化（ parameterize）。权重有时也被称为该层的参数（ parameter）。在这种语境下，<strong>学习的意思是为神经网络的所有层找到一组权重值，使得该网络能够将每个示例输入与其目标正确地一一对应</strong>。</p>
<p>一个深度神经网络可能包含数千万个参数。找到所有参数的正确取值可能是一项非常艰巨的任务，特别是考虑到修改某个参数值将会影响其他所有参数的行为。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-19d814df9cde4e2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="损失函数（loss-function）"><a href="#损失函数（loss-function）" class="headerlink" title="损失函数（loss function）"></a>损失函数（loss function）</h2><p><strong>想要控制神经网络的输出，就需要能够衡量该输出与预期值之间的距离</strong>。这是神经网络损失函数（ loss function）的任务，该函数也叫目标函数（ objective function）。<strong>损失函数的输入是网络预测值与真实目标值（即希望网络输出的结果），然后计算一个距离值，衡量该网络在这个示例上的效果好坏</strong>。简而言之，损失函数用于网络如何衡量在训练数据上的性能，即网络如何朝着正确的方向前进。在训练过程中需要将其最小化。它能够衡量当前任务是否已成功完成。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-652baf565ef24a2b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="优化器（optimizer）"><a href="#优化器（optimizer）" class="headerlink" title="优化器（optimizer）"></a>优化器（optimizer）</h2><p>深度学习的基本技巧是：<strong>利用之前的距离值作为反馈信号来对权重值进行微调（fine tuning），以降低当前示例对应的损失值。这种调节由优化器（ optimizer）来完成，它实现了所谓的反向传播（ backpropagation）算法，这是深度学习的核心算法</strong>。其基于训练数据和损失函数来更新网络的机制。决定如何基于损失函数对网络进行更新。它执行的是随机梯度下降（ SGD）的某个变体。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-b4b7fa5551d1d28f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="神经网络的组件"><a href="#神经网络的组件" class="headerlink" title="神经网络的组件"></a>神经网络的组件</h2><p><strong>神经网络的核心组件是层（ layer），它是一种数据处理模块，可以将它看成数据 filter </strong>。进去一些数据，出来的数据变得更加有用。具体来说，层从输入数据中提取表示。大多数深度学习都是将简单的层链接起来，从而实现渐进式的数据蒸馏（ data distillation）。深度学习模型就像是数据处理的筛子，包含一系列越来越精细的数据 filter （即层）。有些层是无状态的，但大多数的层是有状态的，即层的权重。<strong>权重是利用随机梯度下降学到的一个或多个张量，其中包含网络的知识</strong>。</p>
<h2 id="神经网络的数据表示"><a href="#神经网络的数据表示" class="headerlink" title="神经网络的数据表示"></a>神经网络的数据表示</h2><p>张量的核心在于，它是一个<strong>数据容器</strong>。它包含的数据几乎总是数值数据，因此它是数字的容器。</p>
<h4 id="张量的关键属性有以下三个关键属性来定义："><a href="#张量的关键属性有以下三个关键属性来定义：" class="headerlink" title="张量的关键属性有以下三个关键属性来定义："></a>张量的关键属性有以下三个关键属性来定义：</h4><ol>
<li>轴的个数（阶）</li>
<li>形状，它是一个整数元祖，表示张量沿每个轴的维度大小（元素大小）</li>
<li>数据类型（在Python库中通常叫做<code>dtype</code>）</li>
</ol>
<h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>每个神经层都用下述方法对输入数据进行变换。</p>
<pre><code class="python">output = relu(dot(W, input) + b)
</code></pre>
<p><code>W</code>和<code>b</code>都是张量，均为该层的属性。它们被称为该层的权重（ weight）或可训练参数（ trainable parameter），一开始，这些权重矩阵取较小的随机值，这一步叫作随机初始化（ random initialization）。<code>W</code>和<code>b</code>都是随机的，<code>relu(dot(W, input) + b)</code>肯定不会得到任何有用的表示。虽然得到的表示是没有意义的，但这是一个起点。下一步则是根据反馈信号逐渐调节这些权重。这个逐渐调节的过程叫作<strong>训练</strong>，也就是机器学习中的<strong>学习</strong>。</p>
<p>上述过程发生在一个训练循环（ training loop）内，其具体过程如下（必要时一直重复这些步骤）：</p>
<ol>
<li>抽取训练样本<code>x</code>和对应目标<code>y</code>组成的数据批量</li>
<li>在<code>x</code>上运行网络（这一步叫作<strong>前向传播， forward pass</strong>），得到预测值<code>y_pred</code></li>
<li>计算网络在这批数据上的损失，用于衡量<code>y_pred</code>和<code>y</code>之间的距离</li>
<li>更新网络的所有权重，使网络在这批数据上的损失略微下降</li>
</ol>
<p>最终得到的网络在训练数据上的损失非常小，即预测值<code>y_pred</code>和预期目标<code>y</code>之间的距离非常小。网络“学会”将输入映射到正确的目标。</p>
<p>计算损失相对于网络系数的梯度（ gradient），然后向梯度的反方向改变系数，从而使损失降低。</p>
<h2 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h2><p>一开始对神经网络的权重随机赋值，因此网络只是实现了一系列随机变换。其输出结果自然也和理想值相去甚远，相应地，损失值也很高。但随着网络处理的示例越来越多，权重值也在向正确的方向逐步微调，损失值也逐渐降低。这就是<strong>训练循环（ training loop）</strong>，将这种循环重复足够多的次数（通常对数千个示例进行数十次迭代），得到的权重值可以使损失函数最小。具有最小损失的网络，其输出值与目标值尽可能地接近，这就是训练好的网络。</p>
<p>典型的 Keras 工作流程：</p>
<ol>
<li>定义训练数据：输入张量和目标张量。</li>
<li>定义层组成的网络（或模型），将输入映射到目标。</li>
<li>配置学习过程：选择损失函数、优化器和需要监控的指标。</li>
<li>调用模型的 <code>fit</code> 方法在训练数据上进行迭代。</li>
</ol>
<p>Keras定义模型有两种方法：</p>
<ul>
<li>使用 <code>Sequential</code> 类（仅用于层的线性堆叠，这是目前最常见的网络架构）</li>
<li>函数式 <code>API</code>（functional API，用于层组成的有向无环图，可以构建任意形式的架构）。</li>
</ul>
<h2 id="为什么需要激活函数？"><a href="#为什么需要激活函数？" class="headerlink" title="为什么需要激活函数？"></a>为什么需要激活函数？</h2><p>模型分类：</p>
<ul>
<li>回归模型：预测连续值，是多少的问题。例如，房价是多少？</li>
<li>分类模型：预测离散值，是不是的问题。例如，这只动物是不是狗？</li>
</ul>
<p>卷积层就是我们所做的一大堆特定的滤波器，该滤波器会对某些特定的特征进行强烈的响应，一般情况下是结果值非常大。而对一些无关特性，其响应很小，大部分是结果相对较小，或者几乎为 <code>0</code>。</p>
<p>这样就可以看做为激活，当特定卷积核发现特定特征时，就会做出响应，输出大的数值，而响应函数的存在把输出归为 <code>0~1</code>，那么大的数值就接近 <code>1</code>，小的数值就接近 <code>0</code>。因此在最后计算每种可能所占比重时，自然大的数值比重大。</p>
<p>对于分类问题，画一条直线，这个问题还是比较简单，一条直线解决不了两条就可以了。 </p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-5df4d99bde230f65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-782e86daf2859c11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这就是一个没有激活函数的网络，可以看出该网络是 <code>x1</code> 和 <code>x2</code> 的线性组合。</p>
<h3 id="线性组合只能是直线"><a href="#线性组合只能是直线" class="headerlink" title="线性组合只能是直线"></a>线性组合只能是直线</h3><p><img src="https://upload-images.jianshu.io/upload_images/1351548-fc050203e463932d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>再加一层变为：<img src="https://upload-images.jianshu.io/upload_images/1351548-26835092163ca6e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">。拆开后，结果还是线性的，这样就严重影响了分类的效果，这样根本无法解决非线性问题。</p>
<p>神经网络的激活函数其实是：将线性转化为非线性的一个函数，并非只是简单地给予 <code>0</code>，或者给予 <code>1</code>。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-32dfb94ae27e7905.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-302b87bd7d560649.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>总而言之，如果不用激活函数，多层神经网络和一层神经网络就没什么区别了。经过多层神经网络的加权计算，都可以展开成一次的加权计算。</p>
<h1 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h1><p>刻画了两个概率分布之间的距离，它是分类问题中使用比较广的一种损失函数。给定两个概率分布 <code>p</code> 和 <code>q</code> ， 通过 <code>q</code> 来表示 <code>p</code> 的交叉熵为 ：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-d65fdc08cb1301b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>交叉熵刻画的是两个概率分布之间的距离 ， 然而神经网络的输出却不一定是一个概率分布。概率分布刻画了不同事件发生的概率。当事件总数有限的情况下 ，概率分布函数 <img src="https://upload-images.jianshu.io/upload_images/1351548-aef4ae4834de9636.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""> 满足 ：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-4aba17810c6147af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>也就是说，任意事件发生的概率都在 <code>0</code> 和 <code>1</code> 之间，且总有某一个事件发生 （概率的和为 <code>1</code> ）。如果将分类问题中“ 一个样例属于某一个类别”看成一个概率事件，那么训练数据的正确答案就符合一个概率分布。因为事件“一个样例属于不正确的类别”的概率为 <code>1</code>，而“ 一个样例属于正确的类别”的概率为 <code>1</code> 。如何将神经网络前向传播得到的结果也变成概率分布呢？ <code>Softmax</code> 回归就是一个非常常用的方法 。</p>
<p>在 TensorFlow 中，<code>Softmax</code> 回归的参数被去掉了，它只是一层额外的处理层，将神经网络的输出变成一个概率分布 。假设原始的神经网络输出为<img src="https://upload-images.jianshu.io/upload_images/1351548-b44113b7e9413800.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">，那么经过 <code>Softmax</code> 回归处理之后的输出为 ：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-7c98927906272ad0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>从以上公式中可以看出，原始神经网络的输出被用作置信度来生成新的输出，而新的输出满足概率分布的所有要求。这个新的输出可以理解为经过神经网络的推导，一个样例为不同类别的概率分别是多大。这样就把神经网络的输出也变成了一个概率分布，从而可以通过交叉熵来计算预测的概率分布和真实答案的概率分布之间的距离了。</p>
<p>从交叉熵的公式中可以看到，交叉熵函数不是对称（<img src="https://upload-images.jianshu.io/upload_images/1351548-0221c1bbb69f4c5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">），它刻画的是通过概率分布 <code>q</code> 来表达概率分布 <code>p</code> 的困难程度。因为正确答案是希望得到的结果，所以当交叉熵作为神经网络的损失函数时，<code>p</code> 代表的是正确答案，<code>q</code> 代表的是预测值。交叉熵刻画的是两个概率分布的距离，也就是说交叉熵值越小，两个概率分布越接近。</p>
<h1 id="全连接层的作用"><a href="#全连接层的作用" class="headerlink" title="全连接层的作用"></a>全连接层的作用</h1><ul>
<li>全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为 <code>1x1</code> 的卷积；而前层是卷积层的全连接层可以转化为卷积核为 <code>hxw</code> 的全局卷积，<code>h</code> 和 <code>w</code> 分别为前层卷积结果的高和宽。</li>
<li>目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数 <code>80%</code> 左右），近期一些性能优异的网络模型如 ResNet 和 GoogLeNet 等均用全局平均池化（global average pooling，GAP）取代 FC 来融合学到的深度特征，最后仍用 softmax 等损失函数作为网络目标函数来指导学习过程。需要指出的是，用 GAP 替代 FC 的网络通常有较好的预测性能。</li>
<li>在 FC 越来越不被看好的当下，我们近期的研究（<a href="http://link.zhihu.com/?target=https%3A//cs.nju.edu.cn/wujx/paper/PCM2017_FC.pdf" target="_blank" rel="noopener">In Defense of Fully Connected Layers in Visual Representation Transfer</a>）发现，FC 可在模型表示能力迁移过程中充当“防火墙”的作用。具体来讲，假设在 ImageNet 上预训练得到的模型为<img src="http://www.zhihu.com/equation?tex=%5Cmathcal%7BM%7D" alt="\mathcal{M}"> ，则 ImageNet 可视为源域（迁移学习中的 source domain）。微调（fine tuning）是深度学习领域最常用的迁移学习技术。针对微调，若目标域（target domain）中的图像与源域中图像差异巨大（如相比 ImageNet，目标域图像不是物体为中心的图像，而是风景照，见下图），不含 FC 的网络微调后的结果要差于含 FC 的网络。因此，FC 可视作模型表示能力的“防火墙”，特别是在源域与目标域差异较大的情况下，FC 可保持较大的模型 capacity 从而保证模型表示能力的迁移（冗余的参数并不一无是处）。</li>
</ul>
<p><img src="https://pic1.zhimg.com/v2-523de4ca9fa2b92d181bbdb81fe7d3f0_b.jpg" alt="img"><img src="https://pic1.zhimg.com/80/v2-523de4ca9fa2b92d181bbdb81fe7d3f0_hd.jpg" alt="img"></p>
<p><strong>【注】</strong> 有关卷积操作“实现”全连接层，需要注意：以 VGG-16 为例，对 <code>224x224x3</code> 的输入，最后一层卷积可得输出为 <code>7x7x512</code>，如后层是一层含 <code>4096</code> 个神经元的 FC，则可用卷积核为 <code>7x7x512x4096</code> 的全局卷积来实现这一全连接运算过程，其中该卷积核参数如下：</p>
<pre><code class="python">filter_size = 7, 
padding = 0, 
stride = 1, 
D_in = 512, 
D_out = 4096
</code></pre>
<p>经过此卷积操作后可得输出为 <code>1x1x4096</code>。如需再次叠加一个 <code>2048</code> 的 FC，则可设定卷积层操作参数为：</p>
<pre><code class="python">filter_size = 1, 
padding = 0, 
stride = 1, 
D_in = 4096, 
D_out = 2048
</code></pre>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a><code>Dropout</code></h1><h2 id="1-Dropout-出现的缘由"><a href="#1-Dropout-出现的缘由" class="headerlink" title="1. Dropout 出现的缘由"></a>1. <code>Dropout</code> 出现的缘由</h2><p>在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。</p>
<p>过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。</p>
<p>总而言之，训练深度神经网络的时候，总是会遇到两大缺点：</p>
<ul>
<li>容易过拟合</li>
<li>费时</li>
</ul>
<p><code>Dropout</code> 可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。</p>
<h2 id="2-Dropout-的概念"><a href="#2-Dropout-的概念" class="headerlink" title="2. Dropout 的概念"></a>2. <code>Dropout</code> 的概念</h2><p>在 2012 年，<em>Hinton</em> 在其论文《<em>Improving neural networks by preventing co-adaptation of feature detectors</em>》中提出 <code>Dropout</code>。当一个复杂的前馈神经网络被训练在小的数据集时，容易造成过拟合。为了防止过拟合，可以通过阻止特征检测器的共同作用来提高神经网络的性能。</p>
<p><code>Dropout</code> 可以作为训练深度神经网络的一种 trick 供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为 <code>0</code>），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。</p>
<p>简而言之，<code>Dropout</code> 就是在前向传播的时候，让某个神经元的激活值以一定的概率 <code>p</code> 停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如下图所示。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-8870250e1e7f84f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3-Dropout-具体工作流程"><a href="#3-Dropout-具体工作流程" class="headerlink" title="3. Dropout 具体工作流程"></a>3. <code>Dropout</code> 具体工作流程</h2><p>假设要训练这样一个神经网络：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-84b4cf8c8881db40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>输入是 <code>x</code>，输出是 <code>y</code>，正常的流程是：首先把 <code>x</code> 通过网络前向传播，然后把误差反向传播以决定如何更新参数让网络进行学习。使用 <code>Dropout</code> 之后，过程变成如下：</p>
<ol>
<li><p>首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（下图中虚线为部分临时被删除的神经元）</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-48f78862458547db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
</li>
<li><p>然后把输入 <code>x</code> 通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数 <code>(w, b)</code>。</p>
</li>
<li><p>然后继续重复以下过程：</p>
<ol>
<li><p>恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）</p>
</li>
<li><p>从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。</p>
</li>
<li><p>对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数<code>(w, b)</code> （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。</p>
</li>
</ol>
</li>
</ol>
<p>不断重复这一过程。</p>
<h2 id="4-Dropout-的使用"><a href="#4-Dropout-的使用" class="headerlink" title="4. Dropout 的使用"></a>4. <code>Dropout</code> 的使用</h2><p><code>Dropout</code> 具体怎么让某些神经元以一定的概率停止工作（就是被删除掉）？代码层面如何实现呢？</p>
<p><code>Dropout</code> 代码层面的一些公式推导及代码实现思路。</p>
<h4 id="1-在训练模型阶段"><a href="#1-在训练模型阶段" class="headerlink" title="1. 在训练模型阶段"></a>1. 在训练模型阶段</h4><p>无可避免的，在训练网络的每个单元都要添加一个概率流程。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-34f2a6455fe9745d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>现在描述 <code>dropout</code> 神经网络模型，其中有 <code>L</code> 层隐藏层，隐藏层索引为<img src="https://upload-images.jianshu.io/upload_images/1351548-15f0aafde8e5c982.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">。<img src="https://upload-images.jianshu.io/upload_images/1351548-4f2bb9114cfcc3e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">表示第<img src="https://upload-images.jianshu.io/upload_images/1351548-29499d0ec3799f4a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">层隐藏层的输入向量， <img src="https://upload-images.jianshu.io/upload_images/1351548-13ef3ac1c619bd91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">表示第<img src="https://upload-images.jianshu.io/upload_images/1351548-29499d0ec3799f4a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">层隐藏层的输出向量（<img src="https://upload-images.jianshu.io/upload_images/1351548-6a36c02a3b0be0fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">表示输入）。<img src="https://upload-images.jianshu.io/upload_images/1351548-37a6899d8d68a218.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">和<img src="https://upload-images.jianshu.io/upload_images/1351548-f182a13ab3dc7563.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">分别表示第<img src="https://upload-images.jianshu.io/upload_images/1351548-29499d0ec3799f4a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">层隐藏层的权重与偏置值，<img src="https://upload-images.jianshu.io/upload_images/1351548-8add07a883a36714.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">表示激活函数。</p>
<p>对应的公式变化如下：</p>
<ul>
<li><p>没有 <code>Dropout</code> 的网络计算公式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-cbdd98802363e79f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
</li>
<li><p>采用 <code>Dropout</code> 的网络计算公式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-d7b5af5489261e1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面公式中，<em>Bernoulli</em> 函数是为了生成概率 <code>r</code> 向量，也就是随机生成一个 <code>0</code>、<code>1</code> 的向量。</p>
</li>
</ul>
<p>代码层面实现让某个神经元以概率 <code>p</code> 停止工作，其实就是让它的激活函数值的从概率 <code>p</code> 变为 <code>0</code>。比如某一层网络神经元的个数为 <code>1000</code> 个，其激活函数输出值为 <code>y1</code>、<code>y2</code>、<code>y3</code>、<code>...</code>、<code>y1000</code>，<code>dropout</code>比率选择 <code>0.4</code>，那么这一层神经元经过 <code>dropout</code> 后，<code>1000</code> 个神经元中会有大约 <code>400</code> 个的值被置为 <code>0</code>。</p>
<p><strong>注意：</strong> 经过上面屏蔽掉某些神经元，使其激活值为 <code>0</code> 以后，还需要对向量 <code>y1</code>、<code>y2</code>、<code>y3</code>、<code>...</code>、<code>y1000</code> 进行缩放，也就是乘以 <code>1/(1-p)</code>。如果在训练的时候，经过置 <code>0</code> 后，没有对 <code>y1</code>、<code>y2</code>、<code>y3</code>、<code>...</code>、<code>y1000</code> 进行缩放（rescale），那么在测试的时候，就需要对权重进行缩放，操作如下。</p>
<h4 id="2-在测试模型阶段"><a href="#2-在测试模型阶段" class="headerlink" title="2. 在测试模型阶段"></a>2. 在测试模型阶段</h4><p>预测模型的时候，每一个神经单元的权重参数要乘以概率 <code>p</code>。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-1b734a8afb4847b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>测试阶段 <code>dropout</code> 公式：</p>
<p><img src="C:\Users\Magicmanoooo\AppData\Roaming\Typora\typora-user-images\1549420022775.png" alt="1549420022775"></p>
<h2 id="5-为什么Dropout-可以解决过拟合？"><a href="#5-为什么Dropout-可以解决过拟合？" class="headerlink" title="5. 为什么Dropout 可以解决过拟合？"></a>5. 为什么<code>Dropout</code> 可以解决过拟合？</h2><ol>
<li><strong>取平均的作用：</strong> 先回到标准的模型（即没有 <code>dropout</code>），用相同的训练数据去训练 <code>5</code> 个不同的神经网络，一般会得到 <code>5</code> 个不同的结果，此时可以采用 “<code>5</code> 个结果取均值”或者“多数取胜的投票策略“去决定最终结果。例如，<code>3</code> 个网络判断结果为数字 <code>9</code>，那么很有可能真正的结果就是数字 <code>9</code>，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。<code>dropout</code> 掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个 <code>dropout</code> 过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</li>
<li><strong>减少神经元之间复杂的共适应关系：</strong> 因为 <code>dropout</code> 程序导致两个神经元不一定每次都在一个 <code>dropout</code> 网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说，假如神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看 <code>dropout</code> 就有点像 <code>L1</code>，<code>L2</code> 正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。</li>
</ol>
<h2 id="6-Dropout-在-Keras-中的源码分析"><a href="#6-Dropout-在-Keras-中的源码分析" class="headerlink" title="6. Dropout 在 Keras 中的源码分析"></a>6. <code>Dropout</code> 在 Keras 中的源码分析</h2><pre><code class="python">def dropout(x, level, noise_shape=None, seed=None):
    &quot;&quot;&quot;Sets entries in `x` to zero at random,
    while scaling the entire tensor.
    # Arguments
        x: tensor
        level: fraction of the entries in the tensor
            that will be set to 0.
        noise_shape: shape for randomly generated keep/drop flags,
            must be broadcastable to the shape of `x`
        seed: random seed to ensure determinism.
    &quot;&quot;&quot;
    if level &lt; 0. or level &gt;= 1:
        raise ValueError(&#39;Dropout level must be in interval [0, 1[.&#39;)
    if seed is None:
        seed = np.random.randint(1, 10e6)
    if isinstance(noise_shape, list):
        noise_shape = tuple(noise_shape)

    rng = RandomStreams(seed=seed)
    retain_prob = 1. - level

    if noise_shape is None:
        random_tensor = rng.binomial(x.shape, p=retain_prob, dtype=x.dtype)
    else:
        random_tensor = rng.binomial(noise_shape, p=retain_prob, dtype=x.dtype)
        random_tensor = T.patternbroadcast(random_tensor,
                                           [dim == 1 for dim in noise_shape])
    x *= random_tensor
    x /= retain_prob
    return x
</code></pre>
<p>对 Keras 中 <code>dropout</code> 实现函数做一些修改，让 <code>dropout</code> 函数可以单独运行（函数中，<code>x</code> 是本层网络的激活值，<code>level</code> 就是 <code>dropout</code> 每个神经元要被丢弃的概率。）。</p>
<pre><code class="python">import numpy as np

# dropout函数的实现
def dropout(x, level):
    #level是概率值，必须在0~1之间
    if level &lt; 0. or level &gt;= 1: 
        raise ValueError(&#39;Dropout level must be in interval [0, 1[.&#39;)
    retain_prob = 1. - level

    # 通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，可以把每个神经元当做抛硬币一样
    # 硬币 正面的概率为p，n表示每个神经元试验的次数
    # 因为每个神经元只需要抛一次就可以了，所以n=1，size参数是有多少个硬币。
    random_tensor = np.random.binomial(n=1, p=retain_prob, size=x.shape) #即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了
    print(random_tensor)

    x *= random_tensor
    print(x)
    x /= retain_prob

    return x

#对dropout的测试，大家可以跑一下上面的函数，了解一个输入x向量，经过dropout的结果  
x=np.asarray([1,2,3,4,5,6,7,8,9,10],dtype=np.float32)
dropout(x,0.4)

# [1 0 0 0 0 1 1 1 1 0]
# [1. 0. 0. 0. 0. 6. 7. 8. 9. 0.]

# [1 1 1 0 1 1 1 0 0 1]
# [ 1.  2.  3.  0.  5.  6.  7.  0.  0. 10.]
</code></pre>
<p><strong>注意：</strong> Keras 中 <code>dropout</code> 的实现，是屏蔽掉某些神经元，使其激活值为 <code>0</code> 以后，对激活值向量 <code>x1</code>、<code>...</code>、<code>x1000</code> 进行放大，也就是乘以 <code>1/(1-p)</code>。</p>
<p>在训练的时候会随机的丢弃一些神经元，但是预测的时候就没办法随机丢弃了。如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出 <code>a</code> ，有时候输出 <code>b</code>，结果不稳定，这是实际系统不能接受的，用户可能认为模型预测不准。那么一种”补偿“的方案就是每个神经元的权重都乘以一个 <code>p</code>，这样在“总体上”使得测试数据和训练数据是大致一样的。比如一个神经元的输出是 <code>x</code>，那么在训练的时候它有 <code>p</code> 的概率参与训练，<code>(1-p)</code> 的概率丢弃，那么它输出的期望是 <code>px+(1-p)0=px</code>。因此，测试的时候把这个神经元的权重乘以 <code>p</code> 可以得到同样的期望。</p>
<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="无监督预训练（Unsupervised-pre-training）"><a href="#无监督预训练（Unsupervised-pre-training）" class="headerlink" title="无监督预训练（Unsupervised pre-training）"></a>无监督预训练（Unsupervised pre-training）</h2><p>指预训练阶段的样本不需要人工标注数据</p>
<h2 id="有监督预训练（Supervised-pre-training）"><a href="#有监督预训练（Supervised-pre-training）" class="headerlink" title="有监督预训练（Supervised pre-training）"></a>有监督预训练（Supervised pre-training）</h2><p>也可以将其称为迁移学习。简而言之，就是<strong>把一个训练好的参数，拿到另外一个任务上，作为神经网络的初始参数值，这样就比直接采用随机初始化的方法的精度要提升很多</strong>。</p>
<p>例如，比如已经有一大堆标注好的人脸年龄分类的图片数据，训练了一个CNN，用于人脸的年龄识别。然后，当新的项目任务是：人脸性别识别时，便可以直接利用已经训练好的年龄识别CNN模型，去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练。</p>
<h2 id="Ground-Truth"><a href="#Ground-Truth" class="headerlink" title="Ground Truth"></a>Ground Truth</h2><p>机器学习包括有监督学习（supervised learning），无监督学习（unsupervised learning），和半监督学习（semi-supervised learning）。</p>
<p>在有监督学习中，数据是有标注的，以 <code>(x, t)</code> 的形式出现，其中 <code>x</code> 是输入数据，<code>t</code> 是标注。<strong>正确的 <code>t</code> 标注是 ground truth，</strong> 错误的标记则不是。（也有人将所有标注数据都叫做 ground truth）</p>
<p>由模型函数的数据则是由 <code>(x, y)</code> 的形式出现的。其中 <code>x</code> 为之前的输入数据，<code>y</code> 为模型预测的值。标注会和模型预测的结果作比较。在损耗函数（loss function / error function）中会将 <code>y</code>  和 <code>t</code> 作比较，从而计算损耗（loss / error）。 比如在最小方差中：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-6258d35d612386e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此，如果标注数据不是 ground truth，那么 loss 的计算将会产生误差，从而影响到模型质量。</p>
<p>例子：（比如输入三维，判断是否性感）</p>
<h4 id="1-错误的数据"><a href="#1-错误的数据" class="headerlink" title="1.  错误的数据"></a>1.  错误的数据</h4><ul>
<li>标注数据 <code>1</code> ：<code>((84,62,86), 1)</code>，其中 <code>x = (84,62,86)</code>，<code>t = 1</code> </li>
<li>标注数据 <code>2</code>：<code>((84,162,86), 1)</code>，其中 <code>x = (84,162,86)</code>，<code>t = 1</code>   </li>
</ul>
<p>这里标注数据 <code>1</code> 是 ground truth， 而标注数据 <code>2</code> 不是。</p>
<ul>
<li>预测数据<code>1</code>：<code>y = -1</code></li>
<li>预测数据 <code>2</code>：<code>y = -1</code></li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-d8c74bafec931ef4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-正确的数据"><a href="#2-正确的数据" class="headerlink" title="2.  正确的数据"></a>2.  正确的数据</h4><ul>
<li>标注数据 <code>1</code>：<code>((84,62,86) ,1)</code>，其中 <code>x =(84,62,86)</code>，<code>t = 1</code>  </li>
<li>标注数据 <code>2</code>：<code>((84,162,86) ,1)</code>，其中 <code>x =(84,162,86)</code>，<code>t = -1</code>（改为 ground truth）</li>
</ul>
<p>这里标注数据 <code>1</code> 和 <code>2</code> 都是 ground truth。</p>
<ul>
<li>预测数据 <code>1</code> ：<code>y = -1</code></li>
<li>预测数据 <code>2</code>：<code>y = -1</code></li>
</ul>
<p>总之一句话：ground truth 就是标定好的真实数据。</p>
<h1 id="top-1-amp-top-5"><a href="#top-1-amp-top-5" class="headerlink" title="top-1 &amp; top-5"></a>top-1 &amp; top-5</h1><p>ImageNet 图像分类大赛评价标准采用 top-5 错误率，或者top-1错误率，即对一张图像预测5个类别，只要有一个和人工标注类别相同就算对，否则算错。</p>
<ul>
<li><code>Top-1 = (正确标记 与 模型输出的最佳标记不同的样本数）/ 总样本数</code></li>
<li><code>Top-5 = （正确标记 不在 模型输出的前5个最佳标记中的样本数）/ 总样本数</code></li>
</ul>
<p>top1 就是你所预测的 label 取最后概率向量里面最大的那一个作为预测结果，如果你的预测结果中概率最大的那个分类正确，则预测正确；否则预测错误。top5 就是最后概率向量最大的前五名中，只要出现了正确概率即为预测正确。否则预测错误。</p>
<p>简而言之，ImageNet 图像通常有 <code>1000</code> 个可能的类别，对每幅图像你可以猜 <code>5</code> 次结果（即同时预测 <code>5</code> 个类别标签），当其中有任何一次预测对了，结果都算对，当 <code>5</code> 次全都错了的时候，才算预测错误，这时候的分类错误率就叫 top5 错误率。同理，top-k 就是对应于有 <code>k</code> 次机会可以进行预测。</p>
<h1 id="epoch"><a href="#epoch" class="headerlink" title="epoch"></a><code>epoch</code></h1><ol>
<li>iteration：表示 <code>1</code> 次迭代（也叫 training step），每次迭代更新 <code>1</code> 次网络结构的参数（batch-size 个训练数据forward+backward后更新参数过程。）</li>
<li>batch-size：<code>1</code> 次迭代所使用的样本量</li>
<li>epoch：<code>1</code> 个 epoch 表示过了 <code>1</code> 遍训练集中的所有样本（<strong>所有</strong>训练数据forward+backward后更新参数的过程）</li>
</ol>
<p>具体计算公式为：</p>
<pre><code class="python">one epoch = numbers of iterations = N = 训练样本的数量 / batch_size
</code></pre>
<p>值得注意的是，在深度学习领域中，常用带 mini-batch 的随机梯度下降算法（Stochastic Gradient Descent, SGD）训练深层结构，它有一个好处就是并不需要遍历全部的样本，当数据量非常大时十分有效。此时，可根据实际问题来定义 epoch，例如定义 <code>10000</code> 次迭代为 <code>1</code> 个 epoch，若每次迭代的 batch-size 设为 <code>256</code>，那么 <code>1</code> 个 epoch 相当于过了 <code>2560000</code> 个训练样本。</p>
<h1 id="感受野（receptive-field）"><a href="#感受野（receptive-field）" class="headerlink" title="感受野（receptive field）"></a>感受野（receptive field）</h1><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>在卷积神经网络 CNN 中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野。这段定义非常简单，用数学的语言就是感受野是 CNN 中的某一层输出结果的一个元素对应输入层的一个映射。再通俗点的解释是，feature map 上的一个点对应输入图上的区域【注意：这里是输入图，不是原始图。好多博客写的都是原图上的区域，经过一番的资料查找，发现并不是原图】。</p>
<p>感受野表示输入空间中一个特定 CNN 特征的范围区域（<em>The receptive field is defined as the region in the input space that a particular CNN’s feature is looking at）</em>。一个特征的感受野可以采用区域的中心位置和特征大小进行描述。</p>
<p>目前流行的物体识别方法都是围绕感受野来做的设计，就如 SSD 和 Faster RCNN。理解好感受野的本质有两个好处：</p>
<ol>
<li>理解卷积的本质</li>
<li>更好的理解 CNN 的整个架构。</li>
</ol>
<h2 id="感受野的计算和可视化"><a href="#感受野的计算和可视化" class="headerlink" title="感受野的计算和可视化"></a>感受野的计算和可视化</h2><p>CNN特征图可视化的两种方式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-3ce868517fa8ad79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如上图所示，采用卷积核 <code>C</code> 的核大小 <code>ksize=3x3</code>，填充大小 <code>padding=1x1</code>，步长 <code>stride=2x2</code>。（图中上面一行）对 <code>5x5</code> 的输入特征图进行卷积生成 <code>3x3</code> 的绿色特征图。（图中下面一行）对上面绿色的特征图采用相同的卷积操作生成 <code>2x2</code> 的橙色特征图（图中左边一列）按列可视化 CNN 特征图，如果只看特征图，我们无法得知特征的位置（即感受野的中心位置）和区域大小（即感受野的大小），而且无法深入了解 CNN 中的感受野信息。（图中右边一列）CNN 特征图的大小固定，其特征位置即感受野的中心位置。</p>
<p>说的有点晦涩难懂，应该是学术的讲法，简洁的理解就是，左图是常规的卷积过程。而对于右图，卷积后的图像和原图一样大，这个操作起来并不难，就是各个特征（可以理解为图像中的像素点）的位置在卷积后保持不变，空的部分用空白来填充。这样做有什么好处，在我们后面会说到。只要注意到，左图和右图在卷积后，其特征的数目（绿色和黄色点的数目）是一样的。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-ad766bdbad457c46.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图的信息量很大，内容很多，有 CNN 的卷积过程，有感受野的计算公式和过程。弄懂了上图就知道感受野到底是个怎么回事了。</p>
<p> 还记得感受野的定义吗？具体看 layer 1 的 feature map 左上角带有红点的特征（可以理解为一个像素），它对应输入 layer 0 的区域大小就是我们要计算的感受野。</p>
<p>很显然，经过 <code>3x3</code> 卷积核卷积后，它对应 layer 0 层上的灰色区域（别忘了还有 padding）。再看 layer 1 到 layer 2 的过程，卷积过程的第一步是先加 padding，<code>p2=1</code>，这里的 <code>1</code> 是特征所占的区域，换句话说就是一个特征所占的感受野。所以 Conv2 过程这张图才会在外面加上了三个格。<code>s2=2</code> 也是同样的道理，步长也是跨过两个特征。<code>k2=3</code> 也是如此，包含 <code>3x3</code> 个特征。经过卷积后就来到了 layer 2 了，左上角特征的感受野大小也很明显了，就是灰色部分。它这一个点可要完成接下来组织交代的历史任务。</p>
<p>这整个过程下来，是不是明白点意思了。感受野的计算有卷积逆过程的意思，这里我不能给出直接的定义，因为还没有权威这么说。之前讲了，明白了感受野的计算能更好理解卷积过程对吧。从上图我们再琢磨一下。特征图的大小逐渐变小，一个特征表示的信息量越来越大，这不就是有点压缩的意思嘛。将原图感兴趣的信息提取出来，不关注的统统抛掉。提的过程就是 CNN 的前向传播，抛的过程就是 CNN 的反馈学习。</p>
<p>上图展示了一些感受野的例子，采用核大小核大小 <code>ksize=3x3</code>，填充大小 <code>padding=1x1</code>，步长 <code>stride=2x2</code> 的卷积核 <code>C</code> 对 <code>5x5</code> 大小的输入图进行卷积操作，将输出 <code>3x3</code> 大小的特征图（绿色图）。对 <code>3x3</code> 大小的特征图进行相同的卷积操作，将输出 <code>2x2</code> 的特征图（橙色）。输出特征图在每个维度上的大小可以采用下面的公式进行计算：</p>
<p><img src="https://pic2.zhimg.com/v2-579648660edc1872c048a6e745b28125_b.png" alt="img"></p>
<p>为了简单，假设 CNN 的架构是对称的，而且输入图像长宽比为 <code>1</code>，因此所有维度上的变量值都相同。若 CNN 架构或者输入图像不是对称的，你也可以分别计算每个维度上的特征图大小。如上图所示，左边一列展示了一种 CNN 特征图的常见可视化方式。这种可视化方式能够获取特征图的个数，但无法计算特征的位置（感受野的中心位置）和区域大小（感受野尺寸）。上图右边一列展示了一种固定大小的 CNN 特征图可视化方式，通过保持所有特征图大小和输入图大小相同来解决上述问题，接下来每个特征位于其感受野的中心。由于特征图中所有特征的感受野尺寸相同，我们就可以非常方便画出特征对应的 bounding box 来表示感受野的大小。因为特征图大小和输入图像相同，所以我们无需将包围盒映射到输入层。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-6aa2428d478c6fbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>另外一种固定大小的 CNN 特征图表示。采用相同的卷积核 <code>C</code> 对 <code>7x7</code> 大小的输入图进行卷积操作，这里在特征中心周围画出了感受野的 bounding box。为了表达更清楚，这里忽略了周围的填充像素。固定尺寸的 CNN 特征图可以采用 <code>3D</code>（左图）或 <code>2D</code>（右图）进行表示。</p>
<p>上图展示了另外一个例子，采用相同的卷积核 <code>C</code> 对 <code>7x7</code> 大小的输入图进行卷积操作。这里给出了 <code>3D</code>（左图）和 <code>2D</code>（右图）表示下的固定尺寸 CNN 特征图。注意：上图中感受野尺寸逐渐扩大，第二个特征层的中心特征感受野很快就会覆盖整个输入图。这一点对于 CNN 设计架构的性能提升非常重要。</p>
<h3 id="感受野的计算（Receptive-Field-Arithmetic）"><a href="#感受野的计算（Receptive-Field-Arithmetic）" class="headerlink" title="感受野的计算（Receptive Field Arithmetic）"></a>感受野的计算（Receptive Field Arithmetic）</h3><p>除了每个维度上特征图的个数，还需要计算每一层的感受野大小，因此我们需要了解每一层的额外信息，包括：当前感受野的尺寸 <code>r</code>，相邻特征之间的距离（或者jump）<code>j</code>，左上角（起始）特征的中心坐标 <code>start</code>，其中特征的中心坐标定义为其感受野的中心坐标（如上述固定大小 CNN 特征图所述）。假设卷积核大小 <code>k</code>，填充大小 <code>p</code>，步长大小 <code>s</code>，则其输出层的相关属性计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-d55e6c69d304c492.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<ul>
<li>公式一基于输入特征个数和卷积相关属性计算输出特征的个数</li>
<li>公式二计算输出特征图的 <code>jump</code>，等于输入图的 <code>jump</code> 与输入特征个数（执行卷积操作时 <code>jump</code> 的个数，stride 的大小）的乘积</li>
<li>公式三计算输出特征图的 receptive field size，等于 <code>k</code> 个输入特征覆盖区域 <img src="https://www.zhihu.com/equation?tex=%28k-1%29%2Aj_%7Bin%7D" alt="(k-1)*j_{in}">加上边界上输入特征的感受野覆盖的附加区域 <img src="https://www.zhihu.com/equation?tex=r_%7Bin%7D" alt="r_{in}"></li>
<li>公式四计算第一个输出特征的感受野的中心位置，等于第一个输入特征的中心位置，加上第一个输入特征位置到第一个卷积核中心位置的距离 <img src="https://www.zhihu.com/equation?tex=%28k-1%29%2F2%2Aj_%7Bin%7D" alt="(k-1)/2*j_{in}">，再减去填充区域大小 <img src="https://www.zhihu.com/equation?tex=p%2Aj_%7Bin%7D" alt="p*j_{in}">。注意：这里都需要乘上输入特征图的 <code>jump</code>，从而获取实际距离或间隔。</li>
</ul>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>用于计算给定 CNN 架构下所有层的感受野信息。程序允许输入任何特征图的名称和图中特征的索引号，输出相关感受野的尺寸和位置。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-caba6b46e9df2420.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>AlexNet 下感受野计算样例：</p>
<pre><code class="python"># [filter size, stride, padding]
#Assume the two dimensions are the same
#Each kernel requires the following parameters:
# - k_i: kernel size
# - s_i: stride
# - p_i: padding (if padding is uneven, right padding will higher than left padding; &quot;SAME&quot; option in tensorflow)
# 
#Each layer i requires the following parameters to be fully represented: 
# - n_i: number of feature (data layer has n_1 = imagesize )
# - j_i: distance (projected to image pixel distance) between center of two adjacent features
# - r_i: receptive field of a feature in layer i
# - start_i: position of the first feature&#39;s receptive field in layer i (idx start from 0, negative means the center fall into padding)

import math
convnet =   [[11,4,0],[3,2,0],[5,1,2],[3,2,0],[3,1,1],[3,1,1],[3,1,1],[3,2,0],[6,1,0], [1, 1, 0]]
layer_names = [&#39;conv1&#39;,&#39;pool1&#39;,&#39;conv2&#39;,&#39;pool2&#39;,&#39;conv3&#39;,&#39;conv4&#39;,&#39;conv5&#39;,&#39;pool5&#39;,&#39;fc6-conv&#39;, &#39;fc7-conv&#39;]
imsize = 227

def outFromIn(conv, layerIn):
  n_in = layerIn[0]
  j_in = layerIn[1]
  r_in = layerIn[2]
  start_in = layerIn[3]
  k = conv[0]
  s = conv[1]
  p = conv[2]

  n_out = math.floor((n_in - k + 2*p)/s) + 1
  actualP = (n_out-1)*s - n_in + k 
  pR = math.ceil(actualP/2)
  pL = math.floor(actualP/2)

  j_out = j_in * s
  r_out = r_in + (k - 1)*j_in
  start_out = start_in + ((k-1)/2 - pL)*j_in
  return n_out, j_out, r_out, start_out

def printLayer(layer, layer_name):
  print(layer_name + &quot;:&quot;)
  print(&quot;\t n features: %s \n \t jump: %s \n \t receptive size: %s \t start: %s &quot; % (layer[0], layer[1], layer[2], layer[3]))

layerInfos = []
if __name__ == &#39;__main__&#39;:
#first layer is the data layer (image) with n_0 = image size; j_0 = 1; r_0 = 1; and start_0 = 0.5
  print (&quot;-------Net summary------&quot;)
  currentLayer = [imsize, 1, 1, 0.5]
  printLayer(currentLayer, &quot;input image&quot;)
  for i in range(len(convnet)):
    currentLayer = outFromIn(convnet[i], currentLayer)
    layerInfos.append(currentLayer)
    printLayer(currentLayer, layer_names[i])
  print (&quot;------------------------&quot;)
  layer_name = raw_input (&quot;Layer name where the feature in: &quot;)
  layer_idx = layer_names.index(layer_name)
  idx_x = int(raw_input (&quot;index of the feature in x dimension (from 0)&quot;))
  idx_y = int(raw_input (&quot;index of the feature in y dimension (from 0)&quot;))

  n = layerInfos[layer_idx][0]
  j = layerInfos[layer_idx][1]
  r = layerInfos[layer_idx][2]
  start = layerInfos[layer_idx][3]
  assert(idx_x &lt; n)
  assert(idx_y &lt; n)

  print (&quot;receptive field: (%s, %s)&quot; % (r, r))
</code></pre>
<p>个人聚德计算计算感受野的作用是可以帮助设计网络结构，至少可以大事了解到在每一层的特征所涵盖的信息量。例如，输入图像大小是 <code>250x250</code>的 情况下，如果最后一层的感受野能超过 <code>250</code> 的话，那么可以认为在做最后的分类判断时所用到的特征已经涵盖了原始图像所有范围的信息了。在保证最后一层特征的感受野大小的情况下，如果能够尽可能的降低网络的参数总量，那么就是件很有意义的事情。事实上 inception model 就是这样的思路，通过非对称的结构来降低参数量，同时还能保证感受野。</p>
<h2 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h2><p><strong>物体检测需要定位出物体的 bounding box</strong>。如下图所示，不仅要定位出车辆的 bounding box，还需要识别出 bounding box里面的物体就是车辆。对于 bounding box 的定位精度，存在一个定位精度评价公式：<code>IoU</code>（因为算法不可能百分百跟人工标注的数据完全匹配）。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-802c153951d23b3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><code>IoU</code>定义了两个 bounding box 的重叠度：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-c66aff1ed95a2a1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>矩形框 <code>A</code>、<code>B</code>的一个重合度 <code>IoU</code> 计算公式为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-1e77ae5572cea6de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>即，矩形框 <code>A</code>、<code>B</code> 的重叠面积占 <code>A</code>、<code>B</code>并集的面积比例:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-45bff28622a98ade.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h2><p>非极大值抑制（NMS）就是：<strong>抑制不是极大值的元素，搜索局部的极大值。这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小</strong>。</p>
<p><strong>【注】：</strong>此处不讨论通用的 <em>NMS</em> 算法，而是应用于目标检测中用于提取分数最高的窗口。例如，在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到<em>NMS</em>来选取那些邻域里分数最高（即，行人的概率最大），并且抑制那些分数低的窗口。</p>
<p>RCNN 算法会从一张图片中找出 <code>n</code> 多个可能是物体的矩形框，然后为每个矩形框分别作类别分类概率。</p>
<p>如下图所示，定位一个车辆，最后算法找出了一堆方框，需要判别哪些矩形框是没用的。非极大值抑制法的步骤：先假设有<code>6</code>个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为<code>A</code>、<code>B</code>、<code>C</code>、<code>D</code>、<code>E</code>、<code>F</code>：</p>
<ol>
<li>从最大概率矩形框 <code>F</code> 开始，分别判断 <code>A~E</code> 与 <code>F</code> 的重叠度 <code>IoU</code> 是否大于某个设定的阈值。</li>
<li>假设<code>B</code>、<code>D</code>与 <code>F</code> 的重叠度超过阈值，则扔掉 <code>B</code>、<code>D</code>，并标记第一个矩形框<code>F</code>是要保留下来的。</li>
<li>从剩下的矩形框<code>A</code>、<code>C</code>、<code>E</code>中，选择概率最大的<code>E</code>，然后判断<code>E</code>与<code>A</code>、<code>C</code>的重叠度<code>IoU</code>，重叠度大于一定的阈值就扔掉；并标记<code>E</code>是保留下来的第二个矩形框。</li>
</ol>
<p>一直重复，找到所有被保留下来的矩形框。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-81e3a20c58bf860a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="对梯度幅值进行非极大值抑制"><a href="#对梯度幅值进行非极大值抑制" class="headerlink" title="对梯度幅值进行非极大值抑制"></a>对梯度幅值进行非极大值抑制</h2><p>图像梯度幅值矩阵中的元素值越大，说明图像中该点的梯度值越大，但这不不能说明该点就是边缘（这仅仅是属于图像增强的过程）。在 <em>Canny</em> 算法中，非极大值抑制是进行边缘检测的重要步骤，通俗意义上是指：<strong>寻找像素点局部最大值，将非极大值点所对应的灰度值置为<code>0</code>，这样可以剔除掉一大部分非边缘的点</strong>。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-016aafd1cd753237.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="非极大值抑制的工作原理："><a href="#非极大值抑制的工作原理：" class="headerlink" title="非极大值抑制的工作原理："></a>非极大值抑制的工作原理：</h4><p>由上图可知，要进行非极大值抑制，首先要确定像素点 <code>C</code> 的灰度值在其八邻域内是否为最大。蓝色线条方向为 <code>C</code> 点的梯度方向（这样就可以确定其局部的最大值肯定分布在这条线上），即除了 <code>C</code> 点外，梯度方向的交点 <code>dTmp1</code> 和 <code>dTmp2</code> 这两个点的值也可能会是局部最大值。</p>
<p>因此，判断 <code>C</code> 点灰度与这两个点灰度大小，即可判断 <code>C</code> 点是否为其邻域内的局部最大灰度点。如果经过判断，<code>C</code> 点灰度值小于这两个点中的任一个，那就说明 <code>C</code> 点不是局部极大值，那么则可以排除 <code>C</code> 点为边缘。</p>
<p>在实际中，其实只能得到 <code>C</code> 点邻域的 <code>8</code> 个点的值，而 <code>dTmp1</code> 和 <code>dTmp2</code> 并不在其中。要得到这两个值，就需要对该两个点两端的已知灰度进行线性插值，即根据上图中的 <code>g1</code> 和 <code>g2</code> 对 <code>dTmp1</code> 进行插值，根据 <code>g3</code> 和 <code>g4</code> 对 <code>dTmp2</code> 进行插值，这要用到其梯度方向。</p>
<p>完成非极大值抑制后，会得到一个二值图像，非边缘的点灰度值均为 <code>0</code>，可能为边缘的局部灰度极大值点可设置其灰度为 <code>128</code>。</p>
<h2 id="物体检测-VS-图片分类"><a href="#物体检测-VS-图片分类" class="headerlink" title="物体检测 VS 图片分类"></a>物体检测 VS 图片分类</h2><p>物体检测和图片分类的区别：</p>
<ul>
<li>图片分类不需要定位，而物体检测需要定位出物体的位置，也就是相当于把物体的 bbox 检测出来。</li>
<li>物体检测是要把所有图片中的物体都识别定位出来。</li>
</ul>
<p>简言之，物体检测需要定位出物体的位置，这种就相当于回归问题，求解一个包含物体的方框。而图片分类其实是逻辑回归。这种方法对于单物体检测还不错，但是对于多物体检测便显得捉襟见肘。</p>
<h1 id="Selective-Search-for-Object-Recognition"><a href="#Selective-Search-for-Object-Recognition" class="headerlink" title="Selective Search for Object Recognition"></a>Selective Search for Object Recognition</h1><h2 id="目标检测-VS-目标识别"><a href="#目标检测-VS-目标识别" class="headerlink" title="目标检测 VS 目标识别"></a>目标检测 VS 目标识别</h2><ul>
<li>目标识别（object recognition）是指明一幅输入图像中存在哪些对象（目标）。它将整张图像作为输入，输出的是该图像中存在的对象（目标）的类标签（class labels）和类概率（class probability）。例如，类标签为“狗”，相关的类概率是 <code>97％</code>。</li>
<li>目标检测（object detection）不仅要告诉输入图像中包含了哪类目标，还要框出该目标的具体位置—利用 bounding boxes <code>(x, y, width, height)</code>来指示图像内对象的位置。</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-8171190cdaa8253e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>所有的目标检测算法的核心是目标识别算法</strong>。在目标检测时，为了定位到目标的具体位置，通常会把图像分成许多子块（sub-regions / patches），然后把子块作为输入，送到目标识别的模型中。生成较小区域最直接方法叫滑动窗口法（Sliding Window Algorithm）。滑动窗口的方法就是按照子块的大小在整幅图像上穷举所有子图像块。这种方法产生的数据量想想都头大。和滑动窗口法相对的是另外一类基于区域（Region Proposal Algorithms）的方法，例如 selective search。</p>
<h2 id="Sliding-Window-Algorithm"><a href="#Sliding-Window-Algorithm" class="headerlink" title="Sliding Window Algorithm"></a>Sliding Window Algorithm</h2><p>在滑动窗口方法中，在图像上滑动框或窗口以选择 patch，并使用对象识别（object recognition）模型对窗口覆盖的每个图像 patch 进行分类。 它对整个图像上的对象进行详尽搜索：不仅需要搜索图像中的所有可能位置，还必须以不同的比例进行搜索（这是因为物体识别模型通常以特定尺度（或尺度范围）训练）。</p>
<p>滑窗法的物体检测流程图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-32cefc1fb88bbc7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>通过滑窗法的主要思路：首先对输入图像进行不同窗口大小的滑窗进行从左往右、从上到下的滑动。每次滑动时候，对当前窗口执行分类器(分类器是事先训练好的)。如果当前窗口得到较高的分类概率，则认为检测到了物体。对每个不同窗口大小的滑窗都进行检测后，会得到不同窗口检测到的物体标记，这些窗口大小会存在重复较高的部分，最后采用NMS进行筛选。最终，经过NMS筛选后获得检测到的物体。</p>
<p>滑窗法简单易于理解，但是不同窗口大小进行图像全局搜索导致效率低下，而且设计窗口大小时候还需要考虑物体的长宽比。所以，对于实时性要求较高的分类器，不推荐使用滑窗法。</p>
<p>滑动窗口方法适用于固定宽高比的物体，例如面部或行人。由于图像是<code>3D</code>对象的<code>2D</code>投影，所以宽高比和形状等对象特征会因为拍摄图像的角度而有很大差异。由于滑动窗口方法需要搜索多个宽高比，所以计算将十分耗时。</p>
<h2 id="Region-Proposal-Algorithms"><a href="#Region-Proposal-Algorithms" class="headerlink" title="Region Proposal Algorithms"></a>Region Proposal Algorithms</h2><p>这种方法将图像作为输入，将输出边 bounding boxes —其对应于图像中最有可能为对象的所有patches。这些区域提议（region proposals）可能是嘈杂的（noisy）、重叠的（overlapping），并且可能没有完全包含对象。但是在这些区域提议中，将有一个非常接近图像中的实际对象的提议（proposal）。然后，可以使用目标识别模型对这些提议进行分类，具有高概率分数的区域提议是对象的位置。</p>
<p>区域提议算法使用分段（segmentation）识别图像中的预期对象。在分割中，基于一些标准（例如颜色，纹理等）将相邻区域进行分组。与在所有像素位置和所有尺度上寻找对象的 sliding window approach 不同，region proposal algorithm 通过以下方式工作：将像素分成为较少数量的段（segments）。因此，生成的最终提案数量比滑动窗口方法少很多倍。这就减少了必须分类的图像 patches 的数量，这些生成的区域提议具有不同的比例和宽高比。</p>
<h2 id="Selective-Search"><a href="#Selective-Search" class="headerlink" title="Selective Search"></a>Selective Search</h2><p>选择搜索算法的主要观点：<strong>图像中物体可能存在的区域应该是有某些相似性或者连续性区域的。因此，选择搜索基于上面这一想法采用子区域合并的方法进行提取 bounding boxes 候选边界框。首先，对输入图像进行分割算法产生许多小的子区域。其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做 bounding boxes(外切矩形)，这些子区域外切矩形就是通常所说的候选框。</strong></p>
<p>选择搜索的物体检测流程图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-a021478bbcff8416.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>滑窗法类似穷举进行图像子区域搜索，但是一般情况下图像中大部分子区域是没有物体的。选择搜索算法的主要观点：<strong>图像中物体可能存在的区域应该是有某些相似性或者连续性区域的</strong>。因此，选择搜索基于这一想法，采用子区域合并的方法进行提取 bounding boxes 候选边界框。</p>
<ul>
<li>首先，对输入图像进行分割算法产生许多小的子区域。</li>
<li>其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做 bounding boxes(外切矩形)，这些子区域外切矩形就是通常所说的候选框。</li>
</ul>
<h3 id="算法流程："><a href="#算法流程：" class="headerlink" title="算法流程："></a>算法流程：</h3><p><img src="https://upload-images.jianshu.io/upload_images/1351548-b02ebd1dc5e6639d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<ul>
<li>step 0：生成区域集 <code>R</code>，具体参见论文<em>《Efficient Graph-Based Image Segmentation》</em></li>
<li>step 1：计算区域集 <code>R</code> 里每个相邻区域的相似度 <code>S = {s1,s2,…}</code> </li>
<li>step 2：找出相似度最高的两个区域，将其合并为新集，添加进 <code>R</code> </li>
<li>step 3：从 <code>S</code> 中移除所有与 step 2中有关的子集 </li>
<li>step 4：计算新集与所有子集的相似度 </li>
<li>step 5：跳至 step 2，直至 <code>S</code> 为空</li>
</ul>
<h3 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h3><p><a href="https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="noopener">Selective Search for Object Recognition论文</a>考虑了颜色、纹理、尺寸和空间交叠这 4 个参数。</p>
<h4 id="颜色相似度（color-similarity）"><a href="#颜色相似度（color-similarity）" class="headerlink" title="颜色相似度（color similarity）"></a>颜色相似度（color similarity）</h4><p>将色彩空间转为 <code>HSV</code>，对于每一个 region 的每个通道以 <code>bins=25</code> 计算直方图，这样每个区域的颜色直方图有 <code>25*3=75</code> 个区间。 对直方图除以区域尺寸做归一化后使用下式计算相似度：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-eff1704538e4f10c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，<img src="https://upload-images.jianshu.io/upload_images/1351548-4e4cfa123e4817ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""> 表示两个不同的 region，<img src="https://upload-images.jianshu.io/upload_images/1351548-62e5b75b53d37959.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">表示颜色直方图。</p>
<h4 id="纹理相似度（texture-similarity）"><a href="#纹理相似度（texture-similarity）" class="headerlink" title="纹理相似度（texture similarity）"></a>纹理相似度（texture similarity）</h4><p>采用方差为<code>1</code>（<img src="https://upload-images.jianshu.io/upload_images/1351548-3bf7bdb571fb795b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">）的高斯分布在<code>8</code>个方向做梯度统计，然后将统计结果（尺寸与区域大小一致）以<code>bins=10</code>计算直方图。直方图区间数为<code>8*3*10=240</code>（使用RGB色彩空间）</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-8fcaf06f2a2cf2f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，<img src="https://upload-images.jianshu.io/upload_images/1351548-1f7c99d38fc5d1f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">是直方图中第<img src="https://upload-images.jianshu.io/upload_images/1351548-a62871eb544528b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">个<code>bin</code>的值</p>
<h4 id="尺寸相似度（size-similarity）"><a href="#尺寸相似度（size-similarity）" class="headerlink" title="尺寸相似度（size similarity）"></a>尺寸相似度（size similarity）</h4><p><img src="https://upload-images.jianshu.io/upload_images/1351548-72e9f09684e62525.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域。</p>
<p><strong>例子：</strong><br>设有区域 <code>a-b-c-d-e-f-g-h</code> ：</p>
<ul>
<li>较好的合并方式是：<code>ab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh</code>。 </li>
<li>不好的合并方法是：<code>ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh</code>。</li>
</ul>
<h4 id="交叠相似度（shape-compatibility-measure）"><a href="#交叠相似度（shape-compatibility-measure）" class="headerlink" title="交叠相似度（shape compatibility measure）"></a>交叠相似度（shape compatibility measure）</h4><p><img src="https://upload-images.jianshu.io/upload_images/1351548-db8f19304134fecf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>例子：左图适于合并，右图不适于合并</strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-184f03ae37b03853.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="最终的相似度"><a href="#最终的相似度" class="headerlink" title="最终的相似度"></a>最终的相似度</h4><p><img src="https://upload-images.jianshu.io/upload_images/1351548-d3e553866a3762f1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h1><p>池化层可以非常有效地缩小矩阵的尺寸，从而减少最后全连接层中的参数。使用池化层，既可以加快计算速度，也有防止过拟合问题的作用。</p>
<p>池化层前向传播的过程也是通过移动 一个类似过滤器的结构完成的 。不过池化层过滤器中 的计算不是节点的加权和，而是采用更加简单的最大值或者平均值运算。使用最大值操作的池化层被称之为最大池化层（ max pooling ），这是被使用得最多的池化层结构。使用平均值操作的池化层被称之为平均池化层（ average pooling ）。</p>
<p>卷积层和池化层中过滤器移动的方式是相似的，唯一的区别在于卷积层使用的过滤器是横跨整个深度的，而池化层使用 的过滤器只影响一个深度上的节点。所以池化层 的过滤器除了在长和宽两个维度移动 ，它还需要在深度这个维度移动。 </p>
<h1 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h1><p><img src="https://upload-images.jianshu.io/upload_images/1351548-47c319fa1b2e4308.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="第一层：卷积层"><a href="#第一层：卷积层" class="headerlink" title="第一层：卷积层"></a>第一层：卷积层</h2><p>这一层的输入就是原始的图像像素 ， LeNet 模型接受的输入层大小为 <code>32×32×l</code>。第一个卷积层 filter 的尺寸为 <code>5×5</code>，深度为 <code>6</code>，不使用全 <code>0</code> 填充，步长为 <code>1</code> 。因为没有使用全 <code>0</code> 填充，所以这 一 层的输出 的尺寸为 <code>32-5+1=28</code>， 深度为 <code>6</code> 。这一个卷积层总共有 <code>5×5×6+6=156</code> 个参数，其中 <code>6</code> 个为偏置项参数。因为下一层的节点矩阵有 <code>28×28=4704</code> 个节点，每个节点和 <code>5×5=25</code> 个当前层节点相连，所以本 层卷积层总共有 <code>4704×(25+1)=122304</code> 个连接。</p>
<h2 id="第二层：池化层"><a href="#第二层：池化层" class="headerlink" title="第二层：池化层"></a>第二层：池化层</h2><p>这一层的输入为第一层的输出， 是一个 <code>28×28×6</code> 的节点矩阵。本层采用的 filter 大小为 <code>2×2</code>，长和宽的步长均为 <code>2</code>，所以本层的输出矩阵大小为 <code>14×14×6</code>。</p>
<h2 id="第三层：卷积层"><a href="#第三层：卷积层" class="headerlink" title="第三层：卷积层"></a>第三层：卷积层</h2><p>本层的输入矩阵大小为 <code>14×14×6</code>，使用的 filter 大小为 <code>5×5</code> ，深度为 <code>16</code>。本层不使用全 <code>0</code> 填充， 步长为 <code>l</code>。本层的输出矩阵大小为 <code>10×10×16</code>。按照标准的卷积层 ，本层应该有 <code>5×5×6×16+16=2416</code> 个参数，<code>10×10×16×(25+1)=41600</code> 个连接。</p>
<h2 id="第四层：池化层"><a href="#第四层：池化层" class="headerlink" title="第四层：池化层"></a>第四层：池化层</h2><p>本层的输入矩阵大小为 <code>10×l0×16</code>，采用的 filter 大小为 <code>2×2</code>，步长为 <code>2</code>。本层的输出矩阵大小为 <code>5×5×l6</code>。</p>
<h2 id="第五层：全连接层"><a href="#第五层：全连接层" class="headerlink" title="第五层：全连接层"></a>第五层：全连接层</h2><p>本层的输入矩阵大小为 <code>5×5×16</code>，在 LeNet 模型的论文中将这一层称为卷积层，但是因为 filter 的大小就是 <code>5×5</code>，所以和全连接层没有区别。如果将 <code>5×5×16</code> 矩阵中的节点拉成一个向量，那么这一层和全连接层输入就一样了。本层的输出节点个数为 <code>120</code>个，总共有 <code>5×5×16×120+120=48120</code> 个参数。</p>
<h2 id="第六层：全连接层"><a href="#第六层：全连接层" class="headerlink" title="第六层：全连接层"></a>第六层：全连接层</h2><p>本层的输入节点个数为 <code>120</code> 个，输出节点个数为 <code>84</code> 个，总共参数为 <code>120×84+84=10164</code>个。</p>
<h2 id="第七层：全连接层"><a href="#第七层：全连接层" class="headerlink" title="第七层：全连接层"></a>第七层：全连接层</h2><p>本层的输入节点个数为 <code>84</code> 个，输出节点个数为 <code>10</code> 个，总共参数为 <code>84×10+10=850</code> 个 。</p>
<p>TensorFlow 的实现（<code>LeNet.py</code>，在 MNIST 中为 <code>mnist_train.py</code>）：</p>
<pre><code class="python"># 配置神经网络参数
INPUT_NODE = 784
OUTPUT_NODE = 10

IMAGE_SIZE = 28
NUM_CHANNELS = 1
NUM_LABELS = 10

# 第一层卷积层的尺寸和深度
CONV1_DEEPTH = 32
CONV1_SIZE = 5

# 第二层卷积层的尺寸和深度
CONV2_DEEPTH = 64
CONV2_SIZE = 5

# 全连接层的结点个数
FC_SIZE = 512

# 次函数表示CNN的的前向传播过程。其中，train用于表示：区分训练过程还是测试过程。
def inference(input_tensor, train, regularizer):
    # 第一层卷积层，和标准的LeNet模型不太一样。
    # 卷积层的输入为：28*28*1 即原始的MNIST图片的像素
    # 使用全0填充
    # 输出为：28*28*32的矩阵
    with tf.variable_scope(&#39;layer1_conv1&#39;):
        # 5*5*32
        conv1_weights = tf.get_variable(
            &#39;weight&#39;, 
            [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEPTH],
            initializer = tf.truncated_normal_initializer(stddev=0.1)            
            )

        conv1_biases = tf.get_variable(
            &#39;bias&#39;,
            [CONV1_DEEPTH],
            initializer = tf.constant_initializer(0.0)
            )

        # 使用边长为5，深度为32的filter，filter移动的步长为1，且使用全0填充。
        conv1 = tf.nn.conv2d(
                input_tensor,
                conv1_weights,
                strides = [1, 1, 1, 1],
                padding = &#39;SAME&#39;
            )

        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))


    with tf.name_scope(&#39;layer2_pool1&#39;):
        # 这一层的输入是上一层的输出：28*28*32
        # 池化层filter为：2*2 全0填充 移动步长为2 
        # 输出为：14*14*32
        pool1 = tf.nn.max_pool(
            relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)

    # 这一层的输入为：14*14*32
    # 输出为：14*14*64
    with tf.name_scope(&#39;layer3_conv2&#39;):
        conv2_weights = tf.get_variable(
            &#39;weight&#39;,
            # 5*5*32*64
            [CONV2_SIZE, CONV2_SIZE, CONV1_DEEPTH, CONV2_DEEPTH],
            initializer=tf.truncated_normal_initializer(stddev=0.1))

        conv2_biases = tf.get_variable(
            &#39;bias&#39;,
            [CONV2_DEEPTH], # 64
            initializer=tf.constant_initializer(0.0))

        # 使用边长为5，深度为64的过滤器，过滤器移动的步长为1
        conv2 = tf.nn.conv2d(
                        pool1,
                        conv2_weights,
                        strides=[1, 1, 1, 1],
                        padding=&#39;SAME&#39;)

        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))

    # 输入为：14*14*64
    # 输出为：7*7*64
    with tf.name_scope(&#39;layer4_pool2&#39;):
        pool2 = tf.nn.max_pool(
            relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&#39;SAME&#39;)

    pool_shape = pool2.get_shape().as_list()

    # pool_shape[0]：为一个 batch 中数据的大小
    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]

    # 将第四层的输出转化为一个 batch 的向量
    reshaped = tf.reshape(pool2, [pool_shape[0], nodes])

    # 这一层的输入是拉直之后的一组向量，长度为：7*7*64=3136。输出为：512的向量
    with tf.variable_scope(&#39;layer5_fc1&#39;):
        fc1_weights = tf.get_variable(
            &#39;weight&#39;,
            [nodes, FC_SIZE],
            initializer=tf.truncated_normal_initializer(stddev=0.1))

        # 只有全连接层的权重才需要加入正则化
        if regularizer != None:
            tf.add_to_collection(&#39;losses&#39;, regularizer(fc1_weights))

        fc1_biases = tf.get_variable(
            &#39;bias&#39;, 
            [FC_SIZE],
            initializer=tf.constant_initializer(0.1))

        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)
        if train: 
            fc1 = tf.nn.dropout(fc1, 0.5)

    with tf.variable_scope(&#39;layer6_fc2&#39;):
        fc2_weights = tf.get_variable(
            &#39;weight&#39;,
            [FC_SIZE, NUM_LABELS],
            initializer=tf.truncated_normal_initializer(stddev=0.1))
        if regularizer != None:
            tf.add_to_collection(&#39;losses&#39;, regularizer(fc2_weights))

        fc2_biases = tf.get_variable(
            &#39;bias&#39;,
            [NUM_LABELS],
            initializer=tf.constant_initializer(0.1))

        logit = tf.matmul(fc1, fc2_weights) + fc2_biases

    return logit
</code></pre>
<p>MNIST 训练过程（<code>mnist_train.py</code>）：</p>
<pre><code class="python">import os
import numpy as np

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

# 加载mnist_inference.py中定义的常量和前向传播的函数
import mnist_inference

# 配置神经网络的参数
BATCH_SIZE = 100                
# 一个训练 batch 中的训练数据的数量。
#    - 数字越小时，训练过程越接近随机梯度下降
#     - 数字越大时，训练就越接近梯度下降

LEARNING_RATE_BASE = 0.01        # 基础学习率
LEARNING_RATE_DECAY = 0.99        # 学习率的衰减率
REGULARAZTION_RATE = 0.0001        # 正则化项中的 \lambda 系数
TRAINING_STEPS = 30000            # 训练轮数
MOVING_AVERAGE_DECAY = 0.99        # 滑动平均衰减率

# 模型保存的路径和文件名
MODEL_SAVE_PATH = &quot;model/&quot;
MODEL_NAME = &quot;model.ckpt&quot;

def train(mnist):
    # 定义输入输出placeholder
    # 调整输入数据placeholder的格式，输入为一个四维矩阵
    x = tf.placeholder(
        tf.float32, 
        [BATCH_SIZE,                             # 第一维表示一个batch中样例的个数
        mnist_inference.IMAGE_SIZE,              # 第二维和第三维表示图片的尺寸
        mnist_inference.IMAGE_SIZE,
        mnist_inference.NUM_CHANNELS],           # 第四维表示图片的深度
        name=&#39;x-input&#39;)

    y_ = tf.placeholder(
        tf.float32, 
        [None, mnist_inference.OUTPUT_NODE], 
        name=&#39;y-input&#39;)

    # L2 正则化项
    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)

    # 直接使用 mnist_inference.py 中定义的前向传播过程
    y = mnist_inference.inference(x, True, regularizer)

    # 在使用 TensorFlow 训练神经网络时，
    # 一般会将代表训练轮数的变量指定为不可训练的参数。    
    global_step = tf.Variable(0, trainable=False)

    # 定义损失函数、学习率、滑动平均操作以及训练过程
    # 创建一个滑动平均类，设置的初始滑动平均衰减率为 0.99，并设置了训练轮数
    # 给定训练轮数的变量可以加快训练早期变量的更新速度。
    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)

    # 在所有代表神经网络参数的变量上使用滑动平均。其他辅助变量（比如 global_step）就
    # 不需要。tf.trainable_variables 返回的就是图上集合 GraphKeys.TRAINABLE_VARIABLES 
    # 中的元索。这个集合的元索就是所有没有指定 trainable=False 的参数。
    variable_averages_op = variable_averages.apply(tf.trainable_variables())

    # 计算交叉熵作为刻画预训值和真实值之间差距的损失函数。这里使用了 TensorFlow 中提
    # 供的 sparse_softmax_cross_entropy_with_logits 函数来计算交叉熵。当分类
    # 问题只有一个正确答案时，可以使用这个函数来加速交叉熵的计算。MNIST问题的图片中
    # 只包含了 0～9 中的一个数字，所以可以使用这个函数来计算交叉熵损失。

    # 第一个参数是神经网络不包括 softmax 层的前向传播结果
    # 第二个是训练数据的正确答案
    # 因为标准答案是 1 个长度为 10 的一维数组，而该函数需要提供的是一个正确答案的数字
    # 所以需要使用 tf.argmax 函数来得到正确答案对应的类别编号。
    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))

    # 计算在当前 batch 中所有样例的交叉熵平均值。
    cross_entropy_mean = tf.reduce_mean(cross_entropy)

    # 总损失 = 交叉熵损 + 正正则化损失
    loss = cross_entropy_mean + tf.add_n(tf.get_collection(&#39;losses&#39;))

    # 设置指数衰减学习率
    learning_rate = tf.train.exponential_decay(
        LEARNING_RATE_BASE,  # 初始学习率，随着迭代的进行，更新变量时使用的学习率在这个基础上递减
        global_step,          # 当前迭代轮数
        mnist.train.num_examples/BATCH_SIZE, # 过完所有的训练数据所需要的迭代次数 
        LEARNING_RATE_DECAY) # 学习率衰减速度

    # 优化损失函数
    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)

    # 在训练神经网络模型时，每过一遍数据：
    #    - 需要通过反向传播来更新神经网络中的参数，
    #      - 需要更新每一个参数的滑平均动值。
    # 为了一次完成多个操作，TensorFlow 提供了tf.control_dependencies 和 tf.group 两种机制。
    # 下式等价于：train_op = tf.group(train_step, variables_averages_op)
    with tf.control_dependencies([train_step, variable_averages_op]):
        train_op = tf.no_op(name=&#39;train&#39;)

    # 初始化 Tensorflow 持久化类
    saver = tf.train.Saver()
    with tf.Session() as sess:
        tf.global_variables_initializer().run()

        # 验证和测试的过程将会有一个独立的程序来完成
        # 一共训练 30000 轮
        for i in range(TRAINING_STEPS):
            # 每次训练所取的 batch 中的训练样本数为 100
            xs, ys = mnist.train.next_batch(BATCH_SIZE)

            # 将输入的训练数据格式调整为一个四维矩阵，并将这个调整后的数据传入sess.run过程
            # xs 的 shape 为 (100, 784)，其中 100 就是 batch_size 的大小，即 100 张图片
            # 经过reshape之后，变为 (100, 28, 28, 1)，100张图片，每张图片像素为 28*28，深度为 1
            reshaped_xs = np.reshape(xs, (    BATCH_SIZE,                 # 100
                                              mnist_inference.IMAGE_SIZE, # 28
                                              mnist_inference.IMAGE_SIZE, # 28
                                              mnist_inference.NUM_CHANNELS) # 1
                                    )
        # 将经过reshape之后的输入传入CNN之中，将结果保存在y_之中   
        _, loss_value, step = sess.run(
            [train_op, loss, global_step], 
            feed_dict={x: reshaped_xs, y_: ys})

            # 每1000轮保存一次模型。
            if i % 1000 == 0:
                # 输出当前的训练情况。这里只输出了模型在当前训练batch上的损失函数大小。通过损失函数的大小可以大概了解训练的情况。
                # 在验证数据集上的正确率信息会有一个单独的程序来生成。
                print(&quot;After %d training step(s), loss on training batch is %f.&quot; % (step, loss_value))
                # 保存当前的模型。这里给出了global_step参数，这样可以让每个被保存模型的文件名末尾加上训练的轮数，比如“model.ckpt-1000”表示训练1000轮后得到的模型
                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)

def main(argv=None):
    mnist = input_data.read_data_sets(&quot;dataset/&quot;, False, one_hot=True)
    train(mnist)

if __name__ == &#39;__main__&#39;:
    tf.app.run()
</code></pre>
<p>接下来，就是验证（<code>mnist_eval.py</code>）：</p>
<pre><code class="python">import time
import numpy as np
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

# 加载mnist_inference.py 和 mnist_train.py中定义的常量和函数
import mnist_inference
import mnist_train

# 每10秒加载一次最新的模型， 并在测试数据上测试最新模型的正确率
EVAL_INTERVAL_SECS = 10


def evaluate(mnist):
    with tf.Graph().as_default() as g:
        # 定义输入输出的格式
        x = tf.placeholder(
            tf.float32, 
            [mnist.validation.num_examples,           # 第一维表示样例的个数
            mnist_inference.IMAGE_SIZE,             # 第二维和第三维表示图片的尺寸
            mnist_inference.IMAGE_SIZE,
            mnist_inference.NUM_CHANNELS],         
            name=&#39;x-input&#39;)

        y_ = tf.placeholder(
            tf.float32, 
            [None, mnist_inference.OUTPUT_NODE], 
            name=&#39;y-input&#39;)

        validate_feed = {
            x: np.reshape(mnist.validation.images, 
                            (mnist.validation.num_examples, 
                             mnist_inference.IMAGE_SIZE, 
                             mnist_inference.IMAGE_SIZE, 
                             mnist_inference.NUM_CHANNELS)),
            y_: mnist.validation.labels
        }

        # 直接通过调用封装好的函数来计算前向传播的结果。
        # 因为测试时不关注正则损失的值，所以这里用于计算正则化损失的函数被设置为None。
        y = mnist_inference.inference(x, False, None)

        # 使用前向传播的结果计算正确率。
        # 如果需要对未知的样例进行分类，那么使用tf.argmax(y,1)就可以得到输入样例的预测类别了。
        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

        # 通过变量重命名的方式来加载模型，这样在前向传播的过程中就不需要调用求滑动平均的函数来获取平局值了。
        # 这样就可以完全共用mnist_inference.py中定义的前向传播过程
        variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)
        variable_to_restore = variable_averages.variables_to_restore()
        saver = tf.train.Saver(variable_to_restore)

        #每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化
        while True:
            with tf.Session() as sess:
                # tf.train.get_checkpoint_state函数会通过checkpoint文件自动找到目录中最新模型的文件名
                ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)
                if ckpt and ckpt.model_checkpoint_path:
                    # 加载模型
                    saver.restore(sess, ckpt.model_checkpoint_path)
                    # 通过文件名得到模型保存时迭代的轮数
                    global_step = ckpt.model_checkpoint_path.split(&#39;/&#39;)[-1].split(&#39;-&#39;)[-1]
                    accuracy_score = sess.run(accuracy, feed_dict = validate_feed)
                    print(&quot;After %s training step(s), validation accuracy = %f&quot; % (global_step, accuracy_score))
                else:
                    print(&quot;No checkpoint file found&quot;)
                    return
            time.sleep(EVAL_INTERVAL_SECS)


def main(argv=None):
    mnist = input_data.read_data_sets(&quot;dataset/&quot;, one_hot=True)
    evaluate(mnist)


if __name__ == &#39;__main__&#39;:
    tf.app.run()
</code></pre>
<p>以下是自己手写图片的识别：</p>
<pre><code class="python">import time
import numpy as np
import tensorflow as tf
from PIL import Image
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data

# 加载mnist_inference.py 和 mnist_train.py中定义的常量和函数
import mnist_inference
import mnist_train

def evaluate(image_array):
    with tf.Graph().as_default() as g:
        # 定义输入输出的格式
        x = tf.placeholder(
            tf.float32, 
            [1,                                     # 第一维表示样例的个数
            mnist_inference.IMAGE_SIZE,             # 第二维和第三维表示图片的尺寸
            mnist_inference.IMAGE_SIZE,
            mnist_inference.NUM_CHANNELS],          # 第四维表示图片的深度
            name=&#39;x-input&#39;)

        y = mnist_inference.inference(x, False, None)
        prediction_value = tf.argmax(y, 1)


        # 通过变量重命名的方式来加载模型，这样在前向传播的过程中就不需要调用求滑动平均的函数来获取平局值了。
        # 这样就可以完全共用mnist_inference.py中定义的前向传播过程
        variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)
        variable_to_restore = variable_averages.variables_to_restore()
        saver = tf.train.Saver(variable_to_restore)

        with tf.Session() as sess:
            # tf.train.get_checkpoint_state函数会通过checkpoint文件自动找到目录中最新模型的文件名
            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)
            if ckpt and ckpt.model_checkpoint_path:
            # 加载模型
                saver.restore(sess, ckpt.model_checkpoint_path)
            # 通过文件名得到模型保存时迭代的轮数
                prediction_value = sess.run(prediction_value, 
                    feed_dict={x: np.reshape(image_array, (1, mnist_inference.IMAGE_SIZE, mnist_inference.IMAGE_SIZE, mnist_inference.NUM_CHANNELS))})
                return prediction_value
            else:
                print(&quot;No checkpoint file found&quot;)
                return

# def pre_pic(picName):
#     # 先打开传入的原始图片
#     img = Image.open(picName)
#     # 使用消除锯齿的方法resize图片
#     reIm = img.resize((28,28),Image.ANTIALIAS)
#     # 变成灰度图，转换成矩阵
#     im_arr = np.array(reIm.convert(&quot;L&quot;))
#     threshold = 50#对图像进行二值化处理，设置合理的阈值，可以过滤掉噪声，让他只有纯白色的点和纯黑色点
#     for i in range(28):
#         for j in range(28):
#             im_arr[i][j] = 255-im_arr[i][j]
#             if (im_arr[i][j]&lt;threshold):
#                 im_arr[i][j] = 0
#             else:
#                 im_arr[i][j] = 255
#     # 将图像矩阵拉成1行784列，并将值变成浮点型（像素要求的仕0-1的浮点型输入）
#     nm_arr = im_arr.reshape([1,784])
#     nm_arr = nm_arr.astype(np.float32)
#     img_ready = np.multiply(nm_arr,1.0/255.0)

#     return img_ready

# 图片预处理函数
def process_image():
    file_name=&#39;pic/2.png&#39; # 导入自己的图片地址
    image = Image.open(file_name).convert(&#39;L&#39;)
    image_array = [(255-x)*1.0/255.0 for x in list(image.getdata())] 
    return image_array

def main(argv=None):
    image_array = process_image()
    # 将处理后的结果输入到预测函数最后返回预测结果
    prediction_value = evaluate(image_array)
    print(&quot;The prediction number is : &quot;, prediction_value)

if __name__ == &#39;__main__&#39;:
    tf.app.run()

#  从mnist中读取数字
# mnist = input_data.read_data_sets(&quot;dataset/&quot;, False, one_hot=True)
# tf.reset_default_graph()

# im = mnist.test.images[1].reshape((28,28))
# img = Image.fromarray(im*255)
# img = img.convert(&#39;RGB&#39;)
# img.save(r&#39;pic\2.jpg&#39;)
# plt.imshow(im, cmap=&quot;gray&quot;)
# plt.show()
</code></pre>
<p>如何设计卷积神经网络的架构呢？以下正则表达式公式总结了一些经典的用于图片分类问题的卷积神经网络架构 ：</p>
<pre><code class="python">输入层 --&gt; (卷积层+ --&gt; 池化层?)＋ --&gt; 全连接层+
</code></pre>
<p>在以上公式中：</p>
<ul>
<li><code>卷积层+</code> 表示一层或者多层卷积层，大部分卷积神经网络中一般最多连续使用三层卷积层。</li>
<li><code>池化层?</code> 表示没有或者一层池化层。池化层虽然可以起到减少参数防止过拟合问题，但是在部分论文中也发现可以直接通过调整卷积层步长来完成。 所以有些卷积神经网络中没有地化层。在多轮卷积层和池化层之后，卷积神经网络在输出之前一般会经过 <code>1～2</code> 个全连接层。</li>
</ul>
<p>比如 LeNet-5 模型就可以表示为以下结构。</p>
<pre><code class="python">输入层 --&gt; 卷积层 --&gt; 池化层 --&gt; 卷积层 --&gt; 池化层 --&gt; 全连接层 --&gt; 全连接层 --&gt; 输出层 
</code></pre>
<p>在输入和输出层之间的神经网络叫做隐藏层， 一般一个神经网络的隐藏层越多，这个神经网络越“深”。而所谓深度学习中的这个“深度”和神经网络的层数也是密切相关的。 </p>
<h1 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h1><h2 id="整体过程："><a href="#整体过程：" class="headerlink" title="整体过程："></a>整体过程：</h2><ol>
<li>输入一张多目标图像，采用 <em>selective search</em> 算法提取约 <code>2000</code> 个建议框</li>
<li>先在每个建议框周围加上 <code>16</code> 个像素值为建议框像素平均值的边框，再直接变形为 <code>227×227</code> 的大小</li>
<li>先将所有建议框像素减去该建议框像素平均值后【预处理操作】，再依次将每个 <code>227×227</code> 的建议框输入 <em>AlexNet CNN</em> 网络获取 <code>4096</code> 维的特征【比以前的人工经验特征低两个数量级】，<code>2000</code> 个建议框的CNN特征组合成 <code>2000×4096</code> 维矩阵</li>
<li>将 <code>2000×4096</code> 维特征与 <code>20</code> 个 SVM 组成的权值矩阵 <code>4096×20</code> 相乘【<code>20</code> 种分类，SVM 是二分类器，则有 <code>20</code> 个 SVM】，获得 <code>2000×20</code> 维矩阵表示每个建议框是某个物体类别的得分</li>
<li>分别对上述 <code>2000×20</code> 维矩阵中每一列即每一类进行非极大值抑制剔除重叠建议框，得到该列即该类中得分最高的一些建议框</li>
<li>分别用 <code>20</code> 个回归器对上述 <code>20</code> 个类别中剩余的建议框进行回归操作，最终得到每个类别的修正后的得分最高的 bounding box</li>
</ol>
<h2 id="1-算法的整体思路"><a href="#1-算法的整体思路" class="headerlink" title="1. 算法的整体思路"></a>1. 算法的整体思路</h2><p>通过利用 <em>recongnition using regions</em> 操作来解决 CNN 的定位问题，此方法在目标检测和语义分割中都取得了成功。测试阶段，此方法对每一个输入的图片产生近 <code>2000</code> 个不分种类的 <em>region proposals</em>，使用 CNN  从每个 <em>region proposals</em> 中提取一个固定长度的特征向量，然后对每个 <em>region proposal</em> 提取的特征向量使用特定种类的线性SVM进行分类（CNN + SVM for classification）。</p>
<p>RCNN 采用的方法是：首先输入一张图片，先定位出 <code>2000</code> 个物体候选框，然后采用 CNN 提取每个候选框中图片的特征向量，特征向量的维度为 <code>4096</code> 维，接着采用 SVM 算法对各个候选框中的物体进行分类识别。</p>
<p>RCNN 算法主要分为四个步骤：</p>
<ol>
<li>找出候选框（一张图像生成 <code>1K~2K</code> 个候选区域 ）</li>
<li>利用 CNN 提取特征向量（对每个候选区域，使用深度网络提取特征 ）</li>
<li>利用 SVM 进行特征向量分类（ 特征送入每一类的 SVM 分类器，判别是否属于该类）</li>
<li>使用回归器精细修正候选框位置</li>
</ol>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-8980bce3730d6a7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="2-候选框的搜索"><a href="#2-候选框的搜索" class="headerlink" title="2. 候选框的搜索"></a>2. 候选框的搜索</h2><p>当输入一张图片时，搜索出所有可能是物体的区域，这个采用的方法是传统文献的算法：《Selective Search for Object Recognition》，通过这个算法可以搜索出 <code>2000</code> 个候选框（搜出的候选框是矩形的，而且是大小各不相同）。然而，CNN 对输入图片的大小的要求是固定的，如果把搜索到的矩形选框不做处理，就扔进 CNN 中，肯定不行。</p>
<p>因此，对于每个输入的候选框都需要缩放到固定的大小。为了简单起见，假设下一阶段 CNN 所需要的输入图片大小是个正方形图片：<code>227×227</code>。由于经过 <em>selective search</em> 得到的是矩形框，可以采用两种不同的处理方法：</p>
<h3 id="1-各向异性缩放"><a href="#1-各向异性缩放" class="headerlink" title="1. 各向异性缩放"></a>1. 各向异性缩放</h3><p>即不管图片的长宽比例，也不管其是否扭曲，直接缩放成 CNN 输入的大小 <code>227×227</code>（如图 <code>(D)</code> 所示）。</p>
<h3 id="2-各向同性缩放"><a href="#2-各向同性缩放" class="headerlink" title="2. 各向同性缩放"></a>2. 各向同性缩放</h3><p>因为图片扭曲后，可能会对后续 CNN 的训练精度有影响。各向同性缩放有两种方案：</p>
<ol>
<li>直接在原始图片中，把 bounding box 的边界扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用 bounding box 中的颜色均值填充（如图<code>(B)</code>所示）。</li>
<li>先把 bounding box 图片裁剪出来，然后用固定的背景颜色填充成正方形图片（背景颜色也采用 bounding box 的像素颜色均值，如图<code>(C)</code>所示）。</li>
</ol>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-2a5b0f40b0e82d33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>得到指定大小的图片后，后面还要继续用这 <code>2000</code> 个候选框图片继续训练 CNN、SVM。在一张图中，人工标注时就只标注了正确的 bounding box，搜索出来的 <code>2000</code> 个矩形框不可能会出现一个与人工标注完全匹配的候选框。</p>
<p>因此，需要用 <code>IoU</code> 为 <code>2000</code> 个 bounding box 打标签，以便下一步 CNN 训练使用。在 CNN 阶段，如果用 <em>selective search</em> 挑选出来的候选框与物体的人工标注矩形框的重叠 <code>IoU</code> 大于 <code>0.5</code>，就把这个候选框标注成物体类别，否则就把它当做背景类别。</p>
<h3 id="非极大值抑制的具体操作"><a href="#非极大值抑制的具体操作" class="headerlink" title="非极大值抑制的具体操作"></a>非极大值抑制的具体操作</h3><p>在测试过程完成到第 <code>4</code> 步之后，获得 <code>2000×20</code> 维矩阵表示每个建议框是某个物体类别的得分情况，此时会遇到下图所示情况，同一个车辆目标会被多个建议框包围，这时需要非极大值抑制操作去除得分较低的候选框以减少重叠框。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-994e742fcbc5f915.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h4><ol>
<li>对 <code>2000×20</code> 维矩阵中每列按从大到小进行排序</li>
<li>从每列最大的得分建议框开始，分别与该列后面的得分建议框进行 <code>IoU</code> 计算，若 <code>IoU &gt; 阈值</code>，则剔除得分较小的建议框，否则认为图像中存在多个同一类物体</li>
<li>从每列次大的得分建议框开始，重复步骤 <code>2</code> </li>
<li>重复步骤 <code>3</code>，直到遍历完该列所有建议框</li>
<li>遍历完 <code>2000×20</code> 维矩阵所有列，即所有物体种类都做一遍非极大值抑制</li>
<li>最后剔除各个类别中剩余建议框得分少于该类别阈值的建议框</li>
</ol>
<h2 id="3-CNN特征提取"><a href="#3-CNN特征提取" class="headerlink" title="3. CNN特征提取"></a>3. CNN特征提取</h2><h3 id="1-网络结构设计"><a href="#1-网络结构设计" class="headerlink" title="1. 网络结构设计"></a>1. 网络结构设计</h3><p>网络结构有两个可选方案：</p>
<ul>
<li>经典的 <em>Alexnet</em></li>
<li><em>VGG16</em></li>
</ul>
<p>经过测试，<em>Alexnet</em> 精度为 <code>58.5%</code>，<em>VGG16</em> 精度为 <code>66%</code>。 <em>VGG</em> 模型的特点是：<strong>选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是 <em>Alexnet</em> 的 <code>7</code> 倍</strong>。为简单起见，直接选用 <em>Alexnet</em>。<em>Alexnet</em> 特征提取部分包含了 <code>5</code> 个卷积层、<code>3</code> 个全连接层，在 <em>Alexnet</em> 中 <code>p5</code> 层神经元个数为 <code>9216</code>，<code>f6</code>、<code>f7</code> 的神经元个数都是 <code>4096</code>，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个 <code>4096</code> 维的特征向量。</p>
<h3 id="2-有监督预训练"><a href="#2-有监督预训练" class="headerlink" title="2. 有监督预训练"></a>2. 有监督预训练</h3><p>参数初始化部分：物体检测的一个难点在于，物体标签训练数据少，如果直接采用随机初始化 CNN 参数的方法，那么目前的训练数据量是远远不够的。</p>
<p>这种情况下，最好的是采用某些方法，把参数初始化了，然后再进行有监督的参数微调。RCNN 采用有监督的预训练，所以在设计网络结构时，直接用 <em>Alexnet</em> 的网络（连参数也是直接采用它的参数，作为初始的参数值，然后再 fine-tuning 训练）。网络优化求解：采用随机梯度下降法，学习速率大小为 <code>0.001</code>。</p>
<h3 id="3-fine-tuning-训练"><a href="#3-fine-tuning-训练" class="headerlink" title="3. fine-tuning 训练"></a>3. <em>fine-tuning</em> 训练</h3><p>采用 <em>selective search</em> 搜索出来的候选框，处理到指定的大小，继续对上面预训练的 CNN 模型进行 <em>fine-tuning</em> 训练。</p>
<p>假设要检测的物体类别有 <code>N</code> 类，那么就需要把上面预训练阶段的 CNN 模型的最后一层给替换掉，替换成 <code>N+1</code> 个输出的神经元(加 <code>1</code>，表示还有一个背景)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变。接着进行 <em>SGD</em> 训练。开始的时候，<em>SGD</em> 学习率选择 <code>0.001</code>，在每次训练的时候，batch size 大小选择 <code>128</code>（其中，<code>32</code> 个为正样本、<code>96</code> 个为负样本）。</p>
<h3 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h3><h4 id="1-既然-CNN-都是用于提取特征，那么直接用-Alexnet-做特征提取，省去-fine-tuning-阶段可以吗？"><a href="#1-既然-CNN-都是用于提取特征，那么直接用-Alexnet-做特征提取，省去-fine-tuning-阶段可以吗？" class="headerlink" title="1. 既然 CNN 都是用于提取特征，那么直接用 Alexnet 做特征提取，省去  fine-tuning 阶段可以吗？"></a>1. 既然 CNN 都是用于提取特征，那么直接用 <em>Alexnet</em> 做特征提取，省去  <em>fine-tuning</em> 阶段可以吗？</h4><p>可以。可以不需重新训练 CNN，直接采用 <em>Alexnet</em> 模型，提取出 <code>p5</code> 或者 <code>f6</code>、<code>f7</code> 的特征作为特征向量，然后进行训练 SVM（只不过这样精度会比较低）。</p>
<h4 id="2-没有-fine-tuning-的时候，要选择哪一层的特征作为-CNN-提取到的特征呢？由于可以选择-p5、f6、f7，这三层的神经元个数分别是-9216、4096、4096。从-p5-到-p6-这层的参数个数是：4096-9216，从-f6-到-f7-的参数是4096-4096。那么具体是选择-p5、f6-还是-f7-呢？"><a href="#2-没有-fine-tuning-的时候，要选择哪一层的特征作为-CNN-提取到的特征呢？由于可以选择-p5、f6、f7，这三层的神经元个数分别是-9216、4096、4096。从-p5-到-p6-这层的参数个数是：4096-9216，从-f6-到-f7-的参数是4096-4096。那么具体是选择-p5、f6-还是-f7-呢？" class="headerlink" title="2. 没有 fine-tuning 的时候，要选择哪一层的特征作为 CNN 提取到的特征呢？由于可以选择 p5、f6、f7，这三层的神经元个数分别是 9216、4096、4096。从 p5 到 p6 这层的参数个数是：4096*9216，从 f6 到 f7 的参数是4096*4096。那么具体是选择 p5、f6 还是 f7 呢？"></a>2. 没有 <em>fine-tuning</em> 的时候，要选择哪一层的特征作为 CNN 提取到的特征呢？由于可以选择 <code>p5</code>、<code>f6</code>、<code>f7</code>，这三层的神经元个数分别是 <code>9216</code>、<code>4096</code>、<code>4096</code>。从 <code>p5</code> 到 <code>p6</code> 这层的参数个数是：<code>4096*9216</code>，从 <code>f6</code> 到 <code>f7</code> 的参数是<code>4096*4096</code>。那么具体是选择 <code>p5</code>、<code>f6</code> 还是 <code>f7</code> 呢？</h4><p>RCNN 论文证明了一个理论：如果不进行 <em>fine-tuning</em>，即直接把 <em>Alexnet</em> 模型当做万金油使用，类似于 <em>HOG</em>、<em>SIFT</em> 一样做特征提取，不针对特定的任务，然后把提取的特征用于分类，结果发现 <code>p5</code> 的精度竟然跟 <code>f6</code>、<code>f7</code> 差不多，而且 <code>f6</code> 提取到的特征还比 <code>f7</code> 的精度略高；如果进行了 <em>fine-tuning</em>，那么 <code>f7</code>、<code>f6</code> 提取到的特征就会让训练的 SVM 分类器的精度飙涨。</p>
<p>据此，如果不针对特定任务进行 <em>fine-tuning</em>，而是把 CNN 当做特征提取器的话，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于 SIFT 算法一样，可以用于提取各种图片的特征，而 <code>f6</code>、<code>f7</code> 所学习到的特征是用于针对特定任务的特征。打个比方：对于人脸性别识别来说，一个 CNN 模型前面的卷积层所学习到的特征就类似于学习人脸共性特征，然后全连接层所学习的特征就是针对性别分类的特征。</p>
<h4 id="3-CNN-在进行训练的时候，本来就是对-bounding-box-的物体进行识别分类训练，是一个端到端的任务。在训练的最后一层-softmax-就是分类层，那么为什么要先用-CNN-做特征提取（提取-fc7层数据），然后再把提取的特征用于训练-SVM-分类器？"><a href="#3-CNN-在进行训练的时候，本来就是对-bounding-box-的物体进行识别分类训练，是一个端到端的任务。在训练的最后一层-softmax-就是分类层，那么为什么要先用-CNN-做特征提取（提取-fc7层数据），然后再把提取的特征用于训练-SVM-分类器？" class="headerlink" title="3. CNN 在进行训练的时候，本来就是对 bounding box 的物体进行识别分类训练，是一个端到端的任务。在训练的最后一层 softmax 就是分类层，那么为什么要先用 CNN 做特征提取（提取 fc7层数据），然后再把提取的特征用于训练 SVM 分类器？"></a>3. CNN 在进行训练的时候，本来就是对 bounding box 的物体进行识别分类训练，是一个端到端的任务。在训练的最后一层 <em>softmax</em> 就是分类层，那么为什么要先用 CNN 做特征提取（提取 <code>fc7</code>层数据），然后再把提取的特征用于训练 SVM 分类器？</h4><p>这是因为 SVM 训练和 CNN 训练过程的正负样本定义方式各有不同，导致最后采用 CNN softmax 输出比采用 SVM 精度还低。</p>
<p>CNN 在训练的时候，对训练数据做了比较宽松的标注（比如一个 bounding box 可能只包含物体的一部分），那么把它也标注为正样本，用于训练 CNN；采用这个方法的主要原因在于 CNN 容易过拟合，所以需要大量的训练数据。在 CNN 训练阶段，是对 bounding box 的位置限制条件限制的比较松(<code>IoU</code> 只要大于 <code>0.5</code> 都被标注为正样本)；</p>
<p>然而 SVM 训练的时候，因为 SVM 适用于少样本训练，所以对于训练样本数据的 <code>IoU</code> 要求比较严格，只有当bounding box 把整个物体都包含进去了，才把它标注为物体类别，然后训练 SVM。</p>
<h4 id="4-为什么需要回归器？"><a href="#4-为什么需要回归器？" class="headerlink" title="4. 为什么需要回归器？"></a>4. 为什么需要回归器？</h4><p>目标检测不仅是要对目标进行识别，还要完成定位任务，所以最终获得的bounding-box也决定了目标检测的精度（定位精度可以用算法得出的物体检测框与实际标注的物体边界框的<code>IoU</code>值来近似表示）。</p>
<p>如下图所示，绿色框为实际标准的卡宴车辆框，即Ground Truth；黄色框为<em>selective search</em>算法得出的建议框，即Region Proposal。即使黄色框中物体被分类器识别为卡宴车辆，但是由于绿色框和黄色框<code>IoU</code>值并不大，所以最后的目标检测精度并不高。采用回归器是为了对建议框进行校正，使得校正后的Region Proposal与<em>selective search</em>更接近， 以提高最终的检测精度。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-a8e13a1891d2f2b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="如何设计回归器（Bounding-box-regression）？"><a href="#如何设计回归器（Bounding-box-regression）？" class="headerlink" title="如何设计回归器（Bounding-box regression）？"></a>如何设计回归器（Bounding-box regression）？</h4><p>如下图所示，黄色框口<code>P</code>表示建议框Region Proposal，绿色窗口<code>G</code>表示实际框Ground Truth，红色窗口表示Region Proposal进行回归后的预测窗口。现在的目标是：找到<code>P</code>的线性变换【当Region Proposal与Ground Truth的<code>IoU &gt; 0.6</code>时，可以认为是线性变换】，使得与<code>G</code>越相近，这就相当于一个可以用最小二乘法解决的线性回归问题。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-caf7d246cb76372b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><code>P</code>窗口的数学表达式：<img src="https://upload-images.jianshu.io/upload_images/1351548-c8c359a767422ca3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">，其中表示第<code>i</code>个窗口的中心点坐标，<br><img src="https://upload-images.jianshu.io/upload_images/1351548-702ecd0e93aa424d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">分别为第<code>i</code>个窗口的宽和高</p>
<p><code>G</code>窗口的数学表达式为：<img src="https://upload-images.jianshu.io/upload_images/1351548-e8debf684ccb4c6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">。 </p>
<p>定义四种变换函数<img src="https://upload-images.jianshu.io/upload_images/1351548-3d2f421e8b246347.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">和<img src="https://upload-images.jianshu.io/upload_images/1351548-80e0e68c9f14c187.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">（即，通过平移对<code>x</code>和<code>y</code>进行变化，通过缩放对<code>w</code>和<code>h</code>进行变化）：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-7d66da72dae05358.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>每一个函数<img src="https://upload-images.jianshu.io/upload_images/1351548-f76229a3f16e64d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">【<img src="https://upload-images.jianshu.io/upload_images/1351548-0264e01165b94213.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">表示<img src="https://upload-images.jianshu.io/upload_images/1351548-6bdc48bd16750c5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">中的一个】都是一个<em>AlexNet</em> CNN网络的<img src="https://upload-images.jianshu.io/upload_images/1351548-1f68ce4afa19558b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">层特征（用<img src="https://upload-images.jianshu.io/upload_images/1351548-59295275a54393c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">表示）的线性函数。所以有<img src="https://upload-images.jianshu.io/upload_images/1351548-916467909df38793.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">，其中，<img src="https://upload-images.jianshu.io/upload_images/1351548-dedf7ed6344d06bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">为可学习模型参数（learnable<br>model parameters）的向量，它就是所需要学习的回归参数。</p>
<p>损失函数（使用岭回归）为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-d9dc5ab538a7d48b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>损失函数中加入正则项是为了避免归回参数过大。其中，回归目标（<img src="https://upload-images.jianshu.io/upload_images/1351548-d8cc2bba0073551c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">）由训练输入对按下式计算得来：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-196e59564c32e911.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>回归的整体过程为：</strong></p>
<ol>
<li>构造样本对。为了提高每类样本框回归的有效性，对每类样本都仅仅采集与Ground Truth相交<code>IoU</code>最大的Region Proposal，并且<code>IoU &gt; 0.6</code>的Region Proposal作为样本对，一共产生<code>20</code>对样本对【<code>20</code>个类别】 </li>
<li>每种类型的回归器进行单独训练，输入该类型样本对<code>N</code>个以及其所对应的<em>AlexNet</em> CNN网络<img src="https://upload-images.jianshu.io/upload_images/1351548-421cfeb01de6f46e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">层特征 </li>
<li>利用<code>(6)-(9)</code>式和输入样本对进行计算 </li>
<li>根据损失函数<code>(5)</code>进行回归，得到使损失函数最小的参数</li>
</ol>
<h2 id="4-SVM训练"><a href="#4-SVM训练" class="headerlink" title="4. SVM训练"></a>4. SVM训练</h2><p>这是一个二分类问题，我么假设要检测车辆。只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么就可以把它当做负样本。</p>
<p>但问题是当检测窗口只有部分包含物体，那该怎么定义正负样本呢？通过训练发现，如果选择<code>IoU</code>阈值为<code>0.3</code>效果最好，即当重叠度小于<code>0.3</code>的时候，就把它标注为负样本。一旦CNN <code>f7</code>层特征被提取出来，那么将为每个物体累训练一个SVM分类器。当用CNN提取<code>2000</code>个候选框，可以得到<code>2000×4096</code>的特征向量矩阵，然后只需要把这样的一个矩阵与SVM权值矩阵<code>4096×N</code>点乘(<code>N</code>为分类类别数目，因为训练的<code>N</code>个SVM，每个SVM包含了<code>4096</code>个<code>W</code>)，就可以得到结果。</p>
<p>图片分类标注好的训练数据非常多，但是物体检测的标注数据却很少，如何用少量的标注数据，训练高质量的模型，这就是RCNN最大的特点。其采用了迁移学习的思想：先利用<em>ILSVRC2012</em>这个训练数据库（一个图片分类训练数据库，其拥有大量的标注数据，共包含了<code>1000</code>种类别物体），进行网络的图片分类训练。因此，预训练阶段CNN模型的输出是<code>1000</code>个神经元，或者也可以直接采用<em>Alexnet</em>训练好的模型参数。</p>
<p><strong>扩展阅读：</strong></p>
<ul>
<li><a href="https://blog.csdn.net/u014696921/article/details/52824097" target="_blank" rel="noopener">R-CNN论文详解</a></li>
<li><a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="noopener">RCNN论文</a></li>
</ul>
<h1 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h1><p>经过 R-CNN 和 Fast RCNN 的积淀，<em>Ross B. Girshick</em> 在 2016 年提出了新的 Faster RCNN，在结构上，Faster RCNN 已经将特征抽（feature extraction），proposal 提取，bounding box regression（rect refine），classification 都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-c45c2eab6eac9222.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>依作者看来，如上图，Faster RCNN 其实可以分为 4 个主要内容：</p>
<ol>
<li>Conv layers。作为一种 CNN 网络目标检测方法，Faster RCNN 首先使用一组基础的 <code>conv + relu + pooling</code> 层提取 image 的 feature maps。该 feature maps 被共享用于后续 RPN 层和全连接层。</li>
<li>Region Proposal Networks。RPN 网络用于生成 region proposals。该层通过 softmax 判断 anchors 属于 foreground 或者 background，再利用 bounding box regression 修正 anchors 获得精确的 proposals。</li>
<li>Roi Pooling。该层收集输入的 feature maps 和 proposals，综合这些信息后提取 proposal feature maps，送入后续全连接层判定目标类别。</li>
<li>Classification。利用 proposal feature maps 计算 proposal 的类别，同时再次 bounding box regression 获得检测框最终的精确位置。</li>
</ol>
<p>所以本文以上述 4 个内容作为切入点介绍 Faster R-CNN 网络。下图展示了Python 版本中的 VGG16 模型中的 <code>faster_rcnn_test.py</code> 的网络结构，可以清晰的看到该网络对于一副任意大小 <code>PxQ</code> 的图像：</p>
<ul>
<li>首先缩放至固定大小 <code>MxN</code>，然后将 <code>MxN</code> 图像送入网络</li>
<li>而 Conv layers 中包含了 <code>13</code> 个 <code>conv</code> 层 + <code>13</code> 个 <code>relu</code> 层 + <code>4</code> 个 <code>pooling</code> 层</li>
<li>RPN 网络首先经过 <code>3x3</code> 卷积，再分别生成 foreground anchors 与 bounding box regression 偏移量，然后计算出 proposals</li>
<li>而 Roi Pooling 层则利用 proposals 从 feature maps 中提取 proposal feature 送入后续全连接和 softmax 网络作 classification（即分类 proposal 到底是什么 object）</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-75b0a13a4a6a8baf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="1-Conv-layers"><a href="#1-Conv-layers" class="headerlink" title="1. Conv layers"></a>1. Conv layers</h2><p>Conv layers 包含了 <code>conv</code>，<code>pooling</code>，<code>relu</code> 三种层。以 Python版本中的 VGG16 模型中的 <code>faster_rcnn_test.py</code> 的网络结构为例，如上图。Conv layers 部分共有 <code>13</code> 个 <code>conv</code> 层，<code>13</code> 个 <code>relu</code> 层，<code>4</code> 个 <code>pooling</code> 层。在 Conv layers 中：</p>
<ul>
<li>所有的 <code>conv</code> 层为：<ul>
<li><code>kernel_size = 3</code></li>
<li><code>padding = 1</code></li>
<li><code>stride = 1</code></li>
</ul>
</li>
<li>所有的 <code>pooling</code> 层为：<ul>
<li><code>kernel_size = 2</code></li>
<li><code>padding = 0</code></li>
<li><code>stride = 2</code></li>
</ul>
</li>
</ul>
<p>在 Faster RCNN Conv layers 中对所有的卷积都做了扩边处理（ <code>padding=1</code>，即填充一圈 <code>0</code>），导致原图变为 <code>(M+2)x(N+2)</code> 大小，再做 <code>3x3</code> 卷积后输出 <code>MxN</code>。正是这种设置，导致 Conv layers 中的 <code>conv</code> 层不改变输入和输出矩阵大小。如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-49a933372f94f832.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>类似的是，Conv layers 中的 <code>pooling</code> 层 <code>kernel_size=2</code>，<code>stride=2</code>。这样，·每个经过 <code>pooling</code> 层的 <code>MxN</code> 矩阵，都会变为 <code>(M/2)x(N/2)</code> 大小。综上所述，在整个 Conv layers 中，<code>conv</code> 和 <code>relu</code> 层不改变输入输出大小，只有 <code>pooling</code> 层使输出长宽都变为输入的 <code>1/2</code>。</p>
<p>那么，一个 <code>MxN</code> 大小的矩阵经过 Conv layers 固定变为 <code>(M/16)x(N/16)</code>。这样 Conv layers 生成的 feature map 中都可以和原图对应起来。</p>
<h2 id="2-Region-Proposal-Networks（RPN）"><a href="#2-Region-Proposal-Networks（RPN）" class="headerlink" title="2.  Region Proposal Networks（RPN）"></a>2.  Region Proposal Networks（RPN）</h2><p>经典的检测方法生成检测框都非常耗时，如 OpenCV adaboost 使用滑动窗口 + 图像金字塔生成检测框；或如 R-CNN 使用 SS（Selective Search）方法生成检测框。而 Faster RCNN 则抛弃了传统的滑动窗口和 SS 方法，直接使用 RPN 生成检测框，这也是 Faster R-CNN 的巨大优势，能极大提升检测框的生成速度。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-ac14a124c3d47e62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图展示了 RPN 网络的具体结构。可以看到 RPN 网络实际分为两条线：</p>
<ul>
<li>上面一条通过 softmax 分类 anchors 获得 foreground 和 background（检测目标是 foreground）</li>
<li>下面一条用于计算对于 anchors 的 bounding box regression 偏移量，以获得精确的 proposal。</li>
</ul>
<p>最后的 Proposal 层则负责综合 foreground anchors 和 bounding box regression 偏移量获取 proposals，同时剔除太小和超出边界的 proposals。其实整个网络到了 Proposal Layer 这里，就完成了相当于目标定位的功能。</p>
<h3 id="2-1-多通道图像卷积基础知识介绍"><a href="#2-1-多通道图像卷积基础知识介绍" class="headerlink" title="2.1 多通道图像卷积基础知识介绍"></a>2.1 多通道图像卷积基础知识介绍</h3><ol>
<li>对于单通道图像+单卷积核做卷积，比较简单。</li>
<li>对于多通道图像+多卷积核做卷积，计算方式如下：</li>
</ol>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-946967852bd6cbe9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如上图，输入有 <code>3</code> 个通道，同时有 <code>2</code> 个卷积核。对于每个卷积核，先在输入 <code>3</code> 个通道分别作卷积，再将 <code>3</code> 个通道结果加起来得到卷积输出。所以对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量。</p>
<p>对多通道图像做 <code>1x1</code> 卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的通道“联通”在了一起。</p>
<h3 id="2-2-anchors"><a href="#2-2-anchors" class="headerlink" title="2.2  anchors"></a>2.2  anchors</h3><p>与 RPN 网络密切相关的是 anchors。anchors，实际上就是一组由<code>rpn/generate_anchors.py</code> 生成的矩形。直接运行作者 demo 中的 <code>generate_anchors.py</code> 可以得到以下输出：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-4ca50a18887a966a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">                                                                                     </p>
<p>其中每行的 <code>4</code> 个值 <img src="https://upload-images.jianshu.io/upload_images/1351548-ad6b1deee1003558.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""> 表矩形左上和右下角点坐标。<code>9</code> 个矩形共有 <code>3</code> 种形状，长宽比为大约为： <img src="https://upload-images.jianshu.io/upload_images/1351548-e131eef7c6a62439.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">三种，如下图。实际上通过 anchors 就引入了检测中常用到的多尺度方法。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-e5aebe2f28f70a06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>注：关于上面的 anchors size，其实是根据检测图像设置的。在 Python demo 中，会把任意大小的输入图像 reshape 成 <code>800x600</code>（即之前图中的<code>M=800</code>，<code>N=600</code>）。再回头来看 anchors 的大小，anchors 中长宽 <code>1:2</code> 中最大为 <code>352x704</code>，长宽 <code>2:1</code> 中最大 <code>736x384</code>，基本是 cover 了 <code>800x600</code> 的各个尺度和形状。</p>
<p>那么这 <code>9</code> 个 anchors 是做什么的呢？借用 Faster RCNN 论文中的原图，如图7，遍历 Conv layers 计算获得的 feature maps，为每一个点都配备这 <code>9</code> 种 anchors 作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有 <code>2</code> 次 bounding box regression 可以修正检测框位置。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-54211e7aaa9c8818.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>解释一下上面这张图的数字：</p>
<ol>
<li>在原文中使用的是 ZF model 中，其 Conv Layers 中最后的 <code>conv5</code> 层 <code>num_output=256</code>，对应生成 <code>256</code> 张特征图，所以相当于 feature map 每个点都是 <code>256-dimensions</code></li>
<li>在 <code>conv5</code> 之后，做了 <code>rpn_conv/3x3</code>卷积且 <code>num_output=256</code>，相当于每个点又融合了周围 <code>3x3</code> 的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时 <code>256-d</code> 不变</li>
<li>假设在 <code>conv5</code> feature map 中每个点上有 <code>k</code> 个 anchor（默认 <code>k=9</code>），而每个 anchor 要分 foreground 和 background，所以每个点由 <code>256d</code> feature 转化为 <code>cls=2k scores</code>；而每个 anchor 都有 <code>(x, y, w, h)</code> 对应 <code>4</code> 个偏移量，所以 <code>reg=4k coordinates</code></li>
<li>补充一点，全部 anchors 拿去训练太多了，训练程序会在合适的 anchors 中<strong>随机</strong>选取 <code>128</code> 个 positive anchors + <code>128</code> 个negative anchors 进行训练（什么是合适的 anchors 下文解释）</li>
</ol>
<p>注意，在本文使用的 VGG <code>conv5</code> <code>num_output=512</code>，所以是 <code>512d</code>，其他类似。</p>
<p><strong>其实 RPN 最终就是在原图尺度上，设置了密密麻麻的候选 anchor。然后用 CNN 去判断哪些 anchor是里面有目标的 foreground anchor，哪些是没目标的 background。所以，仅仅是个二分类而已。</strong></p>
<p>那么 anchor 一共有多少个？原图 <code>800x600</code>，VGG 下采样 <code>16</code> 倍，feature map 每个点设置 <code>9</code> 个 anchor，所以：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-efc5e871341345bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中 <code>ceil()</code> 表示向上取整，是因为 VGG 输出的 feature map <code>size= 50*38</code>。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-8aa64e78296c35c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h3 id="2-3-softmax-判定-foreground-与-background"><a href="#2-3-softmax-判定-foreground-与-background" class="headerlink" title="2.3  softmax 判定 foreground 与 background"></a>2.3  softmax 判定 foreground 与 background</h3><p>一副 <code>MxN</code> 大小的矩阵送入 Faster RCNN 网络后，到 RPN 网络变为 <code>(M/16)x(N/16)</code>，不妨设 <code>W=M/16</code>，<code>H=N/16</code>。在进入 reshape 与 softmax 之前，先做了 <code>1x1</code> 卷积，如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-7838873912a5f891.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>该 <code>1x1</code> 卷积的 caffe prototxt 定义如下：</p>
<pre><code class="python">layer {
  name: &quot;rpn_cls_score&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;rpn/output&quot;
  top: &quot;rpn_cls_score&quot;
  convolution_param {
    num_output: 18   # 2(bg/fg) * 9(anchors)
    kernel_size: 1 pad: 0 stride: 1
  }
}
</code></pre>
<p>可以看到其 <code>num_output=18</code>，也就是经过该卷积的输出图像为 <code>WxHx18</code> 大小。这也就刚好对应了 feature maps 每一个点都有 <code>9</code> 个 anchors，同时每个 anchors 又有可能是 foreground 和 background，所有这些信息都保存 <code>WxHx(9*2)</code> 大小的矩阵。</p>
<p>为何这样做？后面接 softmax 分类获得 foreground anchors，也就相当于初步提取了检测目标候选区域 box（一般认为目标在 foreground anchors 中）。</p>
<p>那么为何要在 softmax 前后都接一个 reshape layer？其实只是为了便于 softmax 分类，至于具体原因这就要从 caffe 的实现形式说起了。在 caffe 基本数据结构 blob 中以如下形式保存数据：</p>
<pre><code class="python">blob=[batch_size, channel，height，width]
</code></pre>
<p>对应至上面的保存 <code>bg/fg</code> anchors的矩阵，其在 caffe <code>blob</code> 中的存储形式为 <code>[1, 2x9, H, W]</code>。而在 softmax 分类时需要进行 <code>fg/bg</code> 二分类，所以 reshape layer 会将其变为 <code>[1, 2, 9xH, W]</code> 大小，即单独“腾空”出来一个维度以便 softmax 分类，之后再 reshape 回复原状。贴一段 caffe <code>softmax_loss_layer.cpp</code> 的 <code>reshape</code> 函数的解释，非常精辟：</p>
<pre><code class="python">&quot;Number of labels must match number of predictions; &quot;
&quot;e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), &quot;
&quot;label count (number of labels) must be N*H*W, &quot;
&quot;with integer values in {0, 1, ..., C-1}.&quot;;
</code></pre>
<p>综上所述，RPN 网络中利用 anchors 和 softmax 初步提取出 foreground anchors 作为候选区域。</p>
<h3 id="2-4-bounding-box-regression-原理"><a href="#2-4-bounding-box-regression-原理" class="headerlink" title="2.4  bounding box regression 原理"></a>2.4  bounding box regression 原理</h3><h1 id="目标检测中-region-proposal-的作用？"><a href="#目标检测中-region-proposal-的作用？" class="headerlink" title="目标检测中 region proposal 的作用？"></a>目标检测中 region proposal 的作用？</h1><h2 id="1-理由一"><a href="#1-理由一" class="headerlink" title="1.  理由一"></a>1.  理由一</h2><p>以 Faster RCNN 举例。在 Faster RCNN 里面，anchor（或者说 RPN 网络）的作用是代替以往 RCNN 使用的 selective search 的方法寻找图片里面可能存在物体的区域。当一张图片输入 Resnet 或者 VGG，在最后一层的 feature map 上面，寻找可能出现物体的位置，这时候分别以这张 feature map 的每一个点为中心，在原图上画出 <code>9</code> 个尺寸不一的 anchor。然后计算 anchor 与GT（ground truth） box 的 IoU（重叠率），满足一定 IoU 条件的 anchor，便认为是这个 anchor 包含了某个物体。</p>
<p>目标检测的思想是，首先在图片中寻找<strong>“可能存在物体的位置（regions）”</strong>，然后再判断<strong>“这个位置里面的物体是什么东西”</strong>，所以region proposal就参与了判断物体可能存在位置的过程。</p>
<p><strong>region proposal 是让模型学会去看哪里有物体，GT box 就是给它进行参考，告诉它是不是看错了，该往哪些地方看才对。</strong></p>
<h2 id="2-理由二"><a href="#2-理由二" class="headerlink" title="2.  理由二"></a>2.  理由二</h2><p>首先明确一个定义，当前主流的 Object Detection 框架分为 one-stage 和 two-stage，而 two-stage 多出来的这个 stage 就是 Regional Proposal 过程。</p>
<p>Regional Proposal的输出到底是什么？以 Faster R-CNN 为代表的 two-stage 目标检测方法为例：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-22ab86e6c802cde7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，图中有两个 Classification loss 和两个 Bounding-box regression loss，有什么区别呢？</p>
<ol>
<li>Input Image 经过 CNN 特征提取，<strong>首先</strong>来到 Region Proposal 网络。由 Region Proposal Network 输出的Classification，这<strong>并不是</strong>判定物体在 COCO 数据集上对应的 <code>80</code> 类中哪一类，而是输出一个二进制值 <code>p</code>，可以理解为 <img src="https://www.zhihu.com/equation?tex=p%5Cin%5B0%2C1%5D" alt="p\in[0,1]"> ，人工设定一个 <code>threshold=0.5</code>。</li>
<li>RPN 网络做的事情就是，如果一个 Region 的 <img src="https://www.zhihu.com/equation?tex=p%5Cgeq0.5" alt="p\geq0.5"> ，则认为这个 Region 中可能是 <code>80</code> 个类别中的某一类，具体是哪一类现在还不清楚。到此为止，Network 只需要把这些可能含有物体的区域选取出来就可以了，这些被选取出来的 Region 又叫做 ROI （Region of Interests），即感兴趣的区域。当然了，<strong>RPN 同时也会在 feature map 上框定这些 ROI 感兴趣区域的大致位置，即输出 Bounding-box</strong>。</li>
</ol>
<p>所以，RPN 网络做的事情就是，把一张图片中不感兴趣的区域—花花草草、大马路、天空之类的区域忽视掉，只留下一些可能感兴趣的区域—车辆、行人、水杯、闹钟等等，然后之后只需要关注这些感兴趣的区域，进一步确定它到底是车辆、还是行人、还是水杯（分类问题）等。<strong>到此为止，RPN 网络的工作就完成了，即我们现在得到的有：在输入 RPN 网络的 feature map 上，所有可能包含 <code>80</code> 类物体的 Region 区域的信息，其他 Region（非常多）可以直接不考虑了（不用输入后续网络）。</strong></p>
<p>接下来的工作就很简单了，假设输入 RPN 网络的 feature map 大小为 <img src="https://www.zhihu.com/equation?tex=64%5Ctimes64" alt="64\times64"> ，那么<strong>提取的 ROI 的尺寸一定小于 <img src="https://www.zhihu.com/equation?tex=64%5Ctimes64" alt="64\times64"></strong>，因为原始图像某一块的物体在 feature map 上也以同样的比例存在。只需要把这些 Region 从 feature map 上抠出来，由于每个 Region 的尺寸可能不一样，因为原始图像上物体大小不一样，所以<strong>我们需要将这些抠出来的 Region 想办法 resize 到相同的尺寸</strong>，这一步方法很多（Pooling 或者 Interpolation，一般采用 Pooling，因为反向传播时求导方便）。</p>
<p>假设这些抠出来的 ROI Region 被 resize 到了 <img src="https://www.zhihu.com/equation?tex=14%5Ctimes14" alt="14\times14"> 或者 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes7" alt="7\times7"> ，那接下来将这些 Region 输入普通的分类网络，即第一张 Faster R-CNN 的结构图中最上面的部分，即可得到整个网络最终的输出 classification，这里的 class（车、人、狗 ……）才真正对应了 COCO 数据集 <code>80</code> 类中的具体类别。</p>
<p>同时，由于<strong>之前 RPN 确定的 box\region 坐标比较粗略</strong>，即大概框出了感兴趣的区域，所以这里再来一次精确的微调，根据每个 box 中的具体内容微微调整一下这个 box的坐标，即输出第一张图中右上方的 Bounding-box regression。 </p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Region Proposal有什么作用？</p>
<ol>
<li>COCO 数据集上总共只有 <code>80</code> 类物体，如果不进行 Region Proposal，即网络最后的 classification 是对所有 anchor 框定的 Region 进行识别分类，<strong>会严重拖累网络的分类性能，难以收敛</strong>。原因在于，存在过多的不包含任何有用的类别（<code>80</code> 类之外的，例如各种各样的天空、草地、水泥墙、玻璃反射等等）的 Region 输入分类网络，而这些无用的 Region 占了所有 Region 的很大比例。换句话说，这些 Region 数量庞大，却并不能为 softmax 分类器带来有用的性能提升（因为无论怎么预测，其类别都是背景，对于主体的 <code>80</code> 类没有贡献）。</li>
<li>大量无用的 Region 都需要单独进入分类网络，而分类网络由几层卷积层和最后一层全连接层组成，<strong>参数众多，十分耗费计算时间</strong>，Faster R-CNN 本来就不能做到实时，这下更慢了。</li>
</ol>
<h1 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP Net"></a>SPP Net</h1><p>出自 2015 年发表在 IEEE 上的论文—《<em>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</em>》</p>
<p>在此之前，所有的神经网络都是需要输入固定尺寸的图片，比如 <code>224x224</code>（AlexNet）、<code>32x32</code>（LeNet）、<code>96x96</code> 等。这样对于希望检测各种大小的图片的时候，需要经过 crop，或者 war p等一系列操作，这都在一定程度上导致图片信息的丢失和变形，限制了识别精确度。</p>
<h3 id="RCNN-的弊端"><a href="#RCNN-的弊端" class="headerlink" title="RCNN 的弊端"></a>RCNN 的弊端</h3><p>RCNN 使用 CNN 作为特征提取器，首次使得目标检测跨入深度学习的阶段。但是 RCNN 对于每一个区域候选都需要首先将图片放缩到固定的尺寸（<code>224x224</code>），然后为每个区域候选提取 CNN 特征。容易看出这里面存在的一些性能瓶颈：</p>
<ul>
<li>速度瓶颈：重复为每个 region proposal 提取特征是极其费时的，Selective Search 对于每幅图片产生 <code>2K</code> 左右个 region proposal，也就是意味着一幅图片需要经过 <code>2K</code> 次的完整的 CNN 计算得到最终的结果。</li>
<li>性能瓶颈：对于所有的 region proposal 防缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。</li>
</ul>
<p>在 RCNN 中 CNN 阶段的流程大致如下（红色框是 selective search 输出的可能包含物体的候选框（ROI））：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-2a84667b762d7e2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面这个图可以看出SPP Net 和 RCNN 的区别，首先是输入不需要放缩到指定大小。其次是增加了一个空间金字塔池化层，还有最重要的一点是每幅图片只需要提取一次特征。</p>
<h4 id="为什么要固定输入图片的大小？"><a href="#为什么要固定输入图片的大小？" class="headerlink" title="为什么要固定输入图片的大小？"></a>为什么要固定输入图片的大小？</h4><p>卷积层的参数和输入大小无关，它仅仅是一个卷积核在图像上滑动，不管输入图像多大都没关系，只是对不同大小的图片卷积出不同大小的特征图，但是全连接层的参数就和输入图像大小有关，因为它要把输入的所有像素点连接起来，需要指定输入层神经元个数和输出层神经元个数，所以需要规定输入的 feature 的大小。因此，固定长度的约束仅限于全连接层（作为全连接层，如果输入的 <code>x</code> 维数不等，那么参数 <code>w</code> 肯定也会不同。因此，全连接层是必须确定输入，输出个数的）。</p>
<p>一张图图片会有大约 <code>2k</code> 个候选框，每一个都要单独输入 CNN 做卷积等操作很费时。SPP Net 提出：能否在 feature map 上提取 ROI 特征，这样就只需要在整幅图像上做一次卷积。SPP Net 在最后一个卷积层后，接入了金字塔池化层，使用这种方式，可以让网络输入任意的图片，而且还会生成固定大小的输出。</p>
<p>何凯明团队的 SPP Net 给出的解决方案是，既然只有全连接层需要固定的输入，那么我们在全连接层前加入一个网络层，让他对任意的输入产生固定的输出不就好了吗？一种常见的想法是对于最后一层卷积层的输出 pooling 一下，但是这个 pooling 窗口的尺寸及步伐设置为相对值，也就是输出尺寸的一个比例值，这样对于任意输入经过这层后都能得到一个固定的输出。SPP Net 在这个想法上继续加入 SPM 的思路，SPM 其实在传统的机器学习特征提取中很常用，主要思路就是对于一副图像分成若干尺度的一些块，比如一幅图像分成 <code>1</code> 份，<code>4</code> 份，<code>8</code> 份等。然后对于每一块提取特征然后融合在一起，这样就可以兼容多个尺度的特征啦。SPP Net 首次将这种思想应用在 CNN 中，对于卷积层特征我们也先给他分成不同的尺寸，然后每个尺寸提取一个固定维度的特征，最后拼接这些特征不就是一个固定维度的输入了吗？</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-fd0a04600d7ec16a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>所谓空间金字塔池化就是沿着金字塔的低端向顶端一层一层做池化。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-d1eaacbb881e9193.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图的空间金字塔池化层是 SPP Net 的核心，其主要目的是对于任意尺寸的输入产生固定大小的输出。思路是对于任意大小的 feature map 首先分成 <code>16</code>、<code>4</code>、<code>1</code> 个块，然后在每个块上最大池化，池化后的特征拼接得到一个固定维度的输出。以满足全连接层的需要。不过因为不是针对于目标检测的，所以输入的图像为一整副图像。</p>
<p>假设原图输入是 <code>224x224</code>，对于 <code>conv5</code> 出来后的输出是 <code>13x13x256</code> 的，可以理解成有 <code>256</code> 个这样的 filter，每个 filter 对应一张 <code>13x13</code> 的 response map。如果像上图那样将 response map 分成 <code>1x1</code>(金字塔底座)，<code>2x2</code>(金字塔中间)，<code>4x4</code>（金字塔顶座）三张子图，分别做 max pooling 后，出来的特征就是 <code>(16+4+1)x256</code> 维度。如果原图的输入不是 <code>224x224</code>，出来的特征依然是 <code>(16+4+1)x256</code> 维度。这样就实现了不管图像尺寸如何池化 <code>n</code> 的输出永远是 <code>(16+4+1）x256</code> 维度。 </p>
<p>实际运用中只需要根据全连接层的输入维度要求设计好空间金字塔即可。</p>
<h3 id="网络细节"><a href="#网络细节" class="headerlink" title="网络细节"></a>网络细节</h3><h4 id="1-卷积层特征图"><a href="#1-卷积层特征图" class="headerlink" title="1.  卷积层特征图"></a>1.  卷积层特征图</h4><p><img src="https://upload-images.jianshu.io/upload_images/1351548-2ddac4a5368150f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>SPP Net 通过可视化 <code>conv5</code> 层特征，发现卷积特征其实保存了空间位置信息（数学推理中更容易发现这点），并且每一个卷积核负责提取不同的特征，比如 <code>C</code> 图 <code>175</code>、<code>55</code> 卷积核的特征，其中 <code>175</code> 负责提取窗口特征，<code>55</code> 负责提取圆形的类似于车轮的特征。我们可以通过传统的方法聚集这些特征，例如词袋模型或是空间金字塔的方法。</p>
<h4 id="2-空间金字塔池化（Spatial-Pyramid-Pooling）"><a href="#2-空间金字塔池化（Spatial-Pyramid-Pooling）" class="headerlink" title="2.  空间金字塔池化（Spatial Pyramid Pooling）"></a>2.  空间金字塔池化（Spatial Pyramid Pooling）</h4><p>虽然总体流程还是 Selective Search 得到候选区域 <code>—&gt;</code> CNN 提取ROI 特征 <code>—&gt;</code> 类别判断 <code>—&gt;</code> 位置精修，但是由于所有 ROI 的特征直接在 feature map 上提取，大大减少了卷积操作，提高了效率。 </p>
<p>有两个难点要解决：</p>
<ol>
<li>原始图像的 ROI 如何映射到特征图（feature map，一系列卷积层的最后输出）</li>
<li>ROI 的在特征图上的对应的特征区域的维度不满足全连接层的输入要求怎么办（又不可能像在原始 ROI 图像上那样进行截取和缩放）？ </li>
</ol>
<p>​                                                                                                                                                                                                                                           </p>
<p>对于难点 <code>2</code>：</p>
<ul>
<li>这个问题涉及的流程主要有：图像输入 <code>—&gt;</code> 卷积层 <code>1</code> <code>—&gt;</code> 池化 <code>1</code> <code>—&gt;</code> … <code>—&gt;</code> 卷积层 <code>n</code> <code>—&gt;</code> 池化 <code>n</code> <code>—&gt;</code> 全连接层。</li>
<li>引发问题的原因主要有：全连接层的输入维度是固定死的，导致池化 <code>n</code> 的输出必须与之匹配，继而导致图像输入的尺寸必须固定。</li>
</ul>
<p>为了使一些列卷积层的最后输出刚维度好是全连接层的输入维度，解决方法可能有：</p>
<ol>
<li>想办法让不同尺寸的图像也可以使池化 <code>n</code> 产生固定的 输出维度（打破图像输入的固定性）</li>
<li>想办法让全连接层可以接受非固定的输入维度（打破全连接层的固定性，继而也打破了图像输入的固定性）</li>
</ol>
<p>以上的方法 <code>1</code> 就是 SPP Net 的思想。它在池化 <code>n</code> 的地方做了一些手脚（特殊池化手段：空间金字塔池化），使得不同尺寸的图像也可以使池化 <code>n</code> 产生固定的输出维度。至于方法 <code>2</code>，其实就是全连接转换为全卷积，作用的效果等效为在原始图像做滑窗，多个窗口并行处理）</p>
<h4 id="3-SPP-Net应用于图像分类"><a href="#3-SPP-Net应用于图像分类" class="headerlink" title="3.  SPP Net应用于图像分类"></a>3.  SPP Net应用于图像分类</h4><p>SPP Net 的能够接受任意尺寸图片的输入，但是训练难点在于所有的深度学习框架都需要固定大小的输入，因此 SPP Net 做出了多阶段多尺寸训练方法。在每一个 epoch 的时候，我们先将图像放缩到一个 size，然后训练网络。训练完整后保存网络的参数，然后 resize 到另外一个尺寸，并在之前权值的基础上再次训练模型。相比于其他的 CNN 网络，SPP Net 的优点是可以方便地进行多尺寸训练，而且对于同一个尺度，其特征也是个空间金字塔的特征，综合了多个特征的空间多尺度信息。</p>
<h4 id="4-SPP-Net-应用于目标检测"><a href="#4-SPP-Net-应用于目标检测" class="headerlink" title="4.  SPP Net 应用于目标检测"></a>4.  SPP Net 应用于目标检测</h4><p>SPP Net 理论上可以改进任何 CNN 网络，通过空间金字塔池化，使得 CNN 的特征不再是单一尺度的。但是 SPP Net 更适用于处理目标检测问题，首先是网络可以介绍任意大小的输入，也就是说能够很方便地多尺寸训练。其次是空间金字塔池化能够对于任意大小的输入产生固定的输出，这样使得一幅图片的多个 region proposal 提取一次特征成为可能。SPP Net 的做法是：</p>
<ol>
<li><p>首先通过 selective search 产生一系列的 region proposal</p>
</li>
<li><p>然后训练多尺寸识别网络用以提取区域特征，其中处理方法是每个尺寸的最短边大小在尺寸集合中：<img src="https://pic2.zhimg.com/v2-237603b04a4f5f801924219f4fdfad99_b.png" alt="img"></p>
<p>训练的时候通过上面提到的多尺寸训练方法，也就是在每个 epoch 中首先训练一个尺寸产生一个 model，然后加载这个 model 并训练第二个尺寸，直到训练完所有的尺寸。空间金字塔池化使用的尺度为：<code>1x1</code>，<code>2x2</code>，<code>3x3</code>，<code>6x6</code>，一共是 <code>50</code> 个 <code>bins</code>。</p>
</li>
</ol>
<ol start="3">
<li>在测试时，每个 region proposal 选择能使其包含的像素个数最接近 <code>224x224</code> 的尺寸，提取相 应特征。</li>
</ol>
<p>   由于我们的空间金字塔池化可以接受任意大小的输入，因此对于每个region proposal将其映射到feature map上，然后仅对这一块feature map进行空间金字塔池化就可以得到固定维度的特征用以训练CNN了。关于从region proposal映射到feature map的细节我们待会儿去说。</p>
<ol start="4">
<li>训练 SVM，bounding box 回归</li>
</ol>
<h4 id="5-如何从一个-region-proposal-映射到-feature-map-的位置？"><a href="#5-如何从一个-region-proposal-映射到-feature-map-的位置？" class="headerlink" title="5.  如何从一个 region proposal 映射到 feature map 的位置？"></a>5.  如何从一个 region proposal 映射到 feature map 的位置？</h4><p>SPP Net 通过角点尽量将图像像素映射到 feature map 感受野的中央，假设每一层的 padding 都是 <code>p/2</code>（<code>p</code>为卷积核大小）。对于 feature map 的一个像素 <code>(x&#39;,y&#39;)</code>，其实际感受野为：<code>(Sx‘，Sy’)</code>，其中 <code>S</code> 为之前所有层步伐的乘积。然后对于 region proposal 的位置，我们获取左上右下两个点对应的 feature map 的位置，然后取特征就好了。左上角映射为：</p>
<p><img src="https://pic2.zhimg.com/v2-8c5eddc9f856822aad5ae8d030ce1779_b.png" alt="img"></p>
<p>右下角映射为：</p>
<p><img src="https://pic3.zhimg.com/v2-7a4ce0c60b8fcac5eb7ffe365f99572e_b.png" alt="img"></p>
<p>当然，如果 padding 大小不一致，那么就需要计算相应的偏移值啦。</p>
<h4 id="金字塔池化的意义"><a href="#金字塔池化的意义" class="headerlink" title="金字塔池化的意义"></a>金字塔池化的意义</h4><p>总结而言，当网络输入的是一张任意大小的图片，这个时候可以一直进行卷积、池化，直到网络的倒数几层的时候，也就是即将与全连接层连接的时候，就要使用金字塔池化，使得任意大小的特征图都能够转换成固定大小的特征向量，这就是空间金字塔池化的意义（多尺度特征提取出固定大小的特征向量）。</p>
<h4 id="SPP-Net-amp-RCNN"><a href="#SPP-Net-amp-RCNN" class="headerlink" title="SPP Net &amp; RCNN"></a>SPP Net &amp; RCNN</h4><p>对于 RCNN，整个过程是：</p>
<ol>
<li>首先通过 Selective Search，对待检测的图片进行搜索出大约 <code>2000</code> 个候选窗口。 </li>
<li>把这 <code>2k</code> 个候选窗口的图片都缩放到 <code>227x227</code>，然后分别输入 CNN 中，每个 proposal 提取出一个特征向量，也就是说利用 CNN 对每个 proposal 进行提取特征向量。 </li>
<li>把上面每个候选窗口的对应特征向量，利用 SVM 算法进行分类识别。 </li>
</ol>
<p>可以看出 RCNN 的计算量是非常大的，因为 <code>2k</code> 个候选窗口都要输入到 CNN 中，分别进行特征提取。</p>
<p>而对于 SPP Net，整个过程是：</p>
<ol>
<li>首先通过 Selective Search，对待检测的图片进行搜索出 <code>2000</code> 个候选窗口。这一步和 RCNN 一样。</li>
<li>特征提取阶段。这一步就是和 RCNN 最大的区别了，这一步骤的具体操作如下：把整张待检测的图片，输入 CNN 中，进行一次性特征提取，得到 feature maps，然后在 feature maps 中找到各个候选框的区域，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量。而 RCNN 输入的是每个候选框，然后在进入 CNN，因为 SPP Net 只需要一次对整张图片进行特征提取，速度会大大提升。</li>
<li>最后一步也是和 RCNN 一样，采用 SVM 算法进行特征向量分类识别。</li>
</ol>
<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><h2 id="1-模型结构"><a href="#1-模型结构" class="headerlink" title="1. 模型结构"></a>1. 模型结构</h2><p><img src="https://upload-images.jianshu.io/upload_images/1351548-d33870250f3acd76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如上图所示，采用是两台GPU服务器，所有会有两个流程图。该模型一共分为 <code>8</code> 层——<code>5</code> 个卷积层，<code>3</code> 个全连接层（卷积层后面加了最大池化层)，包含 <code>6</code> 亿 <code>3000</code> 万个连接，<code>6000</code> 万个参数和 <code>65</code> 万个神经元。在每一个卷积层中，包含了激励函数 <code>ReLU</code> 以及局部响应归一化（<code>LRN</code>）处理，然后再经过降采样（<code>pooling</code> 处理）。</p>
<h2 id="2-具体结构"><a href="#2-具体结构" class="headerlink" title="2. 具体结构"></a>2. 具体结构</h2><p>整个网络共有 <code>8</code> 个需要训练的层，前 <code>5</code> 个为卷积层，最后 <code>3</code> 层为全连接层。</p>
<h4 id="第一层：卷积层（conv1）"><a href="#第一层：卷积层（conv1）" class="headerlink" title="第一层：卷积层（conv1）"></a>第一层：卷积层（<code>conv1</code>）</h4><ol>
<li>输入图片大小为：<code>224×224×3</code>（经过预处理后实际大小变为 <code>227×227×3</code>）</li>
<li>卷积核：<code>11×11×3</code>；步长： <code>4</code>；数量：<code></code>96<code>（图中为</code>48<code>个是由于采用了</code>2` 个 GPU。由于卷积核的深度与图像深度一样，所以提取到的特征也是彩色的）</li>
<li>卷积后的数据： <code>55×55×96</code>【<code>(227-11+1)/4 = 55</code>（向上取整）】</li>
<li><code>relu1</code> 后的数据： <code>55×55×96</code></li>
<li>最大池化层 <code>pool1</code> 的核：<code>3×3</code>；步长： <code>2</code>。因此输出的尺寸为： <code>(227-11+1)/4 = 55</code>（向上取整）</li>
<li><code>pool1</code> 后的数据（降采样）：<code>27×27×96</code> 【<code>(55-3+1)/2 = 27</code>】</li>
<li>局部归一化 LRN <code>norm1</code>：<code>local_size = 5</code> （归一化运算的尺度为 <code>5×5</code>）</li>
<li>输出：<code>27×27×96</code>。</li>
</ol>
<h4 id="第二层：卷积层（conv2）"><a href="#第二层：卷积层（conv2）" class="headerlink" title="第二层：卷积层（conv2）"></a>第二层：卷积层（<code>conv2</code>）</h4><ol>
<li>输入数据： <code>27×27×96</code></li>
<li>卷积核： <code>5×5×96</code>；步长：  <code>1</code>；数量： <code>256</code></li>
<li>卷积后的数据： <code>27×27×256</code> （SAME padding，使得卷积后图像大小不变）</li>
<li><code>relu2</code> 后的数据： <code>27×27×256</code></li>
<li><code>pool2</code> 的核（最大池化）： <code>3×3</code> ；步长： <code>2</code></li>
<li><code>pool2</code> 后的数据：<code>13×13×256</code> 【<code>(27-3+1)/2 = 13</code>】 </li>
<li><code>norm2</code>：<code>local_size = 5</code> </li>
<li>输出：<code>13×13×256</code></li>
</ol>
<h4 id="第三层：卷积层（conv3）"><a href="#第三层：卷积层（conv3）" class="headerlink" title="第三层：卷积层（conv3）"></a>第三层：卷积层（<code>conv3</code>）</h4><ol>
<li>输入数据：<code>13×13×256</code></li>
<li>卷积核：<code>3×3</code> ；步长：<code>1</code> ；数量： <code>384</code></li>
<li>卷积后数据：<code>13×13×384</code>  （SAME padding）</li>
<li><code>relu3</code> 后的数据：<code>13×13×384</code></li>
<li>输出：<code>13×13×384</code></li>
</ol>
<p><code>conv3</code> 层没有 max pool 层和 norm 层。</p>
<h4 id="第四层：卷积层（conv4）"><a href="#第四层：卷积层（conv4）" class="headerlink" title="第四层：卷积层（conv4）"></a>第四层：卷积层（<code>conv4</code>）</h4><ol>
<li>输入数据：<code>13×13×384</code></li>
<li>卷积核：<code>3×3</code> ；步长：<code>1</code> ；数量：<code>384</code></li>
<li>卷积后数据：<code>13×13×384</code>  （SAME padding）</li>
<li><code>relu4</code> 后的数据：<code>13×13×384</code></li>
<li>输出：<code>13×13×384</code></li>
</ol>
<p><code>conv4</code> 层也没有 max pool 层和 norm 层。</p>
<h4 id="第五层：卷积层（conv5）"><a href="#第五层：卷积层（conv5）" class="headerlink" title="第五层：卷积层（conv5）"></a>第五层：卷积层（<code>conv5</code>）</h4><ol>
<li>输入数据：<code>13×13×384</code></li>
<li>卷积核：<code>3×3</code> ；步长：<code>1</code> ；数量：<code>256</code></li>
<li>卷积后数据：<code>13×13×256</code>  （SAME padding）</li>
<li><code>relu5</code> 后的数据：<code>13×13×256</code></li>
<li><code>pool5</code> 的核：<code>3×3</code> ；步长：<code>2</code></li>
<li><code>pool5</code> 后的数据：<code>6×6×256</code> 【<code>(13-3+1)/2 = 6</code>】</li>
<li>输出：<code>6×6×256</code></li>
</ol>
<p><code>conv5</code> 层有 max pool，没有 norm 层</p>
<h4 id="第六层：全连接层（fc6）"><a href="#第六层：全连接层（fc6）" class="headerlink" title="第六层：全连接层（fc6）"></a>第六层：全连接层（<code>fc6</code>）</h4><ol>
<li>输入数据：<code>6×6×256</code></li>
<li>全连接输出：<code>4096×1</code></li>
<li><code>relu6</code> 后的数据：<code>4096×1</code></li>
<li><code>dropout6</code> 后数据：<code>4096×1</code></li>
<li>输出：<code>4096×1</code></li>
</ol>
<h4 id="第七层：全连接层（fc7）"><a href="#第七层：全连接层（fc7）" class="headerlink" title="第七层：全连接层（fc7）"></a>第七层：全连接层（<code>fc7</code>）</h4><ol>
<li>输入数据：<code>4096×1</code></li>
<li>全连接输出：4096×1</li>
<li><code>relu7</code> 后的数据：<code>4096×1</code></li>
<li><code>dropout7</code> 后数据：<code>4096×1</code></li>
<li>输出：<code>4096×1</code></li>
</ol>
<h4 id="第八层：全连接层（fc8）"><a href="#第八层：全连接层（fc8）" class="headerlink" title="第八层：全连接层（fc8）"></a>第八层：全连接层（<code>fc8</code>）</h4><ol>
<li>输入数据：<code>4096×1</code></li>
<li>全连接输出：<code>1000</code></li>
<li>输出一千种分类的概率</li>
</ol>
<p>Alexnet 网络定义如下（<code>alexnet_inference.py</code>）：</p>
<pre><code class="python">import tensorflow as tf

def print_layers(layer):
  print(layer.op.name, &#39; &#39;, layer.get_shape().as_list())

def inference(input_tensor, keep_prob, num_classes):
    # conv1
    with tf.name_scope(&#39;conv1&#39;):
        conv1_weights = tf.get_variable(
            &#39;weight1&#39;,
            [11, 11, 3, 96],
            initializer=tf.truncated_normal_initializer(stddev=0.1)
            )

        conv1_biases = tf.get_variable(
            &#39;bias1&#39;,
            [96],
            initializer=tf.constant_initializer(0.)
            )

        conv1 = tf.nn.conv2d(
            input_tensor,
            conv1_weights,
             strides=[1, 4, 4, 1],
             padding=&#39;VALID&#39;
             )

        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))

        pool1 = tf.nn.max_pool(
            relu1, 
            ksize=[1, 3, 3, 1],
            strides=[1, 2, 2, 1],
            padding=&#39;VALID&#39;
            )

        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1, alpha=1e-3/9, beta=0.75, name=&#39;norm1&#39;) 

    # conv2
    with tf.name_scope(&#39;conv2&#39;):
        conv2_weights = tf.get_variable(
            &#39;weight2&#39;,
            [5, 5, 96, 256],
            initializer=tf.truncated_normal_initializer(stddev=0.1)
            )

        conv2_biases = tf.get_variable(
            &#39;bias2&#39;,
            [256],
            initializer=tf.constant_initializer(0.)
            )

        conv2 = tf.nn.conv2d(
            norm1,
            conv2_weights,
             strides=[1, 1, 1, 1],
             padding=&#39;SAME&#39;
             )
        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))
        pool2 = tf.nn.max_pool(
            relu2, 
            ksize=[1, 3, 3, 1],
            strides=[1, 2, 2, 1],
            padding=&#39;VALID&#39;
            )

        norm2 = tf.nn.lrn(pool2, depth_radius=4, bias=1, alpha=1e-3/9, beta=0.75, name=&#39;norm1&#39;) 

    # conv3
    with tf.name_scope(&#39;conv3&#39;):
        conv3_weights = tf.get_variable(
            &#39;weight3&#39;,
            [3, 3, 256, 384],
            initializer=tf.truncated_normal_initializer(stddev=0.1)
            )

        conv3_biases = tf.get_variable(
            &#39;bias3&#39;,
            [384],
            initializer=tf.constant_initializer(0.)
            )

        conv3 = tf.nn.conv2d(
            pool2,
            conv3_weights,
            strides=[1, 1, 1, 1],
            padding=&#39;SAME&#39;
             )

        relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_biases))

    # conv4
    with tf.name_scope(&#39;conv4&#39;):
        conv4_weights = tf.get_variable(
            &#39;weight4&#39;,
            [3, 3, 384, 384],
            initializer=tf.truncated_normal_initializer(stddev=0.1)
            )

        conv4_biases = tf.get_variable(
            &#39;bias4&#39;,
            [384],
            initializer=tf.constant_initializer(0.)
            )

        conv4 = tf.nn.conv2d(
            relu3,
            conv4_weights,
             strides=[1, 1, 1, 1],
             padding=&#39;SAME&#39;
             )

        relu4 = tf.nn.relu(tf.nn.bias_add(conv4, conv4_biases))


    # conv5
    with tf.name_scope(&#39;conv5&#39;):
        conv5_weights = tf.get_variable(
            &#39;weight5&#39;,
            [3, 3, 384, 256],
            initializer=tf.truncated_normal_initializer(stddev=0.1)
            )

        conv5_biases = tf.get_variable(
            &#39;bias5&#39;,
            [256],
            initializer=tf.constant_initializer(0.)
            )

        conv5 = tf.nn.conv2d(
            relu4,
            conv5_weights,
             strides=[1, 1, 1, 1],
             padding=&#39;SAME&#39;
             )

        relu5 = tf.nn.relu(tf.nn.bias_add(conv5, conv5_biases))
        pool5 = tf.nn.max_pool(
            relu5, 
            ksize=[1, 3, 3, 1],
            strides=[1, 2, 2, 1],
            padding=&#39;VALID&#39;
            )


    # fc6
    with tf.name_scope(&#39;fc6&#39;):
        flattened = tf.reshape(pool5, shape=[-1, 6*6*256])
        weights = tf.Variable(tf.truncated_normal(
            [6*6*256, 4096],
            dtype=tf.float32,
            stddev=1e-1), name=&#39;weights&#39;)

        biases = tf.Variable(tf.constant(0., shape=[4096], dtype=tf.float32),
                            name=&#39;biases&#39;)
        relu6 = tf.nn.relu(tf.nn.xw_plus_b(flattened, weights, biases))
        dropout6 = tf.nn.dropout(relu6, keep_prob)



    # fc7
    with tf.name_scope(&#39;fc7&#39;):
        weights = tf.Variable(tf.truncated_normal(
            [4096, 4096],
            dtype=tf.float32,
            stddev=1e-1), name=&#39;weights&#39;)

        biases = tf.Variable(tf.constant(0., shape=[4096], dtype=tf.float32),
                            name=&#39;biases&#39;)
        relu7 = tf.nn.relu(tf.nn.xw_plus_b(dropout6, weights, biases))
        dropout7 = tf.nn.dropout(relu7, keep_prob)

    with tf.name_scope(&#39;fc8&#39;):
        weights = tf.Variable(tf.truncated_normal(
            [4096, num_classes],
            dtype=tf.float32,
            stddev=1e-1), name=&#39;weights&#39;)

        biases = tf.Variable(tf.constant(0., shape=[num_classes], dtype=tf.float32),
                            name=&#39;biases&#39;)
        fc8 = tf.nn.xw_plus_b(dropout7, weights, biases)
    return fc8

</code></pre>
<p>Alexnet 网络的训练过程（<code>alexnet_train.py</code>）：</p>
<pre><code class="python">import os
import numpy as np
import tensorflow as tf
import alexnet_inference
import glob
from datetime import datetime

# 配置神经网络的参数
TRAIN_BATCH_SIZE = 10
TEST_BATCH_SIZE = 10
LEARNING_RATE = 1e-3
DROPOUT_RATE = 0.25
NUM_CLASSES = 2 # 类别标签
NUM_EPOCHS = 10

# 模型保存的路径和文件名
MODEL_SAVE_PATH = &quot;model/&quot;
MODEL_NAME = &quot;model.ckpt&quot;
TENSORBOARD_PATH = &quot;tensorboard/&quot; # 存储tensorboard文件

TOTAL_DATASET = &#39;train/&#39;
TRAIN_DATASET_PATH = [&#39;train/cat/&#39;,
                    &#39;train/dog/&#39;]

TEST_DATASET_PATH = [&#39;test/&#39;]
LABEL = [&#39;cat&#39;,
         &#39;dog&#39;]

# 将图像转换为三维数据，将标签转换为one-hot形式
def parse_image(file_name, label):  
    image_string = tf.read_file(file_name) 
    image_decoded = tf.image.decode_jpeg(image_string, channels=3) 
    image_resized = tf.image.resize_images(image_decoded, [227, 227]) # 将图片居中
    image_centered = tf.subtract(image_resized, [123.68, 116.779, 103.939]) 
    image = image_centered[:, :, ::-1]      # 将RGB转换为BGR 
    label = tf.one_hot(label, NUM_CLASSES) 
    return image, label

# 打乱图片顺序
def shuffle_images(train_images, train_labels):
    permutation = np.random.permutation(len(train_labels))
    images = []
    labels = []
    for i in permutation:
        images.append(train_images[i])
        labels.append(train_labels[i])
    return images, labels

# 将图片和标签先转化为tensor，再创建Dataset，
def process_image(train_images, train_labels):
    image_num = len(train_images)
    train_images = tf.convert_to_tensor(train_images, dtype=tf.string)
    train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)
    dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
    dataset = dataset.map(parse_image).batch(TRAIN_BATCH_SIZE)
    return dataset, image_num

def train():
    if not os.path.isdir(MODEL_SAVE_PATH):
        os.mkdir(MODEL_SAVE_PATH)

    if not os.path.isdir(TOTAL_DATASET):
        os.mkdir(TOTAL_DATASET)

    if not os.path.isdir(list(TRAIN_DATASET_PATH)[0]) or not os.path.isdir(list(TRAIN_DATASET_PATH)[1]):
        os.mkdir(list(TRAIN_DATASET_PATH)[0])
        os.mkdir(list(TRAIN_DATASET_PATH)[1])

    # 处理训练集图片
    train_images = []
    train_labels = []

    for path in TRAIN_DATASET_PATH:
        train_images[len(train_images): len(train_images)] = np.array(glob.glob(path + &#39;*jpg&#39;)).tolist()
    for path in train_images:
        # file_name表示文件名
        file_name = path.split(&#39;/&#39;)[-1]
        for i in range(NUM_CLASSES):
            if LABEL[i] in file_name:
                train_labels.append(i)
                break

    train_images, train_labels = shuffle_images(train_images, train_labels)
    train_data, train_data_size = process_image(train_images, train_labels)

    # 将training的过程分为train_batches_per_epoch次迭代完成，每次迭代包含的元素为TRAIN_BATCH_SIZE个
    train_batches_per_epoch = int(np.floor(train_data_size) / TRAIN_BATCH_SIZE)
    train_iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)
    train_initializer = train_iterator.make_initializer(train_data)     # 创建dataset迭代器，需要进行初始化
    train_next_batch = train_iterator.get_next() 

    # 处理测试集图片
    test_images = []
    test_labels = []

    for path in TEST_DATASET_PATH:
        test_images[len(test_images): len(test_images)] = np.array(glob.glob(path + &#39;*jpg&#39;)).tolist()
    for path in test_images:
        # file_name表示文件名
        file_name = path.split(&#39;/&#39;)[-1]
        for i in range(NUM_CLASSES):
            if LABEL[i] in file_name:
                test_labels.append(i)
                break

    test_images, test_labels = shuffle_images(test_images, test_labels)
    test_data, test_data_size = process_image(test_images, test_labels)

    # 将training的过程分为train_batches_per_epoch次迭代完成，每次迭代包含的元素为TRAIN_BATCH_SIZE个
    test_batches_per_epoch = int(np.floor(test_data_size) / TEST_BATCH_SIZE)
    test_iterator = tf.data.Iterator.from_structure(test_data.output_types, test_data.output_shapes)
    test_initializer = test_iterator.make_initializer(test_data)     # 创建dataset迭代器，需要进行初始化
    test_next_batch = test_iterator.get_next() 


    x = tf.placeholder(tf.float32, [None, 227, 227, 3])
    y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])
    keep_prob = tf.placeholder(tf.float32)
    y = alexnet_inference.inference(x, keep_prob, NUM_CLASSES)

    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y, labels=y_))
    train_op = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)), tf.float32))

    init_op = tf.global_variables_initializer()

    # tensorboard
    # tf.summary.scalar(&#39;loss,&#39;)

    saver = tf.train.Saver()

    with tf.Session() as sess:
        sess.run(init_op)

        print(&#39;{}: Start training...&#39;.format(datetime.now()))
        for epoch in range(NUM_EPOCHS):
            sess.run(train_initializer)
            print(&#39;ecpoch number: {} start&#39;.format(epoch+1))

            # train
            # 200 / 10 = 20
            for step in range(train_batches_per_epoch):
                image_batch, label_batch = sess.run(train_next_batch)
                loss_value, train_step = sess.run([loss, train_op], feed_dict={
                                                                  x: image_batch,
                                                                  y_: label_batch,
                                                                  keep_prob: DROPOUT_RATE
                                                })
                if step % 10 ==0:
                    print(&quot;After %d training step(s), loss on training batch is %f.&quot; % (epoch, loss_value))


            # accuracy
            sess.run(test_initializer)
            test_accuracy = 0
            test_count = 0
            for _ in range(test_batches_per_epoch):
                image_batch, label_batch = sess.run(test_next_batch)
                temp_accuracy = sess.run(accuracy, feed_dict={x: image_batch,
                                                              y_: label_batch,
                                                              keep_prob: 1.0})
                test_accuracy += temp_accuracy
                test_count += 1
            try:
                test_accuracy /= test_count
            except:
                print(&#39;ZeroDivisionError!&#39;)
            print(&quot;Accuracy = {:.4f}&quot;.format(test_accuracy))


            # save model
            print(&quot;{}: Saving model... &quot;.format(datetime.now()))
            saver.save(sess, os.path.join(MODEL_SAVE_PATH,MODEL_NAME), global_step=epoch)

def main(argv=None):
    train()

main()
# if __name__ == &#39;__main__&#39;:
#     tf.app.run()
</code></pre>
<h2 id="3-创新点"><a href="#3-创新点" class="headerlink" title="3. 创新点"></a>3. 创新点</h2><h4 id="1-ReLU-Nonlinearity"><a href="#1-ReLU-Nonlinearity" class="headerlink" title="1. ReLU Nonlinearity"></a>1. ReLU Nonlinearity</h4><p>一般神经元的激活函数会选择 <code>sigmoid</code> 函数或者 <code>tanh</code> 函数，然而 Alex 发现在训练时间的梯度衰减方面，这些非线性饱和函数要比非线性非饱和函数慢很多。在 AlexNet 中用的非线性非饱和函数是 <code>f=max(0,x)</code>，即 ReLU。实验结果表明，要将深度网络训练至 training error rate 达到 <code>25%</code> 的话，ReLU 只需 <code>5</code> 个 epochs 的迭代，但 <code>tanh</code> 需要 <code>35</code> 个 epochs 的迭代，用 ReLU 比 tanh 快 <code>6</code> 倍。</p>
<h4 id="2-双-GPU-并行运行"><a href="#2-双-GPU-并行运行" class="headerlink" title="2. 双 GPU 并行运行"></a>2. 双 GPU 并行运行</h4><p>为提高运行速度和提高网络运行规模，采用双 GPU 的设计模式。并且规定  GPU只能在特定的层进行通信交流。其实就是每一个 GPU 负责一半的运算处理。值得注意的是，虽然 one-GPU 网络规模只有 two-GPU 的一半，但其实这两个网络其实并非等价的。</p>
<h4 id="3-LRN-局部响应归一化"><a href="#3-LRN-局部响应归一化" class="headerlink" title="3. LRN 局部响应归一化"></a>3. LRN 局部响应归一化</h4><h1 id="ZF-Net"><a href="#ZF-Net" class="headerlink" title="ZF Net"></a>ZF Net</h1><p>源自论文《<em>Visualizing and Understanding Convolutional Networks</em> 》</p>
<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1.  概述"></a>1.  概述</h2><p>本文设计了一种可以可视化卷积层中 feature map 的系统，通过可视化每层 layer 的某些 activation 来探究 CNN 网络究竟是怎样“学习”的，同时文章通过可视化了 AlexNet 发现了因为结构问题，导致有“影像重叠”（aliasing artifacts），因此对网络进行了改进，设计出了 ZF Net。</p>
<p>文章通过把 activation（feature map 中的数值）映射回输入像素的空间，去了解什么样的输入模式会生成 feature map  中的一个给定activation，这个模型主要通过反卷积（deconvolution），反向池化（Unpooling）与“反向激活”（Rectification），其实就是把整个 CNN 网络倒过来，另外值得说一下的是，并不是完全倒过来，只是近似，所有的“反向”操作都是近似，主要是使得从各层 layer 的尺度还原到在原始图像中相应大小的尺度。</p>
<p>同时文章还分析了每层 layer 学习到了什么，以及可视化最强 activation 的演化过程来关系模型的收敛过程，同时也利用遮挡某些部位来学习 CNN 是学习 object 本身还是周围环境。</p>
<h2 id="2-可视化结构"><a href="#2-可视化结构" class="headerlink" title="2.  可视化结构"></a>2.  可视化结构</h2><h3 id="2-1-Unpooling"><a href="#2-1-Unpooling" class="headerlink" title="2.1  Unpooling"></a>2.1  Unpooling</h3><p>要想完全还原 max pooling 是不太现实的，除非记录每一层 feature，那有些得不偿失，文章通过记录池化过程中最大激活值所在位置以及数值，在 unpooling 的时候，还原那个数值，其他的位置设为 <code>0</code>，从而近似“反向池化”，具体如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-ccb747524abe3965.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h3 id="2-2-Rectification"><a href="#2-2-Rectification" class="headerlink" title="2.2  Rectification"></a>2.2  Rectification</h3><p>CNN 使用 ReLU 确保每层输出的激活之都是正数，因此对于反向过程，同样需要保证每层的特征图为正值，也就是说这个反激活过程和激活过程没有什么差别，都是直接采用 ReLU 函数。</p>
<h3 id="2-3-Filtering"><a href="#2-3-Filtering" class="headerlink" title="2.3  Filtering"></a>2.3  Filtering</h3><p>卷积过程使用学习到的过滤器对 feature map 进行卷积，为近似反转这个过程，反卷积使用该卷积核的转置来进行卷积操作。</p>
<p>注意：在上述重构过程中没有使用任何对比度归一化操作</p>
<h2 id="3-Feature-Visualization"><a href="#3-Feature-Visualization" class="headerlink" title="3.  Feature Visualization"></a>3.  Feature Visualization</h2><p>在 ImageNet 验证集上使用反卷积进行特征图的可视化，如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-c32de690d0152aab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-f87dee560b3aa6c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-fc3409361cffca63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于一个给定的 feature map，展示了响应最大的九张响应图，每个响应图向下映射到原图像素空间，右面的原图通过找到在原图的感受野来截取对应的原图。</p>
<p>通过观察可以发现，来自每个层中的投影显示出网络中特征的分层特性。第二层响应角落和其他的边缘/颜色信息，层三具有更复杂的不变性，捕获相似的纹理，层四显示了显著的变化，并且更加类别具体化，层五则显示了具有显著姿态变化的整个对象，所以这就是常说的 CNN 结构前几层通常学习简单的线条纹理，一些共性特征，后面将这些特征组合成 不同的更丰富的语义内容。</p>
<h2 id="4-Feature-Evolution-during-Training"><a href="#4-Feature-Evolution-during-Training" class="headerlink" title="4.  Feature Evolution during Training"></a>4.  Feature Evolution during Training</h2><p>文中对于一个 layer 中给定的 feature map，图中给出在训练 epochs 在 <code>[1,2,5,10,20,30,40,64]</code>时，训练集对该 feature map 响应最大的可视化图片，如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-21fccc427dc0c7d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>从图中可以看出，较低层 <code>(L1, L2)</code> 只需要几个 epochs 就可以完全收敛，而高层 <code>(L5)</code> 则需要很多次迭代，需要让模型完全收敛之后。这一点正好与深层网络的梯度弥散现象正好相反，但是这种底层先收敛，然后高层再收敛的现象也很符合直观。</p>
<h2 id="5-Feature-Invariance"><a href="#5-Feature-Invariance" class="headerlink" title="5.  Feature Invariance"></a>5.  Feature Invariance</h2><p><img src="https://upload-images.jianshu.io/upload_images/1351548-a4c0c7c1b274a786.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图显示出了相对于未变换的特征，通过垂直平移，旋转和缩放的 <code>5</code> 个样本图像在可视化过程中的变化。小变换对模型的第一层有着显著的影响，但对顶层影响较小，对于平移和缩放是准线性的。网络输出对于平移和缩放是稳定的。但是一般来说，除了具有旋转对称性的物体来说，输出来旋转来说是不稳定的（这说明了卷积操作对于平移和缩放具有很好的不变性，而对于旋转的不变性较差）。</p>
<h2 id="6-ZF-Net"><a href="#6-ZF-Net" class="headerlink" title="6.  ZF Net"></a>6.  ZF Net</h2><p><img src="https://upload-images.jianshu.io/upload_images/1351548-b8fde413c0efc595.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可视化训练模型不但可以洞察 CNN 的操作，也可以帮助我们在前几层选择更好的模型架构。通过可视化 AlexNet 的前两层（图中b，d），就可以看出问题:</p>
<ol>
<li>第一层 filter 是非常高频和低频的信息，中间频率的filter很少覆盖</li>
<li>第二层的可视化有些具有混叠效应，由于第一层比较大的 stride</li>
</ol>
<p>为了解决这些问题：</p>
<ol>
<li><p>将第一层的 filter 的尺寸从 <code>11x11</code> 减到 <code>7x7</code></p>
</li>
<li><p>缩小间隔，从 <code>4</code> 变为 <code>2</code>。</p>
</li>
</ol>
<p>这两个改动形成的新结构，获取了更多的信息，而且提升了分类准确率。</p>
<h2 id="7-实验"><a href="#7-实验" class="headerlink" title="7.  实验"></a>7.  实验</h2><p>首先，作者进行了网络结构尺寸调整实验。去除掉包含大部分网络参数最后两个全连接层之后，网络性能下降很少；去掉中间两层卷积层之后，网络性能下降也很少；但是当把上述的全连接层和卷积层都去掉之后，网络性能急剧下降，由此作者得出结论：模型深度对于模型性能很重要，存在一个最小深度，当小于此深度时，模型性能大幅下降。<br>作者固定了通过 ImageNet pre-train 网络的权值，只是使用新数据训练了 softmax 分类器，效果非常好。这就形成了目前的人们对于卷积神经网络的共识：卷积网络相当于一个特征提取器。特征提取器是通用的，因为 ImageNet 数据量，类别多，所以由 ImageNet 训练出来的特征提取器更具有普遍性。也正是因为此，目前的卷积神经网络的 Backbone Network 基本上都是 ImageNet 上训练出来的网络。</p>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>本文最大的贡献在于<strong>通过使用可视化技术揭示了神经网络各层到底在干什么，起到了什么作用</strong>。可视化技术依赖于反卷积操作，即卷积的逆过程，将特征映射到像素上。具体过程如下图所示： </p>
<ul>
<li><strong>Unpooling</strong>：在卷积神经网络中，最大池化是不可逆的，作者采用近似的实现，使用一组转换变量 switch 记录<strong>每个池化区域最大值的位置</strong>。在反池化的时候，将最大值返回到其所应该在的位置，其他位置用 <code>0</code> 补充。 </li>
<li><strong>Rectification</strong>：反卷积的时候也同样利用 ReLU 激活函数 </li>
<li><strong>Filtering</strong>：解卷积网络中利用卷积网络中相同的 filter 的转置应用到 Rectified Unpooled Maps，也就是对 filter 进行水平方向和垂直方向的翻转。</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-454028f1fe5834fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可视化不仅能够看到一个训练完的模型的内部操作，而且还能够帮助改进网络结构从而提高网络性能。ZF Net 模型是在 AlexNet 基础上进行改动，网络结构上并没有太大的突破。差异表现在，AlexNet 是用两块 GPU 的稀疏连接结构，而 ZF Net 只用了一块 GPU 的稠密链接结构；改变了 AlexNet 的第一层，将过滤器的大小由 <code>11x 11</code> 变成 <code>7x7</code>，并且将步长由 <code>4</code> 变成 <code>2</code> ，使用更小的卷积核和步长，保留更多特征；将<code>3</code>，<code>4</code>，<code>5</code> 层变成了全连接。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-9143fb452ee8eeb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="VGG-Net"><a href="#VGG-Net" class="headerlink" title="VGG Net"></a>VGG Net</h1><p>出自论文 《<em>Very Deep Convolutional Networks for Large-Scale Image Recognition</em>》</p>
<h2 id="1-概括"><a href="#1-概括" class="headerlink" title="1.  概括"></a>1.  概括</h2><p>VGG 模型由牛津大学 VGG 组提出。<strong>VGG 全部使用了 <code>3x3</code> 的卷积核和 <code>2x2</code> 最大池化核通过不断加深网络结构来提神性能</strong>。采用堆积的小卷积核优于采用大的卷积核，因为多层非线性层可以增加网络深层来保证学习更复杂的模式，而且所需的参数还比较少。</p>
<p>VGG 论文给出了一个非常振奋人心的结论：卷积神经网络的深度增加和小卷积核的使用对网络的最终分类识别效果有很大的作用。</p>
<h2 id="2-创新点"><a href="#2-创新点" class="headerlink" title="2.  创新点"></a>2.  创新点</h2><p>VGG 全部使用 <code>3x3</code> 的卷积核和 <code>2x2</code> 的池化核，通过不断加深网络结构来提升性能。网络层数的增长并不会带来参数量上的爆炸，因为参数量主要集中在最后三个全连接层中。同时，两个 <code>3x3</code> 卷积层的串联相当于 <code>1</code> 个 <code>5x5</code> 的卷积层（在像素关联性上，两个 <code>3x3</code> 的卷积可连接到范围与一个 <code>5x5</code> 卷积可连接到的范围相当），<code>3</code> 个 <code>3x3</code> 的卷积层串联相当于 <code>1</code> 个 <code>7x7</code> 的卷积层，即 <code>3</code> 个 <code>3x3</code> 卷积层的感受野大小相当于 <code>1</code> 个 <code>7x7</code> 的卷积层。但是 <code>3</code> 个 <code>3x3</code> 的卷积层参数量只有 <code>7x7</code> 的一半左右，同时前者可以有 <code>3</code> 个非线性操作，而后者只有 <code>1</code> 个非线性操作，这样使得前者对于特征的学习能力更强。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-4d78d45d7516d4c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p> 使用 <code>1x1</code> 的卷积层来增加线性变换，输出的通道数量上并没有发生改变。这里提一下 <code>1x1</code> 卷积层的其他用法，<code>1x1</code> 的卷积层常被用来提炼特征，即多通道的特征组合在一起，凝练成较大通道或者较小通道的输出，而每张图片的大小不变。有时 <code>1x1</code> 的卷积神经网络还可以用来替代全连接层。</p>
<p>VGG 在训练的时候先训级别 <code>A</code> 的简单网络，再复用 <code>A</code> 网络的权重来初始化后面的几个复杂模型，这样收敛速度更快。VGG 作者总结出 LRN 层作用不大，越深的网络效果越好，<code>1x1</code> 的卷积也是很有效的，但是没有 <code>3x3</code> 的卷积效果好，因为 <code>3x3</code> 的网络可以学习到更大的空间特征。</p>
<h2 id="3-VGG-网路结构："><a href="#3-VGG-网路结构：" class="headerlink" title="3.  VGG 网路结构："></a>3.  VGG 网路结构：</h2><p>网络的输入为 <code>224x224</code>的 RGB 图片，后面跟卷积层，卷积核的大小基本都为 <code>3x3</code> ，这一种最小的、可以保留图片空间分辨率的卷积核，步长为 <code>1</code> 个像素，偶尔会有 <code>1x1</code> 的卷积核，这就相当于加入了一个非线性变换而已。再往后接 pooling 层，它的大小为 <code>2x2</code>，步长为 <code>2</code> 个像素，并且采用 max pooling 的方法；再往后就是三层的全连层，第一层为 <code>4096</code> 个单元，第二层为 <code>4096</code> 个单元，第三层为 <code>1000</code> 个单元，即对应 <code>1000</code> 个类别，根据这 <code>1000</code> 个单元的输出其实这样就可以分类了；再往后为一个 softmax 层，目的其实就是用于计算网络的代价函数时用于求误差。</p>
<p>VGG 的网络结构如下图所示。VGG 包含很多级别的网络，深度从 <code>11</code> 层到 <code>19</code> 层不等，比较常用的是 VGG-16 和 VGG-19。VGG 把网络分成了 <code>5</code> 段，每段都把多个 <code>3x3</code> 的卷积网络串联在一起，每段卷积后面接一个最大池化层，最后面是 <code>3</code> 个全连接层和一个 softmax 层。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-d5d0636a64ab73a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在上图中，如网络中的 <code>conv3-512</code>，表示网络的卷积核的大小为 <code>3x3</code>, 共有 <code>512</code> 个 feature map。另外，max pool 的具体配置没有写出来。</p>
<p>VGG 网络参数（ 下图是整个模型的参数总量，单位是 millions，参数来自卷积层和全连接层，主要是全连接层）：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-161341629f6b9b6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<ul>
<li>两个堆叠的卷积层（卷积核为 <code>3x3</code>）有限感受野是 <code>5x5</code>，三个堆叠的卷积层（卷积核为 <code>3x3</code>）的感受野为 <code>7x7</code>，故可以堆叠含有小尺寸卷积核的卷积层来代替具有大尺寸的卷积核的卷积层，并且能够使得感受野大小不变，而且多个 <code>3x3</code> 的卷积核比一个大尺寸卷积核有更多的非线性（每个堆叠的卷积层中都包含激活函数），使得 decision function更加具有判别性。</li>
<li>假设一个 <code>3</code> 层的 <code>3x3</code> 卷积层的输入和输出都有 <code>C</code> channels，堆叠的卷积层的参数个数为 <img src="https://www.zhihu.com/equation?tex=3%5Ctimes+%5Cleft+%28+3%5E%7B2%7D%5Ccdot+C%5E%7B2%7D+%5Cright+%29+%3D+27C%5E%7B2%7D" alt="3\times \left ( 3^{2}\cdot C^{2} \right ) = 27C^{2}"> ，而等同的一个单层的 <code>7x7</code> 卷积层的参数为 <img src="https://www.zhihu.com/equation?tex=7%5E%7B2%7D+%5Ccdot+C%5E%7B2%7D+%3D49C%5E%7B2%7D" alt="7^{2} \cdot C^{2} =49C^{2}"></li>
</ul>
<p>可以看到 VGG-D 使用了一种块结构：多次重复使用统一大小的卷积核来提取更复杂和更具有表达性的特征。VGG 系列中，最多使用是 VGG-16，下图来自 Andrew Ng 深度学习里面对 VGG-16 架构的描述。如图所示，在 VGG-16 的第三、四、五块：<code>256</code>、<code>512</code>、<code>512</code> 个过滤器依次用来提取复杂的特征，其效果就等于一个带有3各卷积层的大型 <code>512x512</code> 大分类器。</p>
<p> <img src="https://upload-images.jianshu.io/upload_images/1351548-29d5f1b929690b4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h2><h3 id="4-1-训练初始化参数"><a href="#4-1-训练初始化参数" class="headerlink" title="4.1 训练初始化参数"></a>4.1 训练初始化参数</h3><p>VGG 采用了 min-batch gradient descent 去优化 multinomial logistic regression objective：</p>
<ul>
<li><code>batch size</code>：<code>256</code></li>
<li><code>momentum</code>：<code>0.9</code></li>
<li><code>learning_rate</code>：<code>1e−2</code>（训练过程中，降低了三次，每次减少 <code>0.1</code>)</li>
<li><code>max iterations</code>：<code>370K</code> 次迭代，<code>74</code> epochs</li>
</ul>
<p>正则化方法：</p>
<ul>
<li>增加了对权重的正则化：<img src="https://upload-images.jianshu.io/upload_images/1351548-c58d439505477c9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></li>
<li>对 FC 全连接层进行 dropout 正则化，<code>dropout ratio = 0.5</code></li>
</ul>
<p>说明：虽然模型的参数和深度相比 AlexNet 有了很大的增加，但是模型的训练迭代次数却要求更少，这是因为：</p>
<ol>
<li>正则化 + 小卷积核</li>
<li>特定层的预初始化</li>
</ol>
<p>初始化策略（VGG 在训练时用到的 track）：</p>
<ul>
<li><p>首先，随机初始化网络结构 <code>A</code>（<code>A</code> 的深度较浅）</p>
</li>
<li><p>利用 <code>A</code> 的网络参数，给其他的模型进行初始化（初始化前 <code>4</code> 层卷积 + 全连接层，其他的层采用正态分布随机初始化，<code>mean=0</code>，<code>variance=1e−2</code> ，<code>biases = 0</code>，即均值为 <code>0</code>，方差为 <code>1e-2</code>）</p>
</li>
</ul>
<p>简而言之，就是先训练级别 <code>A</code> 的简单网络，再复用 <code>A</code> 网络的权重来初始化后面的几个复杂模型，这样训练收敛的速度更快。最后证明，即使随机初始化所有的层，模型也能训练的很好</p>
<h3 id="4-2-训练输入"><a href="#4-2-训练输入" class="headerlink" title="4.2 训练输入"></a>4.2 训练输入</h3><p>采用随机裁剪的方式，获取固定大小 <code>224x224</code> 的输入图像，并且采用了随机水平镜像和随机平移图像通道来丰富数据。</p>
<ul>
<li>Training image size：令 <code>S</code> 为图像的最小边，如果最小边 <code>S=224</code>，则直接在图像上进行 <code>224x224</code> 区域随机裁剪，这时相当于裁剪后的图像能够几乎覆盖全部的图像信息；如果最小边 <code>S&gt;&gt;224</code>，那么做完 <code>224x224</code> 区域随机裁剪后，每张裁剪图，只能覆盖原图的一小部分内容【注：因为训练数据的输入为 <code>224x224</code>，从而图像的最小边 <code>S</code>，不应该小于 <code>224</code>】</li>
<li>数据生成方式：首先对图像进行放缩变换，将图像的最小边缩放到 <code>S</code> 大小，然后：<ul>
<li>方法 1：在 <code>S=224</code> 和 <code>S=384</code> 的尺度下，对图像进行 <code>224x224</code> 区域随机裁剪</li>
<li>方法 2：令 <code>S</code> 随机的在<img src="https://upload-images.jianshu.io/upload_images/1351548-d6a7404f6508b0b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">区间内值，放缩完图像后，再进行随机裁剪（其中<img src="https://upload-images.jianshu.io/upload_images/1351548-5e7c758a60eca0f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">，<img src="https://upload-images.jianshu.io/upload_images/1351548-f66a6e48869b494a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">）</li>
</ul>
</li>
<li>预测（作者考虑了两种预测方式）：<ul>
<li>方法 1：multi-crop，即对图像进行多样本的随机裁剪，然后通过网络预测每一个样本的结构，最终对所有结果平均</li>
<li>方法 2：densely， 利用 FCN 的思想，将原图直接送到网络进行预测，将最后的全连接层改为 <code>1x1</code> 的卷积，这样最后可以得出一个预测的 score map，再对结果求平均</li>
<li>对于上述这两种方法：<ul>
<li>multi-crops 相对于 FCN 效果要好</li>
<li>multi-crops 相当于对于 dense evaluation 的补充，原因在于，两者在边界的处理方式不同：multi-crop 相当于 padding 补充 <code>0</code> 值，而 dense evaluation 相当于 padding 补充了相邻的像素值，并且增大了感受野</li>
<li>multi-crop 存在重复计算带来的效率的问题</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>在预测时，VGG 采用 Multi-Scale 的方法：即将图像 scale 到一个尺寸 <code>Q</code>，并将图片输入卷积网络计算。然后在最后一个卷积层使用滑窗的方式进行分类预测，将不同窗口的分类结果平均，再将不同尺寸 <code>Q</code> 的结果平均得到最后结果，这样可提高图片数据的利用率并提升预测准确率。在训练中，VGG 还使用 Multi-Scale 的方法做数据增强，将原始图像缩放到不同尺寸 <code>S</code>，然后再随机裁切 <code>224x224</code> 的图片，这样能增加很多数据量，对于防止模型过拟合有很不错的效果。实践中，作者令 <code>S</code> 在 <code>[256, 512]</code> 这个区间内取值，使用 Multi-Scale 获得多个版本的数据，并将多个版本的数据合在一起进行训练。</p>
<h2 id="5-实验效果（CLASSIFICATION-EXPERIMENTS-）"><a href="#5-实验效果（CLASSIFICATION-EXPERIMENTS-）" class="headerlink" title="5. 实验效果（CLASSIFICATION EXPERIMENTS ）"></a>5. 实验效果（CLASSIFICATION EXPERIMENTS ）</h2><h3 id="5-1-单尺度（SINGLE-SCALE-EVALUATION-）"><a href="#5-1-单尺度（SINGLE-SCALE-EVALUATION-）" class="headerlink" title="5.1 单尺度（SINGLE SCALE EVALUATION ）"></a>5.1 单尺度（SINGLE SCALE EVALUATION ）</h3><p><img src="https://upload-images.jianshu.io/upload_images/1351548-b03aa8a79529037c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>结论：</p>
<ul>
<li>模型 E（VGG19）的效果最好，即网络越深，效果越好</li>
<li>同一种模型，随机 scale jittering 的效果好于固定 <code>S</code> 大小的 <code>256</code>，<code>384</code> 两种尺度，即 scale jittering （<img src="https://upload-images.jianshu.io/upload_images/1351548-a97be47f04476592.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">）数据增强能更准确的提取图像多尺度信息</li>
</ul>
<h3 id="5-2-多尺度（MULTI-SCALE-EVALUATION-）"><a href="#5-2-多尺度（MULTI-SCALE-EVALUATION-）" class="headerlink" title="5.2 多尺度（MULTI-SCALE EVALUATION ）"></a>5.2 多尺度（MULTI-SCALE EVALUATION ）</h3><p>下图为 VGG 使用 Multi-Scale 训练时得到的结果，可以看到 D 和 E 都可以达到 <code>7.5%</code> 的错误率。最终提交到 ILSVRC 2014 的版本是仅使用 Single-Scale 的 <code>6</code> 个不同等级的网络与 Multi-Scale 的 <code>D</code> 网络的融合，达到了 <code>7.3%</code> 的错误率。不过比赛结束后作者发现只融合 Multi-Scale 的 <code>D</code> 和 <code>E</code> 可以达到更好的效果，错误率达到 <code>7.0%</code>，再使用其他优化策略最终错误率可达到 <code>6.8%</code> 左右，非常接近同年的冠军Google Inception-Net。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-0f86d90bfce75277.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>结论：</p>
<ul>
<li>对比单尺度预测，多尺度综合预测能够提升预测的精度</li>
<li>同单尺度预测，多尺度预测也证明了 scale jittering 的作用</li>
</ul>
<h3 id="5-3-多尺度裁剪（MULTI-CROP-EVALUATION-）"><a href="#5-3-多尺度裁剪（MULTI-CROP-EVALUATION-）" class="headerlink" title="5.3 多尺度裁剪（MULTI-CROP EVALUATION ）"></a>5.3 多尺度裁剪（MULTI-CROP EVALUATION ）</h3><p><img src="https://upload-images.jianshu.io/upload_images/1351548-aa2dac233e97d455.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>结论：</p>
<ul>
<li>数据生成方式 multi-crop 效果略优于 dense，但作者上文也提高，精度的提高不足以弥补计算上的损失</li>
<li>multi-crop于dense 方法结合的效果最后，也证明了作者的猜想：multi-crop和dense两种方法互为补充</li>
</ul>
<p>同时，作者在对比各级网络时总结出了以下几个观点：</p>
<ol>
<li>LRN 层作用不大（VGG 不使用局部响应标准化，这种标准化并不能在 ILSVRC 数据集上提升性能，却导致更多的内存消耗和计算时间）</li>
<li>越深的网络效果越好</li>
<li><code>1x1</code> 的卷积也是很有效的，但是没有 <code>3x3</code> 的卷积好，大一些的卷积核可以学习更大的空间特征。</li>
</ol>
<p>在训练的过程中，比 AlexNet 收敛的要快一些，原因为：</p>
<ol>
<li>使用小卷积核和更深的网络进行的正则化</li>
<li>在特定的层使用了预训练得到的数据进行参数的初始化。</li>
</ol>
<p>对于较浅的网络，如网络 <code>A</code>，可以直接使用随机数进行随机初始化，而对于比较深的网络，则使用前面已经训练好的较浅的网络中的参数值对其前几层的卷积层和最后的全连接层进行初始化。</p>
<h3 id="5-4-网络融合（CONVNET-FUSION）"><a href="#5-4-网络融合（CONVNET-FUSION）" class="headerlink" title="5.4 网络融合（CONVNET FUSION）"></a>5.4 网络融合（CONVNET FUSION）</h3><p>其实就是多个网络模型的输出结果求平均再用于识别。在 ILSVRC 的比赛中大家都在用，为什么呢？因为这样做基本上稍微弄弄，都可以提高网络的识别率。原理可能就是所谓的不同网络模型之间的互补（complementarity，这也说明了网络的不稳定吧）。</p>
<h2 id="6-Q-amp-A"><a href="#6-Q-amp-A" class="headerlink" title="6. Q&amp;A"></a>6. Q&amp;A</h2><h4 id="Q1-为什么-3-个-3x3-的卷积可以代替-7x7-的卷积？"><a href="#Q1-为什么-3-个-3x3-的卷积可以代替-7x7-的卷积？" class="headerlink" title="Q1: 为什么 3 个 3x3 的卷积可以代替 7x7 的卷积？"></a>Q1: 为什么 <code>3</code> 个 <code>3x3</code> 的卷积可以代替 <code>7x7</code> 的卷积？</h4><ul>
<li><code>3</code> 个 <code>3x3</code> 的卷积，使用了 <code>3</code> 个非线性激活函数，增加了非线性表达能力，使得分割平面更具有可分性</li>
<li>减少了参数个数。对于 <code>C</code> 个通道的卷积核，<code>7x7</code> 含有参数 ，参数大大减少</li>
</ul>
<h4 id="Q2：2-个-3x3-卷积核可以来代替-5x5-卷积核？"><a href="#Q2：2-个-3x3-卷积核可以来代替-5x5-卷积核？" class="headerlink" title="Q2：2 个 3x3 卷积核可以来代替 5x5 卷积核？"></a>Q2：<code>2</code> 个 <code>3x3</code> 卷积核可以来代替 <code>5x5</code> 卷积核？</h4><p><code>5x5</code> 卷积看做一个小的全连接网络在 <code>5x5</code> 区域滑动，我们可以先用一个 <code>3x3</code> 的卷积滤波器卷积，然后再用一个全连接层连接这个 <code>3x3</code> 卷积输出，这个全连接层我们也可以看做一个 <code>3x3</code> 卷积层。这样我们就可以用两个 <code>3x3</code> 卷积级联（叠加）起来代替一个  <code>5x5</code> 卷积。</p>
<p>具体如下图所示：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1351548-0dfdf81f0a0cca52.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>
<h4 id="Q3-1x1-卷积核的作用？"><a href="#Q3-1x1-卷积核的作用？" class="headerlink" title="Q3: 1x1 卷积核的作用？"></a>Q3: <code>1x1</code> 卷积核的作用？</h4><ul>
<li><p>在不影响感受野的情况下，增加模型的非线性性</p>
</li>
<li><p><code>1x1</code> 卷积相当于线性变换，非线性激活函数起到非线性作用</p>
</li>
</ul>
<h4 id="Q4-网络深度对结果的影响（同年-google-也独立发布了深度为-22-层的网络-GoogleNet）"><a href="#Q4-网络深度对结果的影响（同年-google-也独立发布了深度为-22-层的网络-GoogleNet）" class="headerlink" title="Q4: 网络深度对结果的影响（同年 google 也独立发布了深度为 22 层的网络 GoogleNet）"></a>Q4: 网络深度对结果的影响（同年 google 也独立发布了深度为 <code>22</code> 层的网络 GoogleNet）</h4><ul>
<li>VGG 与 GoogleNet 模型都很深</li>
<li>都采用了小卷积</li>
<li>VGG 只采用 <code>3x3</code>，而 GoogleNet 采用 <code>1x1</code>，<code>3x3</code>，<code>5x5</code>，模型更加复杂（模型开始采用了很大的卷积核，来降低后面卷机层的计算）</li>
</ul>
<h2 id="7-VGG-TensorFlow-实现"><a href="#7-VGG-TensorFlow-实现" class="headerlink" title="7. VGG TensorFlow 实现"></a>7. VGG TensorFlow 实现</h2><h3 id="7-1-自己实现"><a href="#7-1-自己实现" class="headerlink" title="7.1 自己实现"></a>7.1 自己实现</h3><pre><code class="python">import tensorflow as tf
from tensorflow.contrib.layers import xavier_initializer_conv2d
from tensorflow.contrib.layers import flatten

# 定义卷积层
def conv(input_tensor, fileter_size, out_channels, step, name):
    in_channels = input_tensor.get_shape()[-1].value
    with tf.name_scope(name) as scope:
        weights = tf.get_variable(
                        shape=[fileter_size, fileter_size, in_channels, out_channels],
                        dtype=tf.float32,
                        initializer=xavier_initializer_conv2d(),
                        name=scope+&#39;weights&#39;)

        biases = tf.Variable(
                        tf.constant(value=0., shape=[out_channels], dtype=tf.float32),
                        trainable=True,
                        name=&#39;biases&#39;)

        relu = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input_tensor, 
                                                    weights, 
                                                    strides=[1, step, step, 1],
                                                    padding=&#39;SAME&#39;), 
                                    biases),
                        name=scope)
        return relu

# 最大池化
def max_pool(input_tensor, fileter_size, step, name):
    return tf.nn.max_pool(
                    input_tensor,
                    ksize=[1, fileter_size, fileter_size, 1],
                    strides=[1, step, step, 1],
                    padding=&#39;SAME&#39;,
                    name=name)

# 全连接层
def fc(input_tensor, out_channels, name):
    in_channels = input_tensor.get_shape()[-1].value
    with tf.name_scope(name) as scope:
        weights = tf.get_variable(
                        shape=[in_channels, out_channels],
                        dtype=tf.float32,
                        initializer=xavier_initializer_conv2d(),
                        name=scope+&#39;weights&#39;)

        biases = tf.Variable(
                        tf.constant(value=0., shape=[out_channels], dtype=tf.float32),
                        name=&#39;biases&#39;)
        relu = tf.nn.relu_layer(input_tensor, weights, biases, name=scope)
        return relu


# 定义 VGG-16 网络
def vgg_16(images, keep_prob):
    # 第一个块结构，包括两个con3-64
    with tf.name_scope(&#39;block_1&#39;):
        conv1_1 = conv(images, fileter_size=3, out_channels=64, step=1, name=&#39;conv1_1&#39;)
        conv1_2 = conv(conv1_1, fileter_size=3, out_channels=64, step=1, name=&#39;conv1_2&#39;)
        pool1 = max_pool(conv1_2, fileter_size=2, step=2, name=&#39;pooling_1&#39;)

    # 第二个块结构，包括两个con3-128
    with tf.name_scope(&#39;block_2&#39;):
        conv2_1 = conv(pool1, fileter_size=3, out_channels=128, step=1, name=&#39;conv2_1&#39;)
        conv2_2 = conv(conv2_1, fileter_size=3, out_channels=128, step=1, name=&#39;conv2_2&#39;)
        pool2 = max_pool(conv2_2, fileter_size=2, step=2, name=&#39;pooling_2&#39;)

    # 第三个块结构，包括两个con3-256
    with tf.name_scope(&#39;block_3&#39;):
        conv3_1 = conv(pool2, fileter_size=3, out_channels=256, step=1, name=&#39;conv3_1&#39;)
        conv3_2 = conv(conv3_1, fileter_size=3, out_channels=256, step=1, name=&#39;conv3_2&#39;)
        conv3_3 = conv(conv3_2, fileter_size=3, out_channels=256, step=1, name=&#39;conv3_3&#39;)
        pool3 = max_pool(conv3_3, fileter_size=2, step=2, name=&#39;pooling_3&#39;)

    # 第四个块结构，包括两个con3-512
    with tf.name_scope(&#39;block_4&#39;):
        conv4_1 = conv(pool3, fileter_size=3, out_channels=512, step=1, name=&#39;conv4_1&#39;)
        conv4_2 = conv(conv4_1, fileter_size=3, out_channels=512, step=1, name=&#39;conv4_2&#39;)
        conv4_3 = conv(conv4_2, fileter_size=3, out_channels=512, step=1, name=&#39;conv4_3&#39;)
        pool4 = max_pool(conv4_3, fileter_size=2, step=2, name=&#39;pooling_4&#39;)

    # 第五个块结构，包括两个con3-512
    with tf.name_scope(&#39;block_3&#39;):
        conv5_1 = conv(pool4, fileter_size=3, out_channels=512, step=1, name=&#39;conv5_1&#39;)
        conv5_2 = conv(conv5_1, fileter_size=3, out_channels=512, step=1, name=&#39;conv5_2&#39;)
        conv5_3 = conv(conv5_2, fileter_size=3, out_channels=512, step=1, name=&#39;conv5_3&#39;)
        pool5 = max_pool(conv5_3, fileter_size=2, step=2, name=&#39;pooling_5&#39;)


    # flatten
    flattened = flatten(pool5)
    dim = flattened.shape[1].value

    with tf.name_scope(&#39;fc1_4096&#39;):
        fc1 =fc(flattened, out_channels=4096, name=&#39;fc1_4096&#39;)
        fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob, name=&#39;fc1_drop&#39;)

    with tf.name_scope(&#39;fc2_4096&#39;):
        fc2 =fc(fc1_drop, out_channels=4096, name=&#39;fc2_4096&#39;)
        fc2_drop = tf.nn.dropout(fc2, keep_prob=keep_prob, name=&#39;fc2_drop&#39;)

    with tf.name_scope(&#39;fc_1000&#39;):
        fc3 =fc(fc2_drop, out_channels=1000, name=&#39;fc_1000&#39;)
        logits = tf.nn.softmax(fc3)
        return logits
</code></pre>
<h3 id="7-2-Google-官方-slim-实现"><a href="#7-2-Google-官方-slim-实现" class="headerlink" title="7.2 Google 官方 slim 实现"></a>7.2 Google 官方 slim 实现</h3><pre><code class="python">import tensorflow as tf
slim = tf.contrib.slim

def vgg_arg_scope(weight_decay=0.0005):
    with slim.arg_scope([slim.conv2d, slim.fully_connected],
                      activation_fn=tf.nn.relu,
                      weights_regularizer=slim.l2_regularizer(weight_decay),
                      biases_initializer=tf.zeros_initializer()):
    with slim.arg_scope([slim.conv2d], padding=&#39;SAME&#39;) as arg_sc:
        return arg_sc


def vgg_a(inputs,
          num_classes=1000,
          is_training=True,
          dropout_keep_prob=0.5,
          spatial_squeeze=True,
          scope=&#39;vgg_a&#39;,
          fc_conv_padding=&#39;VALID&#39;,
          global_pool=False):


    with tf.variable_scope(scope, &#39;vgg_a&#39;, [inputs]) as sc:
        end_points_collection = sc.original_name_scope + &#39;_end_points&#39;
        # Collect outputs for conv2d, fully_connected and max_pool2d.
        with slim.arg_scope([slim.conv2d, slim.max_pool2d],
                        outputs_collections=end_points_collection):
              net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=&#39;conv1&#39;)
              net = slim.max_pool2d(net, [2, 2], scope=&#39;pool1&#39;)
             net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=&#39;conv2&#39;)
              net = slim.max_pool2d(net, [2, 2], scope=&#39;pool2&#39;)
              net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=&#39;conv3&#39;)
              net = slim.max_pool2d(net, [2, 2], scope=&#39;pool3&#39;)
              net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=&#39;conv4&#39;)
              net = slim.max_pool2d(net, [2, 2], scope=&#39;pool4&#39;)
              net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=&#39;conv5&#39;)
              net = slim.max_pool2d(net, [2, 2], scope=&#39;pool5&#39;)

              # Use conv2d instead of fully_connected layers.
              net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=&#39;fc6&#39;)
              net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                         scope=&#39;dropout6&#39;)
              net = slim.conv2d(net, 4096, [1, 1], scope=&#39;fc7&#39;)
              # Convert end_points_collection into a end_point dict.
              end_points = slim.utils.convert_collection_to_dict(end_points_collection)
              if global_pool:
                net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=&#39;global_pool&#39;)
                end_points[&#39;global_pool&#39;] = net
              if num_classes:
                net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                                   scope=&#39;dropout7&#39;)
                net = slim.conv2d(net, num_classes, [1, 1],
                                  activation_fn=None,
                                  normalizer_fn=None,
                                  scope=&#39;fc8&#39;)
            if spatial_squeeze:
                  net = tf.squeeze(net, [1, 2], name=&#39;fc8/squeezed&#39;)
            end_points[sc.name + &#39;/fc8&#39;] = net

        return net, end_points
vgg_a.default_image_size = 224


def vgg_16(inputs,
           num_classes=1000,
           is_training=True,
           dropout_keep_prob=0.5,
           spatial_squeeze=True,
           scope=&#39;vgg_16&#39;,
           fc_conv_padding=&#39;VALID&#39;,
           global_pool=False):

with tf.variable_scope(scope, &#39;vgg_16&#39;, [inputs]) as sc:
    end_points_collection = sc.original_name_scope + &#39;_end_points&#39;
    # Collect outputs for conv2d, fully_connected and max_pool2d.
    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
                        outputs_collections=end_points_collection):
        net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=&#39;conv1&#39;)
          net = slim.max_pool2d(net, [2, 2], scope=&#39;pool1&#39;)
          net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=&#39;conv2&#39;)
          net = slim.max_pool2d(net, [2, 2], scope=&#39;pool2&#39;)
          net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=&#39;conv3&#39;)
          net = slim.max_pool2d(net, [2, 2], scope=&#39;pool3&#39;)
          net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=&#39;conv4&#39;)
          net = slim.max_pool2d(net, [2, 2], scope=&#39;pool4&#39;)
          net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=&#39;conv5&#39;)
          net = slim.max_pool2d(net, [2, 2], scope=&#39;pool5&#39;)

          # Use conv2d instead of fully_connected layers.
          net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=&#39;fc6&#39;)
          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                         scope=&#39;dropout6&#39;)
          net = slim.conv2d(net, 4096, [1, 1], scope=&#39;fc7&#39;)
          # Convert end_points_collection into a end_point dict.
          end_points = slim.utils.convert_collection_to_dict(end_points_collection)
          if global_pool:
            net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=&#39;global_pool&#39;)
            end_points[&#39;global_pool&#39;] = net
          if num_classes:
            net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                           scope=&#39;dropout7&#39;)
            net = slim.conv2d(net, num_classes, [1, 1],
                          activation_fn=None,
                          normalizer_fn=None,
                          scope=&#39;fc8&#39;)
        if spatial_squeeze:
            net = tf.squeeze(net, [1, 2], name=&#39;fc8/squeezed&#39;)
        end_points[sc.name + &#39;/fc8&#39;] = net
         return net, end_points

vgg_16.default_image_size = 224


def vgg_19(inputs,
           num_classes=1000,
           is_training=True,
           dropout_keep_prob=0.5,
           spatial_squeeze=True,
           scope=&#39;vgg_19&#39;,
           fc_conv_padding=&#39;VALID&#39;,
           global_pool=False):

    with tf.variable_scope(scope, &#39;vgg_19&#39;, [inputs]) as sc:
        end_points_collection = sc.original_name_scope + &#39;_end_points&#39;
        # Collect outputs for conv2d, fully_connected and max_pool2d.
        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
                        outputs_collections=end_points_collection):
          net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=&#39;conv1&#39;)
          net = slim.max_pool2d(net, [2, 2], scope=&#39;pool1&#39;)
          net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=&#39;conv2&#39;)
          net = slim.max_pool2d(net, [2, 2], scope=&#39;pool2&#39;)
          net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=&#39;conv3&#39;)
          net = slim.max_pool2d(net, [2, 2], scope=&#39;pool3&#39;)
          net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=&#39;conv4&#39;)
          net = slim.max_pool2d(net, [2, 2], scope=&#39;pool4&#39;)
          net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=&#39;conv5&#39;)
          net = slim.max_pool2d(net, [2, 2], scope=&#39;pool5&#39;)

          # Use conv2d instead of fully_connected layers.
          net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=&#39;fc6&#39;)
          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                         scope=&#39;dropout6&#39;)
          net = slim.conv2d(net, 4096, [1, 1], scope=&#39;fc7&#39;)
          # Convert end_points_collection into a end_point dict.
            end_points = slim.utils.convert_collection_to_dict(end_points_collection)
          if global_pool:
              net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=&#39;global_pool&#39;)
            end_points[&#39;global_pool&#39;] = net
          if num_classes:
            net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                           scope=&#39;dropout7&#39;)
            net = slim.conv2d(net, num_classes, [1, 1],
                          activation_fn=None,
                          normalizer_fn=None,
                          scope=&#39;fc8&#39;)
        if spatial_squeeze:
            net = tf.squeeze(net, [1, 2], name=&#39;fc8/squeezed&#39;)
            end_points[sc.name + &#39;/fc8&#39;] = net
          return net, end_points

vgg_19.default_image_size = 224

# Alias
vgg_d = vgg_16
vgg_e = vgg_19
</code></pre>
<h1 id="8-模型参数计算"><a href="#8-模型参数计算" class="headerlink" title="8. 模型参数计算"></a>8. 模型参数计算</h1><p><img src="https://upload-images.jianshu.io/upload_images/1351548-7d22c2f94bbd923b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="Inception-v1（GoogLeNet）"><a href="#Inception-v1（GoogLeNet）" class="headerlink" title="Inception v1（GoogLeNet）"></a>Inception v1（GoogLeNet）</h1><p>源自论文 《<em>Going Deeper with Convolutions</em>》</p>
<blockquote>
<p>2014 年，GoogLeNet 和 VGG 是当年 ImageNet 挑战赛（ILSVRC14）的双雄，GoogLeNet 获得了第一名、VGG 获得了第二名，这两类模型结构的共同特点是层次更深了。</p>
<p>VGG 继承了 LeNet 以及 AlexNet 的一些框架结构，而 GoogLeNet 则做了更加大胆的网络结构尝试，虽然深度只有 <code>22</code> 层，但大小却比 AlexNet 和 VGG 小很多，GoogleNet 参数为 <code>500</code> 万个，AlexNet 参数个数是 GoogleNet 的 <code>12</code> 倍，VGG 参数又是 AlexNet 的 <code>3</code> 倍，因此在内存或计算资源有限时，GoogleNet 是比较好的选择；从模型结果来看，GoogLeNet 的性能却更加优越。不过 VGG 相对延续传统的结构也给它带来了优势，便于理解，泛化到其他类型的任务时效果也非常的好。</p>
</blockquote>
<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><ul>
<li><p>GoogLeNet 的参数量相比 AlexNet 要少很多，是 AlexNet 参数的 <code>1/12</code>（<code>12x</code> fewer parameters than AlexNet）</p>
</li>
<li><p>由于手机终端和嵌入式设备的发展，深度学习算法的运行效率和内存控制显得尤其重要，深度神经网络不仅具有学术性，更应该能够应用于现实生活中</p>
</li>
<li><p>论文起源于 NIN（Network in Network），并且在网络深度上进行了研究</p>
</li>
</ul>
<h2 id="2-研究动机"><a href="#2-研究动机" class="headerlink" title="2. 研究动机"></a>2. 研究动机</h2><p>针对之前的网络结构设计，主要存在几个方面的缺点：</p>
<ul>
<li><p>参数太多，模型的熵容量太大，容易导致过拟合，尤其对于比较小的数据集</p>
</li>
<li><p>网络越大，计算复杂度也就越大，在实际应用中，带来了困难</p>
</li>
<li><p>网络越深，模型训练越困难，梯度消失问题不可避免</p>
</li>
</ul>
<p>GoogLeNet 的研究动机从两个方面进行考虑：</p>
<ul>
<li><p>深度： 网络采用了一个 <code>22</code> 层的结构，<code>9</code> 个 Inception（<code>2-5-2</code> 结构），但是为了避免梯度消失问题，GoogLeNet 在不同层增加了 Loss 的输出</p>
</li>
<li><p>宽度： 网络在每一个小的结构中，利用了 <code>1x1</code>，<code>3x3</code>，<code>5x5</code> 的卷积核和直接 Max Pooling 操作（为了保证输出的特征层的大小与卷积核输出大小相同，步长设置为 <code>1</code>），为了避免由于宽度的增加带来每一个卷机核或者 Max Pooling 输出的特征层 cancate 之后，特征层的深度过深，也为了减少网络参数，在每一个 <code>3x3</code>，<code>5x5</code> 的卷积核前面和 <code>3x3</code> Max Pooling 后面都利用了一个 <code>1x1</code> 的卷积核进行降维，如下图：</p>
</li>
</ul>
<p>  <img src="https://upload-images.jianshu.io/upload_images/1351548-edf7f03393e343f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3-网络结构"><a href="#3-网络结构" class="headerlink" title="3. 网络结构"></a>3. 网络结构</h2><p>网络结构中的具体参数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-4e090ef2d1f803e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>说明：</p>
<ul>
<li>前面 <code>4</code> 层，为普通的卷积和 max pooling 操作。在网络的开始利用了 <code>7x7</code>，<code>s=2</code> 的 filter，其目的是为了降低图像的特征层大小，减少后面的计算量</li>
<li>结构中的 Max Pooling 采用的类似于 AlexNet 中的 Overlapping Pooling （<code>3x3</code>，<code>s=2</code>）</li>
<li>网络中，不断重复上述的 Inception Module 模块，来提高网络的深度：<ul>
<li><code>#1x1</code>：表示网络结构中的最左边的 <code>1x1</code> 卷积核个数</li>
<li><code>#3x3</code> reduce： 表示网络结构进行 <code>3x3</code> 卷积操作之前 <code>1x1</code> 卷积核的个数</li>
<li><code>#3x3</code> ： 表示网络结构中的 <code>3x3</code> 卷积核个数</li>
<li><code>#5x5</code> reduce： 表示网络结构进行 <code>5x5</code> 卷积操作之前 <code>1x1</code> 卷积核的个数</li>
<li><code>#5x5</code>： 表示网络结构中的 <code>5x5</code>  卷积核个数</li>
<li><code>pool proj</code>：表示进行 <code>3x3</code> max pooling 操作之后， <code>1x1</code> 卷积核的个数</li>
</ul>
</li>
<li>所有的网络中的操作，后面都接一个非线性映射变换 ReLU</li>
<li>网络的最后，采用了类似 NIN 网络结构中的 global pooling 操作，对 <code>7x7x1024</code> 的特征层进行 avg pooling 操作，变成了一个 <code>1x1x1024</code> 的向量</li>
<li>采用 dropout 操作，避免过拟合</li>
</ul>
<p>网络的深度增加，给训练带来了很大的困难（梯度消失问题），为了更好的训练网络，GoogLeNet 在 Inception（4a）和 Inception（4d）的输出层，增了 Loss 计算误差，然后反向传播，在整个训练过程中，不断减少这个两个 Loss 的权重。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-ee8ab491ca8d0b2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="4-核心思想"><a href="#4-核心思想" class="headerlink" title="4. 核心思想"></a>4. 核心思想</h2><p>Inception 模块的基本机构如下图，整个 Inception 结构就是由多个这样的 Inception 模块串联起来的。Inception 结构的主要贡献有两个：</p>
<ul>
<li>使用 <code>1x1</code> 的卷积来进行升降维</li>
<li>在多个尺寸上同时进行卷积再聚合</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-955791e1db3d352f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h3 id="4-1-1x1-的卷积"><a href="#4-1-1x1-的卷积" class="headerlink" title="4.1 1x1 的卷积"></a>4.1 <code>1x1</code> 的卷积</h3><p>可以看到上图中有多个黄色的 <code>1x1</code> 卷积模块，这样的卷积有什么用处呢？</p>
<p><strong>作用 1</strong>：在相同尺寸的感受野中叠加更多的卷积，能提取到更丰富的特征。这个观点来自于 《<em>Network in Network</em>》（NIN），上图里三个 <code>1x1</code> 卷积都起到了该作用。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-64072009a0adcdf9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图左侧是是传统的卷积层结构（线性卷积），在一个尺度上只有一次卷积；右图是 Network in Network 结构（NIN结构），先进行一次普通的卷积（比如 <code>3x3</code>），紧跟再进行一次 <code>1x1</code> 的卷积，对于某个像素点来说，<code>1x1</code> 卷积等效于该像素点在所有特征上进行一次全连接的计算，所以右侧图的 <code>1x1</code> 卷积画成了全连接层的形式【需要注意的是：NIN 结构中，无论是第一个 <code>3x3</code> 卷积还是新增的 <code>1x1</code> 卷积，后面都紧跟着激活函数（比如 ReLU）】。</p>
<p>将两个卷积串联，就能组合出更多的非线性特征。举个例子，假设第 1 个 <code>3x3</code> 卷积＋激活函数近似于：<img src="https://upload-images.jianshu.io/upload_images/1351548-da9459752c2d8aeb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">，第二个 <code>1x1</code> 卷积＋激活函数近似于：<img src="https://upload-images.jianshu.io/upload_images/1351548-b3c2f03647fdd119.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">，那<img src="https://upload-images.jianshu.io/upload_images/1351548-7e46c1b9c1533cf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">和<img src="https://upload-images.jianshu.io/upload_images/1351548-b296b5827b65fcfc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">比，哪个非线性更强，更能模拟非线性的特征？答案是显而易见的。NIN 的结构和传统的神经网络中多层的结构有些类似，后者的多层是跨越了不同尺寸的感受野（通过层与层中间加 pooling 层），从而在更高尺度上提取出特征；NIN 结构是在同一个尺度上的多层（中间没有pool层），从而在相同的感受野范围能提取更强的非线性。</p>
<p><strong>作用 2</strong>：使用 <code>1x1</code> 卷积进行降维，降低了计算复杂度。上图中间 <code>3x3</code> 卷积和 <code>5x5</code> 卷积前的 <code>1x1</code> 卷积都起到了这个作用。当某个卷积层输入的特征数较多，对这个输入进行卷积运算将产生巨大的计算量；如果对输入先进行降维，减少特征数后再做卷积计算量就会显著减少。</p>
<p> 下图是优化前后两种方案的乘法次数比较，同样是输入一组有 <code>192</code> 个特征、<code>32x32</code> 大小，输出 <code>256</code> 组特征的数据。第一张图直接用 <code>3x3</code> 卷积实现，需要 <code>192x256x3x3x32x32=452984832</code> 次乘法；第二张图先用 <code>1x1</code> 的卷积降到 <code>96</code> 个特征（则卷积核为：<code>1x1x96</code>），再用 <code>3x3</code> 卷积恢复出256组特征，需要 <code>192x96x1x1x32x32+96x256x3x3x32x32=245366784</code> 次乘法，使用 <code>1x1</code> 卷积降维的方法节省了一半的计算量（注意：因为卷积核深度为 <code>96</code>，而输入层的通道数为 <code>192</code>，由于卷积核中的通道数量必须和输入层中的通道数量保持一致，所以最终的通道数为<code>96</code>，便实现了通道数量的压缩）。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-f306e68b75b4f6c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有人会问，用 <code>1x1</code> 卷积降到 <code>96</code> 个特征后特征数不就减少了么，会影响最后训练的效果么？答案是否定的，只要最后输出的特征数不变（<code>256</code> 组），中间的降维类似于压缩的效果，并不影响最终训练的结果。</p>
<h3 id="4-2-多个尺寸上进行卷积再聚合"><a href="#4-2-多个尺寸上进行卷积再聚合" class="headerlink" title="4.2  多个尺寸上进行卷积再聚合"></a>4.2  多个尺寸上进行卷积再聚合</h3><p><img src="https://upload-images.jianshu.io/upload_images/1351548-64072009a0adcdf9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>从上图可以看到对输入做了 <code>4</code> 个分支，分别用不同尺寸的 filter 进行卷积或池化，最后再在特征维度上拼接到一起。这种全新的结构有什么好处呢？Szegedy 从多个角度进行了解释：</p>
<ul>
<li><p>解释 1：从直观感觉上在多个尺度上同时进行卷积，能提取到不同尺度的特征。特征更为丰富也意味着最后分类判断时更加准确。</p>
</li>
<li><p>解释 2：利用稀疏矩阵分解成密集矩阵计算的原理来加快收敛速度。举个例子下图左侧是个稀疏矩阵（很多元素都为 <code>0</code>，不均匀分布在矩阵中），和一个 <code>2x2</code> 的矩阵进行卷积，需要对稀疏矩阵中的每一个元素进行计算；如果像右图那样把稀疏矩阵分解成 <code>2</code> 个子密集矩阵，再和 <code>2x2</code> 矩阵进行卷积，稀疏矩阵中 <code>0</code> 较多的区域就可以不用计算，计算量就大大降低。这个原理应用到 Inception 上就是要在特征维度上进行分解！传统的卷积层的输入数据只和一种尺度（比如 <code>3x3</code>）的卷积核进行卷积，输出固定维度（比如 <code>256</code> 个特征）的数据，所有 <code>256</code> 个输出特征基本上是均匀分布在 <code>3x3</code> 尺度范围上，这可以理解成输出了一个稀疏分布的特征集；而 Inception 模块在多个尺度上提取特征（比如 <code>1x1</code>，<code>3x3</code>，<code>5x5</code>），输出的 <code>256</code> 个特征就不再是均匀分布，而是相关性强的特征聚集在一起（比如 <code>1x1</code> 的的 <code>96</code> 个特征聚集在一起，<code>3x3</code> 的 <code>96</code> 个特征聚集在一起，<code>5x5</code> 的 <code>64</code> 个特征聚集在一起），这可以理解成多个密集分布的子特征集。这样的特征集中因为相关性较强的特征聚集在了一起，不相关的非关键特征就被弱化，同样是输出 <code>256</code> 个特征，Inception 方法输出的特征“冗余”的信息较少。用这样的“纯”的特征集层层传递最后作为反向计算的输入，自然收敛的速度更快。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-db078bfc072b6407.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
</li>
<li><p>解释 3：Hebbin 赫布原理。Hebbian 原理是神经科学上的一个理论，解释了在学习的过程中脑中的神经元所发生的变化，用一句话概括就是：fire together, wire together。赫布认为“两个神经元或者神经元系统，如果总是同时兴奋，就会形成一种‘组合’，其中一个神经元的兴奋会促进另一个的兴奋”。比如狗看到肉会流口水，反复刺激后，脑中识别肉的神经元会和掌管唾液分泌的神经元会相互促进，“缠绕”在一起，以后再看到肉就会更快流出口水。用在 Inception  结构中就是要把相关性强的特征汇聚到一起。这有点类似上面的解释 2，把 <code>1x1</code>，<code>3x3</code>，<code>5x5</code> 的特征分开。因为训练收敛的最终目的就是要提取出独立的特征，所以预先把相关性强的特征汇聚，就能起到加速收敛的作用。在 Inception 模块中有一个分支使用了 max pooling，作者认为 pooling 也能起到提取特征的作用，所以也加入模块中。注意这个  pooling的 <code>stride=1</code>，pooling 后没有减少数据的尺寸。</p>
</li>
</ul>
<h2 id="5-论文关键点解析"><a href="#5-论文关键点解析" class="headerlink" title="5. 论文关键点解析"></a>5. 论文关键点解析</h2><blockquote>
<p>Page 3：The fundamental way of solving both issues would be by ultimately moving from fully connected to sparsely connected architectures, even inside the convolutions. Besides mimicking biological systems, this would also have the advantage of firmer theoretical underpinnings due to the ground-breaking work of Arora et al. [2]. Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.</p>
</blockquote>
<p>作者提出需要将全连接的结构转化成稀疏连接的结构。稀疏连接有两种方法：一种是空间（spatial）上的稀疏连接，也就是传统的 CNN 卷积结构：只对输入图像的某一部分 patch 进行卷积，而不是对整个图像进行卷积，共享参数降低了总参数的数目减少了计算量；另一种方法是在特征（feature）维度进行稀疏连接，就是前一节提到的在多个尺寸上进行卷积再聚合，把相关性强的特征聚集到一起，每一种尺寸的卷积只输出 <code>256</code> 个特征中的一部分，这也是种稀疏连接。作者提到这种方法的理论基础来自于论文 《<em>Provable bounds for learning some deep representations</em>》。</p>
<blockquote>
<p>Page 3：On the downside, todays computing infrastructures are very inefficient when it comes to numerical calculation on non-uniform sparse data structures. Even if the number of arithmetic operations is reduced by100×, the overhead of lookups and cache misses is so dominant that switching to sparse matrices would not pay off. The gap is widened even further by the use of steadily improving, highly tuned, numerical libraries that allow for extremely fast dense matrix multiplication, exploit-ing the minute details of the underlying CPU or GPU hardware [16,9]. Also, non-uniform sparse models require more sophisticated engineering and computing infrastructure. Most current vision oriented machine learning systems utilize sparsity in the spatial domain just by the virtue of employing convolutions. However, convolutions are implemented as collections of dense connections to the patches in the earlier layer. ConvNets have traditionally used random and sparse connection tables in the feature dimensions since [11] in order to break the symmetry and improve learning, the trend changed back to full connections with [9] in order to better optimize parallel computing. The uniformity of the structure and a large number of filters and greater batch size allow for utilizing efficient dense computation.</p>
</blockquote>
<p>作者提到如今的计算机对稀疏数据进行计算的效率是很低的，即使使用稀疏矩阵算法也得不偿失（之前将稀疏矩阵分解成子密集矩阵来进行计算的方法，注意图中左侧的那种稀疏矩阵在计算机内部都是使用元素值＋行列值的形式来存储，只存储非 <code>0</code> 元素）。使用稀疏矩阵算法来进行计算虽然计算量会大大减少，但会增加中间缓存（具体原因请研究稀疏矩阵的计算方法）。</p>
<p> 当今最常见的利用数据稀疏性的方法是通过卷积对局部 patch 进行计算（CNN 方法，就是前面提到的在 spatial 上利用稀疏性）；另一种利用数据稀疏性的方法是在特征维度进行利用，比如ConvNets 结构，它使用特征连接表来决定哪些卷积的输出才累加到一起（普通结构使用一个卷积核对所有输入特征做卷积，再将所有结果累加到一起，输出一个特征； 而 ConvNets 是选择性的对某些卷积结果做累加）。ConvNets 利用稀疏性的方法现在已经很少用了，因为只有在特征维度上进行全连接才能更高效的利用 GPU 的并行计算的能力，否则你就得为这样的特征连接表单独设计 CUDA 的接口函数，单独设计的函数往往无法最大限度的发挥 GPU 并行计算的能力。</p>
<blockquote>
<p>Page 3：This raises the question whether there is any hope for a next, intermediate step: an architecture that makes use of the extra sparsity, even at filter level, as suggested by the theory, but exploits our current hardware by utilizing computations on dense matrices. The vast literature on sparse matrix computations (e.g. [3]) suggests that clustering sparse matrices into relatively dense submatrices tends to give state of the art practical performance for sparse matrix multiplication.</p>
</blockquote>
<p>前面提到 ConvNets 这样利用稀疏性的方法现在已经很少用了，那还有什么方法能在特征维度上利用稀疏性么？这就引申出了这篇论文的重点：将相关性强的特征汇聚到一起，也就是之前提到的在多个尺度上卷积再聚合。</p>
<blockquote>
<p>page 4：In order to avoid patch-alignment issues, current incarnations of the Inception architecture are restricted to filter sizes 1×1,3×3 and 5×5, however this decision was based more on convenience rather than necessity. It also means that the suggested architecture is a combination of all those layers with their output filter<br>banks concatenated into a single output vector forming the input of the next stage. Additionally, since pooling operations have been essential for the success in current state of the art convolutional networks, it suggests that adding an alternative parallel pooling path in each such stage should have additional beneficial effect, too (see Figure 2(a))</p>
</blockquote>
<p>论文这几句解释了最基本的“ Inception module”的构成：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-de49a19259a4df63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>所以在一层里把不同大小的卷积核叠在一起后，意味着一层里可以产生不同大小的卷积核处理之后的效果，也意味着不用人为的来选择这一层要怎么卷，这个网络自己便会学习用什么样的卷积（或池化）操作最好。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-43047387c6059823.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>但是作者发现由于通道数量太多了，这样计算成本会变得非常高（第三段），所以 <code>1×1</code> 卷积再次登场，用来压缩通道数量，缩减了计算成本（第四段/下边这段）。</p>
<blockquote>
<p>This leads to the second idea of the proposed architecture: judiciously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise.This is based on the success of embeddings: even low dimensional embeddings might contain a lot of information about a relatively large image patch. However, embeddings represent information in a dense, compressed form and compressed information is harder to model. We would like to keep our representation sparse at most places (as required by the conditions of [2]) and compress the signals only whenever they have to be aggregated en masse. That is, 1×1 convolutions are used to compute reductions before the expensive 3×3 and 5×5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose. The final result is depicted in Figure 2(b).</p>
</blockquote>
<p>即先用 <code>1×1</code> 卷积压缩通道数量，再让<code>3×3</code>，<code>5×5</code> 来进行滤波：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-c11d93d9247a3e74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>最终“Inception module”变成了下边的样子：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-0ebbaa20a4208ff4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<blockquote>
<p>Page 6：The use of average pooling before the classifier is based on [12], although our implementation differs in that we use an extra linear layer.</p>
</blockquote>
<p>Network in Network（NIN）最早提出了用 Global Average Pooling（GAP）层来代替全连接层的方法，具体方法就是：对每一个 feature 上的所有点做平均，有 <code>n</code> 个 feature 就输出 <code>n</code> 个平均值作为最后的 softmax 的输入。它的好处：</p>
<ol>
<li>对数据在整个 feature 上作正则化，防止了过拟合</li>
<li>不再需要全连接层，减少了整个结构参数的数目（一般全连接层是整个结构中参数最多的层），过拟合的可能性降低</li>
<li>不用再关注输入图像的尺寸，因为不管是怎样的输入都是一样的平均方法，传统的全连接层要根据尺寸来选择参数数目，不具有通用性。</li>
</ol>
<blockquote>
<p>Page 6：By adding auxiliary classifiers connected to these intermediate layers, we would expect to encourage discrimination in the lower stages in the classifier, increase the gradient signal that gets propagated back, and provide additional regularization.</p>
</blockquote>
<p>Inception 结构在在某些层级上加了分支分类器，输出的 loss 乘以个系数再加到总的 loss 上，作者认为可以防止梯度消失问题（事实上在较低的层级上这样处理基本没作用，作者在后来的 inception v3 论文中做了澄清）。</p>
<h2 id="6-论文的亮点"><a href="#6-论文的亮点" class="headerlink" title="6. 论文的亮点"></a>6. 论文的亮点</h2><p><img src="https://upload-images.jianshu.io/upload_images/1351548-8e04b2fe993808b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><code>1x1</code> 卷积的好处：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-5bd6536f4a0f4386.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><code>1x1</code> 卷积主要在做这些事：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-56d39479c24ce281.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-c4f158bc9ae93bca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如果用 <code>1x1</code> 卷积去处理 <code>6x6x32</code> 的图，相当于给这 <code>32</code> 个通道的同一个位置分配相同的权重。如果用 <code>1x1x32</code> 的卷积去处理 <code>6x6x32</code> 的图，则可以给 <code>32</code> 个通道分配不同的权重。</p>
<p><code>1x1</code> 卷积还可以压缩通道数量：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-0fb2b5c9ad452b89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><code>1x1</code> 卷积相对于其他的滤波器组参数几何倍数减少，毕竟 <code>1x1</code>卷积只有一个参数，<code>3x3</code> 有 <code>9</code> 个参数，<code>5x5</code> 有 <code>25</code>个……..</p>
<h2 id="7-GoogLeNet-slim-实现"><a href="#7-GoogLeNet-slim-实现" class="headerlink" title="7. GoogLeNet slim 实现"></a>7. GoogLeNet slim 实现</h2><p><a href="https://dgschwend.github.io/netscope/#/preset/googlenet" target="_blank" rel="noopener">GoogLeNet 网络详细结构</a></p>
<pre><code class="python">import tensorflow as tf
slim = tf.contrib.slim
trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)

def inception_arg_scope(weight_decay=0.00004,
                        use_batch_norm=True,
                        batch_norm_decay=0.9997,
                        batch_norm_epsilon=0.001,
                        activation_fn=tf.nn.relu,
                        batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,
                        batch_norm_scale=False):
  &quot;&quot;&quot;
  Args:
    weight_decay: The weight decay to use for regularizing the model.
    use_batch_norm: &quot;If `True`, batch_norm is applied after each convolution.
    batch_norm_decay: Decay for batch norm moving average.
    batch_norm_epsilon: Small float added to variance to avoid dividing by zero
      in batch norm.
    activation_fn: Activation function for conv2d.
    batch_norm_updates_collections: Collection for the update ops for
      batch norm.
    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the
      activations in the batch normalization layer.
  Returns:
    An `arg_scope` to use for the inception models.
  &quot;&quot;&quot;
    batch_norm_params = {
          # Decay for the moving averages.
        &#39;decay&#39;: batch_norm_decay,
          # epsilon to prevent 0s in variance.
          &#39;epsilon&#39;: batch_norm_epsilon,
          # collection containing update_ops.
          &#39;updates_collections&#39;: batch_norm_updates_collections,
          # use fused batch norm if possible.
          &#39;fused&#39;: None,
          &#39;scale&#39;: batch_norm_scale,
      }
      if use_batch_norm:
        normalizer_fn = slim.batch_norm
        normalizer_params = batch_norm_params
      else:
        normalizer_fn = None
        normalizer_params = {}
      # Set weight_decay for weights in Conv and FC layers.
      with slim.arg_scope([slim.conv2d, slim.fully_connected],
                      weights_regularizer=slim.l2_regularizer(weight_decay)):
        with slim.arg_scope(
            [slim.conv2d],
            weights_initializer=slim.variance_scaling_initializer(),
            activation_fn=activation_fn,
            normalizer_fn=normalizer_fn,
            normalizer_params=normalizer_params) as sc:

              return sc


def inception_v1_base(inputs,
                      final_endpoint=&#39;Mixed_5c&#39;,
                      scope=&#39;InceptionV1&#39;):
  &quot;&quot;&quot;
  Args:
     final_endpoint: specifies the endpoint to construct the network up to. It
      can be one of [&#39;Conv2d_1a_7x7&#39;, &#39;MaxPool_2a_3x3&#39;, &#39;Conv2d_2b_1x1&#39;,
      &#39;Conv2d_2c_3x3&#39;, &#39;MaxPool_3a_3x3&#39;, &#39;Mixed_3b&#39;, &#39;Mixed_3c&#39;,
      &#39;MaxPool_4a_3x3&#39;, &#39;Mixed_4b&#39;, &#39;Mixed_4c&#39;, &#39;Mixed_4d&#39;, &#39;Mixed_4e&#39;,
      &#39;Mixed_4f&#39;, &#39;MaxPool_5a_2x2&#39;, &#39;Mixed_5b&#39;, &#39;Mixed_5c&#39;]

  Returns:
    A dictionary from components of the network to the corresponding activation.
  &quot;&quot;&quot;
    end_points = {}
      with tf.variable_scope(scope, &#39;Inception_v1&#39;, [inputs]):
          with slim.arg_scope([slim.conv2d, slim.fully_connected],
                          weights_initializer=trusnc_normal(0.01)):
              with slim.arg_scope([slim.conv2d, slim.max_pool2d],
                              stride=1, padding=&#39;SAME&#39;):
                  end_point = &#39;Conv2d_1a_7x7&#39;
                  net = slim.conv2d(inputs, 64, [7,7], stride=2, scope=end_point)
                  end_points[end_point] = net
                  if final_endpoint == end_point: return net, end_points

                  end_point = &#39;MaxPool_2a_3x3&#39;
                  net = slim.max_pool2d(net, [3,3], stride=2, scope=end_point)
                  end_points[end_point] = net
                  if final_endpoint == end_point: return net, end_points

                  end_point = &#39;Conv2d_2b_1x1&#39;
                  net = slim.conv2d(net, 64, [1,1], scope=end_point)
                  end_points[end_point] = net
                  if final_endpoint == end_point: return net, end_points

                  end_point = &#39;Conv2d_2c_3x3&#39;
                  net = slim.conv2d(net, 192, [3,3], scope=end_point)
                  end_points[end_point] = net
                  if final_endpoint == end_point: return net, end_points

                  end_point = &#39;MaxPool_3a_3x3&#39;
                  net = slim.max_pool2d(net, [3,3], stride=2, scope=end_point)
                  end_points[end_point] = net
                  if final_endpoint == end_point: return net, end_points

                  end_point = &#39;Mixed_3b&#39;
                  with tf.variable_scope(end_point):
                      with tf.variable_scope(&#39;Branch_0&#39;):
                          branch_0 = slim.conv2d(net, 64, [1,1], scope=&#39;Conv2d_0a_1x1&#39;)
                      with tf.variable_scope(&#39;Branch_1&#39;):
                          branch_1_1 = slim.conv2d(net, 96, [1,1], scope=&#39;Conv2d_0a_1x1&#39;)
                          branch_1_2 = slim.conv2d(branch_1_1, 128, [3,3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_2&#39;):
                          branch_2_1 = slim.conv2d(net, 16, [1,1], scope=&#39;Conv2d_0a_1x1&#39;)
                          branch_2_2 = slim.conv2d(branch_2_1, 32, [3,3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_3&#39;):
                          branch_3_1 = slim.max_pool2d(net, [3,3], scope=&#39;MaxPool_0a_3x3&#39;)
                          branch_3_2 = slim.conv2d(branch_3_1, 32, [1,1], scope=&#39;Conv2d_0b_1x1&#39;)
                      net = tf.concat(axis=3,
                                      values=[branch_0, branch_1_2, branch_2_2, branch_3_2])
                  end_points[end_point] = net
                  if final_endpoint == end_point: return net, end_points

                  end_point = &#39;Mixed_3c&#39;
                  with tf.variable_scope(end_point):
                      with tf.variable_scope(&#39;Branch_0&#39;):
                          branch_0 = slim.conv2d(net, 128, [1,1], scope=&#39;Conv2d_0a_1x1&#39;)
                      with tf.variable_scope(&#39;Branch_1&#39;):
                          branch_1_1 = slim.conv2d(net, 128, [1,1], scope=&#39;Conv2d_0a_1x1&#39;)
                          branch_1_2 = slim.conv2d(branch_1_1, 192, [3,3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_2&#39;):
                        branch_2_1 = slim.conv2d(net, 32, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_2_2 = slim.conv2d(branch_2_1, 96, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_3&#39;):
                        branch_3_1 = slim.max_pool2d(net, [3, 3], scope=&#39;MaxPool_0a_3x3&#39;)
                        branch_3_2 = slim.conv2d(branch_3_1, 64, [1, 1], scope=&#39;Conv2d_0b_1x1&#39;)
                      net = tf.concat(axis=3, 
                                      values=[branch_0, branch_1_2, branch_2_2, branch_3_2])
                end_points[end_point] = net
                if final_endpoint == end_point: return net, end_points

                end_point = &#39;MaxPool_4a_3x3&#39;
                net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)
                end_points[end_point] = net
                if final_endpoint == end_point: return net, end_points

                end_point = &#39;Mixed_4b&#39;
                with tf.variable_scope(end_point):
                      with tf.variable_scope(&#39;Branch_0&#39;):
                        branch_0 = slim.conv2d(net, 192, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                      with tf.variable_scope(&#39;Branch_1&#39;):
                        branch_1_1 = slim.conv2d(net, 96, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_1_2 = slim.conv2d(branch_1_1, 208, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_2&#39;):
                        branch_2_1 = slim.conv2d(net, 16, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_2_2 = slim.conv2d(branch_2_1, 48, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_3&#39;):
                        branch_3_1 = slim.max_pool2d(net, [3, 3], scope=&#39;MaxPool_0a_3x3&#39;)
                        branch_3_2 = slim.conv2d(branch_3_1, 64, [1, 1], scope=&#39;Conv2d_0b_1x1&#39;)
                      net = tf.concat(axis=3, 
                                      values=[branch_0, branch_1_2, branch_2_2, branch_3_2])
                end_points[end_point] = net
                if final_endpoint == end_point: return net, end_points

                end_point = &#39;Mixed_4c&#39;
                    with tf.variable_scope(end_point):
                      with tf.variable_scope(&#39;Branch_0&#39;):
                        branch_0 = slim.conv2d(net, 160, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                      with tf.variable_scope(&#39;Branch_1&#39;):
                        branch_1_1 = slim.conv2d(net, 112, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_1_2 = slim.conv2d(branch_1_1, 224, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_2&#39;):
                        branch_2_1 = slim.conv2d(net, 24, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_2_2 = slim.conv2d(branch_2_1, 64, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_3&#39;):
                        branch_3_1 = slim.max_pool2d(net, [3, 3], scope=&#39;MaxPool_0a_3x3&#39;)
                        branch_3_2 = slim.conv2d(branch_3_1, 64, [1, 1], scope=&#39;Conv2d_0b_1x1&#39;)
                      net = tf.concat(axis=3, 
                                      values=[branch_0, branch_1_2, branch_2_2, branch_3_2])
                end_points[end_point] = net
                if final_endpoint == end_point: return net, end_points

                end_point = &#39;Mixed_4d&#39;
                with tf.variable_scope(end_point):
                      with tf.variable_scope(&#39;Branch_0&#39;):
                        branch_0 = slim.conv2d(net, 128, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                      with tf.variable_scope(&#39;Branch_1&#39;):
                        branch_1_1 = slim.conv2d(net, 128, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_1_2 = slim.conv2d(branch_1_1, 256, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_2&#39;):
                        branch_2_1 = slim.conv2d(net, 24, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_2_2 = slim.conv2d(branch_2_1, 64, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_3&#39;):
                        branch_3_1 = slim.max_pool2d(net, [3, 3], scope=&#39;MaxPool_0a_3x3&#39;)
                        branch_3_2 = slim.conv2d(branch_3_1, 64, [1, 1], scope=&#39;Conv2d_0b_1x1&#39;)
                      net = tf.concat(axis=3, 
                                      values=[branch_0, branch_1_2, branch_2_2, branch_3_2])
                end_points[end_point] = net
                if final_endpoint == end_point: return net, end_points

                end_point = &#39;Mixed_4e&#39;
                with tf.variable_scope(end_point):
                      with tf.variable_scope(&#39;Branch_0&#39;):
                        branch_0 = slim.conv2d(net, 112, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                      with tf.variable_scope(&#39;Branch_1&#39;):
                        branch_1_1 = slim.conv2d(net, 144, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_1_2 = slim.conv2d(branch_1_1, 288, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_2&#39;):
                        branch_2_1 = slim.conv2d(net, 32, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_2_2 = slim.conv2d(branch_2_1, 64, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_3&#39;):
                        branch_3_1 = slim.max_pool2d(net, [3, 3], scope=&#39;MaxPool_0a_3x3&#39;)
                        branch_3_2 = slim.conv2d(branch_3_1, 64, [1, 1], scope=&#39;Conv2d_0b_1x1&#39;)
                      net = tf.concat(axis=3, 
                                      values=[branch_0, branch_1_2, branch_2_2, branch_3_2])
                end_points[end_point] = net
                if final_endpoint == end_point: return net, end_points

                end_point = &#39;Mixed_4f&#39;
                with tf.variable_scope(end_point):
                      with tf.variable_scope(&#39;Branch_0&#39;):
                        branch_0 = slim.conv2d(net, 256, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                      with tf.variable_scope(&#39;Branch_1&#39;):
                        branch_1_1 = slim.conv2d(net, 160, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_1_2 = slim.conv2d(branch_1_1, 320, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                     with tf.variable_scope(&#39;Branch_2&#39;):
                        branch_2_1 = slim.conv2d(net, 32, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_2_2 = slim.conv2d(branch_2_1, 128, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_3&#39;):
                        branch_3_1 = slim.max_pool2d(net, [3, 3], scope=&#39;MaxPool_0a_3x3&#39;)
                        branch_3_2 = slim.conv2d(branch_3_1, 128, [1, 1], scope=&#39;Conv2d_0b_1x1&#39;)
                      net = tf.concat(axis=3, 
                                      values=[branch_0, branch_1_2, branch_2_2, branch_3_2])
                end_points[end_point] = net
                if final_endpoint == end_point: return net, end_points

                end_point = &#39;MaxPool_5a_2x2&#39;
                net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point)
                end_points[end_point] = net
                if final_endpoint == end_point: return net, end_points

                end_point = &#39;Mixed_5b&#39;
                with tf.variable_scope(end_point):
                      with tf.variable_scope(&#39;Branch_0&#39;):
                        branch_0 = slim.conv2d(net, 256, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                      with tf.variable_scope(&#39;Branch_1&#39;):
                        branch_1_1 = slim.conv2d(net, 160, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_1_2 = slim.conv2d(branch_1_1, 320, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_2&#39;):
                        branch_2_1 = slim.conv2d(net, 32, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_2_2 = slim.conv2d(branch_2_1, 128, [3, 3], scope=&#39;Conv2d_0a_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_3&#39;):
                        branch_3_1 = slim.max_pool2d(net, [3, 3], scope=&#39;MaxPool_0a_3x3&#39;)
                        branch_3_2 = slim.conv2d(branch_3_1, 128, [1, 1], scope=&#39;Conv2d_0b_1x1&#39;)
                      net = tf.concat(axis=3, 
                                      values=[branch_0, branch_1_2, branch_2_2, branch_3_2])
                end_points[end_point] = net
                if final_endpoint == end_point: return net, end_points

                end_point = &#39;Mixed_5c&#39;
                with tf.variable_scope(end_point):
                      with tf.variable_scope(&#39;Branch_0&#39;):
                        branch_0 = slim.conv2d(net, 384, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                      with tf.variable_scope(&#39;Branch_1&#39;):
                        branch_1_1 = slim.conv2d(net, 192, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_1_2 = slim.conv2d(branch_1_1, 384, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_2&#39;):
                        branch_2_1 = slim.conv2d(net, 48, [1, 1], scope=&#39;Conv2d_0a_1x1&#39;)
                        branch_2_2 = slim.conv2d(branch_2_1, 128, [3, 3], scope=&#39;Conv2d_0b_3x3&#39;)
                      with tf.variable_scope(&#39;Branch_3&#39;):
                        branch_3_1 = slim.max_pool2d(net, [3, 3], scope=&#39;MaxPool_0a_3x3&#39;)
                        branch_3_2 = slim.conv2d(branch_3_1, 128, [1, 1], scope=&#39;Conv2d_0b_1x1&#39;)
                      net = tf.concat(axis=3, 
                                      values=[branch_0, branch_1_2, branch_2_2, branch_3_2])
                end_points[end_point] = net
                if final_endpoint == end_point: return net, end_points
    raise ValueError(&#39;Unknown final endpoint %s&#39; % final_endpoint)


def inception_v1(inputs,
                 num_classes=1000,
                 is_training=True,
                 dropout_keep_prob=0.8,
                 prediction_fn=slim.softmax,
                 spatial_squeeze=True,
                 reuse=None,
                 scope=&#39;InceptionV1&#39;,
                 global_pool=False):
  &quot;&quot;&quot;
  Args:
    inputs: a tensor of size [batch_size, height, width, channels].
    num_classes: number of predicted classes. If 0 or None, the logits layer
      is omitted and the input features to the logits layer (before dropout)
      are returned instead.
    is_training: whether is training or not.
    dropout_keep_prob: the percentage of activation values that are retained.
    prediction_fn: a function to get predictions out of logits.
    spatial_squeeze: if True, logits is of shape [B, C], if false logits is of
        shape [B, 1, 1, C], where B is batch_size and C is number of classes.
    reuse: whether or not the network and its variables should be reused. To be
      able to reuse &#39;scope&#39; must be given.
    scope: Optional variable_scope.
    global_pool: Optional boolean flag to control the avgpooling before the
      logits layer. If false or unset, pooling is done with a fixed window
      that reduces default-sized inputs to 1x1, while larger inputs lead to
      larger outputs. If true, any input size is pooled down to 1x1.
  Returns:
    net: a Tensor with the logits (pre-softmax activations) if num_classes
      is a non-zero integer, or the non-dropped-out input to the logits layer
      if num_classes is 0 or None.
    end_points: a dictionary from components of the network to the corresponding
      activation.
  &quot;&quot;&quot;
  # Final pooling and prediction
      with tf.variable_scope(scope, &#39;InceptionV1&#39;, [inputs], reuse=reuse) as scope:
        with slim.arg_scope([slim.batch_norm, slim.dropout],
                        is_training=is_training):
              net, end_points = inception_v1_base(inputs, scope=scope)
              with tf.variable_scope(&#39;Logits&#39;):
                if global_pool:
                  # Global average pooling.
                    net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=&#39;global_pool&#39;)
                      end_points[&#39;global_pool&#39;] = net
                else:
                      # Pooling with a fixed kernel size.
                      net = slim.avg_pool2d(net, [7, 7], stride=1, scope=&#39;AvgPool_0a_7x7&#39;)
                      end_points[&#39;AvgPool_0a_7x7&#39;] = net
                if not num_classes:
                      return net, end_points
                net = slim.dropout(net, dropout_keep_prob, scope=&#39;Dropout_0b&#39;)
                logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
                             normalizer_fn=None, scope=&#39;Conv2d_0c_1x1&#39;)
                if spatial_squeeze:
                      logits = tf.squeeze(logits, [1, 2], name=&#39;SpatialSqueeze&#39;)

                end_points[&#39;Logits&#39;] = logits
                end_points[&#39;Predictions&#39;] = prediction_fn(logits, scope=&#39;Predictions&#39;)
      return logits, end_points
inception_v1.default_image_size = 224

inception_v1_arg_scope = inception_arg_scope
</code></pre>
<h1 id="Inception-v2（Batch-Normalization）"><a href="#Inception-v2（Batch-Normalization）" class="headerlink" title="Inception v2（Batch Normalization）"></a>Inception v2（Batch Normalization）</h1><p>源自 2015 年的论文 《<em>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</em>》</p>
<h2 id="1-提出背景"><a href="#1-提出背景" class="headerlink" title="1. 提出背景"></a>1. 提出背景</h2><h3 id="1-1-炼丹的困扰"><a href="#1-1-炼丹的困扰" class="headerlink" title="1.1 炼丹的困扰"></a>1.1 炼丹的困扰</h3><p>在深度学习中，由于问题的复杂性，我们往往会使用较深层数的网络进行训练，但是对深层神经网络的训练调参更是困难且复杂。在这个过程中，需要去尝试不同的学习率、初始化参数方法（例如 <code>Xavier</code> 初始化）等方式来帮助模型加速收敛。深度神经网络之所以如此难训练，其中一个重要原因就是网络中层与层之间存在高度的关联性与耦合性。下图是一个多层的神经网络，层与层之间采用全连接的方式进行连接。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-b0ae9ea09f46bbbe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>现规定左侧为神经网络的底层，右侧为神经网络的上层。那么网络中层与层之间的关联性会导致如下的状况：随着训练的进行，网络中的参数也随着梯度下降在不停更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另一方面，参数的变化导致每一层的输入分布会发生改变，进而上层的网络需要不停地去适应这些分布变化，使得模型训练变得困难。上述这一现象叫做 Internal Covariate Shift。</p>
<h3 id="1-2-什么是-Internal-Covariate-Shift"><a href="#1-2-什么是-Internal-Covariate-Shift" class="headerlink" title="1.2 什么是 Internal Covariate Shift"></a>1.2 什么是 Internal Covariate Shift</h3><p>Batch Normalization 的原论文作者给了 Internal Covariate Shift 一个较规范的定义：<strong>在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作 Internal Covariate Shift</strong>。</p>
<p>这句话该怎么理解呢？以 1.1 中的图为例：</p>
<ul>
<li>定义每一层的线性变换为：<img src="https://upload-images.jianshu.io/upload_images/1351548-db301ba9fd5ef8fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">，其中 <img src="https://upload-images.jianshu.io/upload_images/1351548-53a2ad7232233e50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""> 代表层数</li>
<li>非线性变换为：<img src="https://upload-images.jianshu.io/upload_images/1351548-26940b4edec8545f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">，其中<img src="https://upload-images.jianshu.io/upload_images/1351548-2cf3620c81a9dbaf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">为第 <img src="https://upload-images.jianshu.io/upload_images/1351548-53a2ad7232233e50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">层的激活函数</li>
</ul>
<p>随着梯度下降的进行，每一层的参数 <img src="https://upload-images.jianshu.io/upload_images/1351548-98776014ee348266.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">和 <img src="https://upload-images.jianshu.io/upload_images/1351548-89d72059167c7829.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">都会被更新，那么<img src="https://upload-images.jianshu.io/upload_images/1351548-844b8f12c6217985.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""> 的分布也就发生了改变，进而 <img src="https://upload-images.jianshu.io/upload_images/1351548-397bf90ce160436e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">也同样出现分布的改变。而 <img src="https://upload-images.jianshu.io/upload_images/1351548-397bf90ce160436e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">作为第 <img src="https://upload-images.jianshu.io/upload_images/1351548-a501217eb510878d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">层的输入，意味着 <img src="https://upload-images.jianshu.io/upload_images/1351548-a501217eb510878d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">层就需要去不停适应这种数据分布的变化，这一过程就被叫做 Internal Covariate Shift。</p>
<h3 id="1-3-Internal-Covariate-Shift-会带来什么问题？"><a href="#1-3-Internal-Covariate-Shift-会带来什么问题？" class="headerlink" title="1.3 Internal Covariate Shift 会带来什么问题？"></a>1.3 Internal Covariate Shift 会带来什么问题？</h3><p><strong>（1）上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低</strong></p>
<p>在上面提到了梯度下降的过程会让每一层的参数 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="W^{[l]}"> 和 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="b^{[l]}"> 发生变化，进而使得每一层的线性与非线性计算结果分布产生变化。后层网络就要不停地去适应这种分布变化，这个时候就会使得整个网络的学习速率过慢。</p>
<p><strong>（2）网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度</strong></p>
<p>当在神经网络中采用饱和激活函数（saturated activation function）时，例如 sigmoid，tanh 激活函数，很容易使得模型训练陷入梯度饱和区（saturated regime）。随着模型训练的进行，我们的参数 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="W^{[l]}"> 会逐渐更新并变大，此时 <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D" alt="Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}"> 就会随之变大，并且 <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D" alt="Z^{[l]}"> 还受到更底层网络参数 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5B1%5D%7D%2CW%5E%7B%5B2%5D%7D%2C%5Ccdots%2CW%5E%7B%5Bl-1%5D%7D" alt="W^{[1]},W^{[2]},\cdots,W^{[l-1]}"> 的影响，随着网络层数的加深， <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D" alt="Z^{[l]}"> 很容易陷入梯度饱和区，此时梯度会变得很小甚至接近于 <code>0</code>，参数的更新速度就会减慢，进而就会放慢网络的收敛速度。</p>
<p>对于激活函数梯度饱和问题，有两种解决思路。第一种就是更为非饱和性激活函数，例如 ReLU 可以在一定程度上解决训练进入梯度饱和区的问题。另一种思路是，可以让激活函数的输入分布保持在一个稳定状态来尽可能避免它们陷入梯度饱和区，这也就是 Normalization 的思路。</p>
<h3 id="1-4-如何减缓-Internal-Covariate-Shift？"><a href="#1-4-如何减缓-Internal-Covariate-Shift？" class="headerlink" title="1.4 如何减缓 Internal Covariate Shift？"></a>1.4 如何减缓 Internal Covariate Shift？</h3><p>要缓解 ICS 问题，就要明白它产生的原因。ICS 产生的原因是由于参数更新带来的网络中每一层输入值分布的改变，并且随着网络层数的加深而变得更加严重，因此我们可以通过固定每一层网络输入值的分布来对减缓 ICS 问题。</p>
<p><strong>（1）白化（Whitening）</strong></p>
<p>白化（Whitening）是机器学习里面常用的一种规范化数据分布的方法，主要是 PCA 白化与 ZCA 白化。白化是对输入数据分布进行变换，进而达到以下两个目的：</p>
<ul>
<li><strong>使得输入特征分布具有相同的均值与方差。</strong>其中 PCA 白化保证了所有特征分布均值为 <code>0</code>，方差为<code>1</code>；而 ZCA 白化则保证了所有特征分布均值为 <code>0</code>，方差相同。</li>
<li><strong>去除特征之间的相关性。</strong></li>
</ul>
<p>通过白化操作，可以减缓 ICS 的问题，进而固定了每一层网络输入分布，加速网络训练过程的收敛。</p>
<p><strong>（2）Batch Normalization提出</strong></p>
<p>既然白化可以解决这个问题，为什么还要提出别的解决办法？当然是现有的方法具有一定的缺陷，白化主要有以下两个问题：</p>
<ul>
<li><strong>白化过程计算成本太高，</strong>并且在每一轮训练中的每一层我们都需要做如此高成本计算的白化操作；</li>
<li><strong>白化过程由于改变了网络每一层的分布</strong>，因而改变了网络层中本身数据的表达能力。底层网络学习到的参数信息会被白化操作丢失掉。</li>
</ul>
<p>既然有了上面两个问题，那我们的解决思路就很简单，一方面，提出的 Normalization 方法要能够简化计算过程；另一方面又需要经过规范化处理后让数据尽可能保留原始的表达能力。于是就有了简化+改进版的白化—Batch Normalization。</p>
<h2 id="2-Batch-Normalization"><a href="#2-Batch-Normalization" class="headerlink" title="2. Batch Normalization"></a>2. Batch Normalization</h2><h3 id="2-1-思路"><a href="#2-1-思路" class="headerlink" title="2.1 思路"></a>2.1 思路</h3><p>既然白化计算过程比较复杂，那就简化一点，比如可以尝试单独对每个特征进行 Normalization 就可以了，让每个特征都有均值为 <code>0</code>，方差为 <code>1</code> 的分布就行。</p>
<p>另一个问题，既然白化操作减弱了网络中每一层输入数据表达能力，那我就再加个线性变换操作，让这些数据再能够尽可能恢复本身的表达能力就好了。因此，基于上面两个解决问题的思路，作者提出了Batch Normalization，下一部分来具体讲解这个算法步骤。</p>
<h3 id="2-2-算法"><a href="#2-2-算法" class="headerlink" title="2.2 算法"></a>2.2 算法</h3><p>在深度学习中，由于采用 full batch 的训练方式对内存要求较大，且每一轮训练时间过长；一般都会采用对数据做划分，用 mini-batch 对网络进行训练。因此，Batch Normalization 也就在 mini-batch 的基础上进行计算。</p>
<h4 id="2-2-1-参数定义"><a href="#2-2-1-参数定义" class="headerlink" title="2.2.1 参数定义"></a>2.2.1 参数定义</h4><p>依旧以下图这个神经网络为例，定义网络总共有 <img src="https://www.zhihu.com/equation?tex=L" alt="L"> 层（不包含输入层）并定义如下符号：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-b0ae9ea09f46bbbe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>参数相关：</strong></p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=l" alt="l"> ：        网络中的层标号</li>
<li><img src="https://www.zhihu.com/equation?tex=L" alt="L"> ：      网络中的最后一层或总层数</li>
<li><img src="https://www.zhihu.com/equation?tex=d_l" alt="d_l"> ：     第 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层的维度，即神经元结点数</li>
<li><img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="W^{[l]}"> ：第 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层的权重矩阵， <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D%5Cin+%5Cmathbb%7BR%7D%5E%7Bd_l%5Ctimes+d_%7Bl-1%7D%7D" alt="W^{[l]}\in \mathbb{R}^{d_l\times d_{l-1}}"> </li>
<li><img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="b^{[l]}"> ：    第 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层的偏置向量， <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D%5Cin+%5Cmathbb%7BR%7D%5E%7Bd_l%5Ctimes+1%7D" alt="b^{[l]}\in \mathbb{R}^{d_l\times 1}"> </li>
<li><img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D" alt="Z^{[l]}"> ：  第 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层的线性计算结果， <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7D%5Ctimes+input%2Bb%5E%7B%5Bl%5D%7D" alt="Z^{[l]}=W^{[l]}\times input+b^{[l]}"> </li>
<li><img src="https://www.zhihu.com/equation?tex=g%5E%7B%5Bl%5D%7D%28%5Ccdot%29" alt="g^{[l]}(\cdot)"> ：第 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层的激活函数</li>
<li><img src="https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D" alt="A^{[l]}"> ：   第 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层的非线性激活结果， <img src="https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D%3Dg%5E%7B%5Bl%5D%7D%28Z%5E%7B%5Bl%5D%7D%29" alt="A^{[l]}=g^{[l]}(Z^{[l]})"> </li>
</ul>
<p><strong>样本相关：</strong></p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=M" alt="M"> ：训练样本的数量</li>
<li><img src="https://www.zhihu.com/equation?tex=N" alt="N"> ：训练样本的特征数</li>
<li><img src="https://www.zhihu.com/equation?tex=X" alt="X"> ：训练样本集， <img src="https://www.zhihu.com/equation?tex=X%3D%5C%7Bx%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C%5Ccdots%2Cx%5E%7B%28M%29%7D%5C%7D%EF%BC%8CX%5Cin+%5Cmathbb%7BR%7D%5E%7BN%5Ctimes+M%7D" alt="X=\{x^{(1)},x^{(2)},\cdots,x^{(M)}\}，X\in \mathbb{R}^{N\times M}"> （注意这里 <img src="https://www.zhihu.com/equation?tex=X" alt="X"> 的一列是一个样本）</li>
<li><img src="https://www.zhihu.com/equation?tex=m" alt="m"> ：batch size，即每个 batch 中样本的数量</li>
<li><img src="https://www.zhihu.com/equation?tex=%5Cchi%5E%7B%28i%29%7D" alt="\chi^{(i)}"> ：第 <img src="https://www.zhihu.com/equation?tex=i" alt="i"> 个 mini-batch 的训练数据， <img src="https://www.zhihu.com/equation?tex=X%3D+%5C%7B%5Cchi%5E%7B%281%29%7D%2C%5Cchi%5E%7B%282%29%7D%2C%5Ccdots%2C%5Cchi%5E%7B%28k%29%7D%5C%7D" alt="X= \{\chi^{(1)},\chi^{(2)},\cdots,\chi^{(k)}\}"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Cchi%5E%7B%28i%29%7D%5Cin+%5Cmathbb%7BR%7D%5E%7BN%5Ctimes+m%7D" alt="\chi^{(i)}\in \mathbb{R}^{N\times m}"></li>
</ul>
<h4 id="2-2-2-算法步骤"><a href="#2-2-2-算法步骤" class="headerlink" title="2.2.2 算法步骤"></a>2.2.2 算法步骤</h4><p>介绍算法思路沿袭前面 BN 提出的思路来讲。第一点，对每个特征进行独立的 Normalization。考虑一个 batch 的训练，传入 <code>m</code> 个训练样本，并关注网络中的某一层，忽略上标 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 。</p>
<p><img src="https://www.zhihu.com/equation?tex=Z%5Cin+%5Cmathbb%7BR%7D%5E%7Bd_l%5Ctimes+m%7D" alt=""> </p>
<p>关注当前层的第 <img src="https://www.zhihu.com/equation?tex=j" alt="j"> 个维度，也就是第 <img src="https://www.zhihu.com/equation?tex=j" alt="j"> 个神经元结点，则有 <img src="https://www.zhihu.com/equation?tex=Z_j%5Cin+%5Cmathbb%7BR%7D%5E%7B1%5Ctimes+m%7D" alt="Z_j\in \mathbb{R}^{1\times m}"> 。对当前维度进行规范化：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmu_j%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em+Z_j%5E%7B%28i%29%7D" alt="\mu_j=\frac{1}{m}\sum_{i=1}^m Z_j^{(i)}"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2_j%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%28Z_j%5E%7B%28i%29%7D-%5Cmu_j%29%5E2" alt="\sigma^2_j=\frac{1}{m}\sum_{i=1}^m(Z_j^{(i)}-\mu_j)^2"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7BZ%7D_j%3D%5Cfrac%7BZ_j-%5Cmu_j%7D%7B%5Csqrt%7B%5Csigma_j%5E2%2B%5Cepsilon%7D%7D" alt="\hat{Z}_j=\frac{Z_j-\mu_j}{\sqrt{\sigma_j^2+\epsilon}}"> </p>
<blockquote>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon"> 是为了防止方差为 <code>0</code> 产生无效计算。</p>
</blockquote>
<p>结合个具体的例子来进行计算。下图只关注第 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层的计算结果，左边的矩阵是 <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D" alt="Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}"> 线性计算结果，还未进行激活函数的非线性变换。此时每一列是一个样本，图中可以看到共有 <code>8</code> 列，代表当前训练样本的 batch 中共有 <code>8</code> 个样本，每一行代表当前 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层神经元的一个节点，可以看到当前 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层共有 <code>4</code> 个神经元结点，即第 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层维度为 <code>4</code>。可以看到，每行的数据分布都不同。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-438e93b28dd50e2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于第一个神经元，可以求得 <img src="https://www.zhihu.com/equation?tex=%5Cmu_1%3D1.65" alt="\mu_1=1.65"> ， <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2_1%3D0.44" alt="\sigma^2_1=0.44"> （其中 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon%3D10%5E%7B-8%7D" alt="\epsilon=10^{-8}"> ），此时利用 <img src="https://www.zhihu.com/equation?tex=%5Cmu_1%2C%5Csigma%5E2_1" alt="\mu_1,\sigma^2_1"> 对第一行数据（第一个维度）进行 Normalization 得到新的值 <img src="https://www.zhihu.com/equation?tex=%5B-0.98%2C-0.23%2C-0.68%2C-1.13%2C0.08%2C0.68%2C2.19%2C0.08%5D" alt="[-0.98,-0.23,-0.68,-1.13,0.08,0.68,2.19,0.08]"> 。同理可以计算出其他输入维度归一化后的值。如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-26f463ac1426ca22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>通过上面的变换，<strong>解决了第一个问题，即用更加简化的方式来对数据进行规范化，使得第 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层的输入每个特征的分布均值为0，方差为 <code>1</code>。</strong></p>
<p>如同上面提到的，Normalization 操作虽然缓解了 ICS 问题，让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是通过变换操作改变了原有数据的信息表达（representation ability of the network），使得底层网络学习到的参数信息丢失。另一方面，通过让每一层的输入分布均值为 <code>0</code>，方差为 <code>1</code>，会使得输入在经过 sigmoid 或 tanh 激活函数时，容易陷入非线性激活函数的线性区域。</p>
<p>因此，BN 又引入了两个可学习（learnable）的参数 <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma"> 与 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta"> 。这两个参数的引入是为了恢复数据本身的表达能力，对规范化后的数据进行线性变换，即 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BZ_j%7D%3D%5Cgamma_j+%5Chat%7BZ%7D_j%2B%5Cbeta_j" alt="\tilde{Z_j}=\gamma_j \hat{Z}_j+\beta_j"> 。特别地，当 <img src="https://www.zhihu.com/equation?tex=%5Cgamma%5E2%3D%5Csigma%5E2%2C%5Cbeta%3D%5Cmu" alt="\gamma^2=\sigma^2,\beta=\mu"> 时，可以实现等价变换（identity transform）并且保留了原始输入特征的分布信息。</p>
<p><strong>通过上面的步骤，我们就在一定程度上保证了输入数据的表达能力。</strong></p>
<p>以上就是整个 Batch Normalization 在模型训练中的算法和思路。</p>
<blockquote>
<p>补充： 在进行 normalization 的过程中，由于规范化操作会对减去均值，因此，偏置项 <img src="https://www.zhihu.com/equation?tex=b" alt="b"> 可以被忽略掉或可以被置为 <code>0</code>，即 <img src="https://www.zhihu.com/equation?tex=BN%28Wu%2Bb%29%3DBN%28Wu%29" alt="BN(Wu+b)=BN(Wu)"></p>
</blockquote>
<h4 id="2-2-3-公式"><a href="#2-2-3-公式" class="headerlink" title="2.2.3 公式"></a>2.2.3 公式</h4><p>对于神经网络中的第 <img src="https://www.zhihu.com/equation?tex=l" alt="l"> 层，有：</p>
<p><img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D" alt="Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmu%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5EmZ%5E%7B%5Bl%5D%28i%29%7D" alt="\mu=\frac{1}{m}\sum_{i=1}^mZ^{[l](i)}"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%28Z%5E%7B%5Bl%5D%28i%29%7D-%5Cmu%29%5E2" alt="\sigma^2=\frac{1}{m}\sum_{i=1}^m(Z^{[l](i)}-\mu)^2"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BZ%7D%5E%7B%5Bl%5D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7BZ%5E%7B%5Bl%5D%7D-%5Cmu%7D%7B%5Csqrt%7B%5Csigma%5E2%2B%5Cepsilon%7D%7D%2B%5Cbeta" alt="\tilde{Z}^{[l]}=\gamma\cdot\frac{Z^{[l]}-\mu}{\sqrt{\sigma^2+\epsilon}}+\beta"></p>
<p><img src="https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D%3Dg%5E%7B%5Bl%5D%7D%28%5Ctilde%7BZ%7D%5E%7B%5Bl%5D%7D%29" alt="A^{[l]}=g^{[l]}(\tilde{Z}^{[l]})"></p>
<h2 id="3-测试阶段如何使用-Batch-Normalization？"><a href="#3-测试阶段如何使用-Batch-Normalization？" class="headerlink" title="3. 测试阶段如何使用 Batch Normalization？"></a>3. 测试阶段如何使用 Batch Normalization？</h2><p>BN 在每一层计算的 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu"> 与 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2" alt="\sigma^2"> 都是基于当前 batch 中的训练数据，但是这就带来了一个问题：在预测阶段，有可能只需要预测一个样本或很少的样本，没有像训练样本中那么多的数据，此时 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu"> 与 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2" alt="\sigma^2"> 的计算一定是有偏估计，这个时候该如何进行计算呢？</p>
<p>利用 BN 训练好模型后，保留了每组 mini-batch 训练数据在网络中每一层的 <img src="https://www.zhihu.com/equation?tex=%5Cmu_%7Bbatch%7D" alt="\mu_{batch}"> 与 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2_%7Bbatch%7D" alt="\sigma^2_{batch}"> 。此时使用整个样本的统计量来对测试数据（test data）进行归一化，具体来说使用均值与方差的无偏估计：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmu_%7Btest%7D%3D%5Cmathbb%7BE%7D+%28%5Cmu_%7Bbatch%7D%29" alt="\mu_{test}=\mathbb{E} (\mu_{batch})"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2_%7Btest%7D%3D%5Cfrac%7Bm%7D%7Bm-1%7D%5Cmathbb%7BE%7D%28%5Csigma%5E2_%7Bbatch%7D%29" alt="\sigma^2_{test}=\frac{m}{m-1}\mathbb{E}(\sigma^2_{batch})"> </p>
<p>得到每个特征的均值与方差的无偏估计后，对测试数据采用同样的 normalization 方法：</p>
<p><img src="https://www.zhihu.com/equation?tex=BN%28X_%7Btest%7D%29%3D%5Cgamma%5Ccdot+%5Cfrac%7BX_%7Btest%7D-%5Cmu_%7Btest%7D%7D%7B%5Csqrt%7B%5Csigma%5E2_%7Btest%7D%2B%5Cepsilon%7D%7D%2B%5Cbeta" alt="BN(X_{test})=\gamma\cdot \frac{X_{test}-\mu_{test}}{\sqrt{\sigma^2_{test}+\epsilon}}+\beta"> </p>
<p>另外，除了采用整体样本的无偏估计外。Ng 在 Coursera 上的 Deep Learning 课程指出，可以对 train 阶段每个 batch 计算的 <code>mean/variance</code> 采用指数加权平均来得到 test 阶段 <code>mean/variance</code> 的估计。</p>
<h2 id="4-Batch-Normalization-的优势"><a href="#4-Batch-Normalization-的优势" class="headerlink" title="4. Batch Normalization 的优势"></a>4. Batch Normalization 的优势</h2><p>Batch Normalization 在实际工程中被证明了能够缓解神经网络难以训练的问题，BN 具有的有事可以总结为以下三点：</p>
<p><strong>（1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度</strong></p>
<p>BN 通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。</p>
<p><strong>（2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</strong></p>
<p>在神经网络中，我们经常会谨慎地采用一些权重初始化方法（例如 <code>Xavier</code>）或者合适的学习率来保证网络稳定训练。</p>
<p>当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用 BN 的网络将不会受到参数数值大小的影响。例如，将参数 <img src="https://www.zhihu.com/equation?tex=W" alt="W"> 进行缩放得到 <img src="https://www.zhihu.com/equation?tex=aW" alt="aW"> 。对于缩放前的值 <img src="https://www.zhihu.com/equation?tex=Wu" alt="Wu"> ，设其均值为 <img src="https://www.zhihu.com/equation?tex=%5Cmu_1" alt="\mu_1"> ，方差为 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2_1" alt="\sigma^2_1"> ；对于缩放值 <img src="https://www.zhihu.com/equation?tex=aWu" alt="aWu"> ，设其均值为 <img src="https://www.zhihu.com/equation?tex=%5Cmu_2" alt="\mu_2"> ，方差为 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2_2" alt="\sigma^2_2"> ，则有：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmu_2%3Da%5Cmu_1" alt="\mu_2=a\mu_1"> ， <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2_2%3Da%5E2%5Csigma%5E2_1" alt="\sigma^2_2=a^2\sigma^2_1"> </p>
<p>忽略 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon"> ，则有：</p>
<p><img src="https://www.zhihu.com/equation?tex=BN%28aWu%29%3D%5Cgamma%5Ccdot%5Cfrac%7BaWu-%5Cmu_2%7D%7B%5Csqrt%7B%5Csigma%5E2_2%7D%7D%2B%5Cbeta%3D%5Cgamma%5Ccdot%5Cfrac%7BaWu-a%5Cmu_1%7D%7B%5Csqrt%7Ba%5E2%5Csigma%5E2_1%7D%7D%2B%5Cbeta%3D%5Cgamma%5Ccdot%5Cfrac%7BWu-%5Cmu_1%7D%7B%5Csqrt%7B%5Csigma%5E2_1%7D%7D%2B%5Cbeta%3DBN%28Wu%29" alt="BN(aWu)=\gamma\cdot\frac{aWu-\mu_2}{\sqrt{\sigma^2_2}}+\beta=\gamma\cdot\frac{aWu-a\mu_1}{\sqrt{a^2\sigma^2_1}}+\beta=\gamma\cdot\frac{Wu-\mu_1}{\sqrt{\sigma^2_1}}+\beta=BN(Wu)"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BBN%28%28aW%29u%29%7D%7D%7B%5Cpartial%7Bu%7D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7BaW%7D%7B%5Csqrt%7B%5Csigma%5E2_2%7D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7BaW%7D%7B%5Csqrt%7Ba%5E2%5Csigma%5E2_1%7D%7D%3D%5Cfrac%7B%5Cpartial%7BBN%28Wu%29%7D%7D%7B%5Cpartial%7Bu%7D%7D" alt="\frac{\partial{BN((aW)u)}}{\partial{u}}=\gamma\cdot\frac{aW}{\sqrt{\sigma^2_2}}=\gamma\cdot\frac{aW}{\sqrt{a^2\sigma^2_1}}=\frac{\partial{BN(Wu)}}{\partial{u}}"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BBN%28%28aW%29u%29%7D%7D%7B%5Cpartial%7B%28aW%29%7D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7Bu%7D%7B%5Csqrt%7B%5Csigma%5E2_2%7D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7Bu%7D%7Ba%5Csqrt%7B%5Csigma%5E2_1%7D%7D%3D%5Cfrac%7B1%7D%7Ba%7D%5Ccdot%5Cfrac%7B%5Cpartial%7BBN%28Wu%29%7D%7D%7B%5Cpartial%7BW%7D%7D" alt="\frac{\partial{BN((aW)u)}}{\partial{(aW)}}=\gamma\cdot\frac{u}{\sqrt{\sigma^2_2}}=\gamma\cdot\frac{u}{a\sqrt{\sigma^2_1}}=\frac{1}{a}\cdot\frac{\partial{BN(Wu)}}{\partial{W}}"> </p>
<blockquote>
<p>注：公式中的 <img src="https://www.zhihu.com/equation?tex=u" alt="u"> 是当前层的输入，也是前一层的输出；不是下标！</p>
</blockquote>
<p>可以看到，经过 BN 操作以后，权重的缩放值会被“抹去”，因此保证了输入数据分布稳定在一定范围内。另外，权重的缩放并不会影响到对 <img src="https://www.zhihu.com/equation?tex=u" alt="u"> 的梯度计算；并且当权重越大时，即 <img src="https://www.zhihu.com/equation?tex=a" alt="a"> 越大， <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Ba%7D" alt="\frac{1}{a}"> 越小，意味着权重 <img src="https://www.zhihu.com/equation?tex=W" alt="W"> 的梯度反而越小，这样 BN 就保证了梯度不会依赖于参数的 scale，使得参数的更新处在更加稳定的状态。</p>
<p>因此，在使用 Batch Normalization 之后，抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强，此时我们可以设置较大的学习率而不用过于担心模型 divergence 的风险。</p>
<p><strong>（3）BN 允许网络使用饱和性激活函数（例如 sigmoid，tanh等），缓解梯度消失问题</strong></p>
<p>在不使用 BN 层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过 normalize 操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习 <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma"> 与 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta"> 又让数据保留更多的原始信息。</p>
<p><strong>（4）BN具有一定的正则化效果</strong></p>
<p>在 Batch Normalization 中，由于使用 mini-batch 的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同 mini-batch 的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与 Dropout 通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。</p>
<p>另外，原作者通过也证明了网络加入 BN 后，可以丢弃 Dropout，模型也同样具有很好的泛化效果。</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h2><p>现在通过实际的代码来对比加入 BN 前后的模型效果。使用 MNIST 数据集作为数据基础，并使用 TensorFlow 中的 Batch Normalization 结构来进行 BN 的实现。</p>
<h1 id="为什么卷积层可以“代替”全连接层？"><a href="#为什么卷积层可以“代替”全连接层？" class="headerlink" title="为什么卷积层可以“代替”全连接层？"></a>为什么卷积层可以“代替”全连接层？</h1><p>卷积和全连接的区别大致在于：</p>
<ul>
<li>卷积是局部连接，计算局部信息</li>
<li>全连接是全局连接，计算全局信息（但二者都是采用的点积运算）</li>
</ul>
<p>如果卷积核的 <code>kernel_size</code> 和输入 feature maps 的 size 一样，那么相当于该卷积核计算了全部 feature maps 的信息，则相当于是一个 <code>kernel_size∗1</code> 的全连接。</p>
<p>在全连接层上，相当于是 <code>n∗m</code>（其中，<code>n</code> 是输入的维度，<code>m</code> 是输出的维度）的全连接，其计算是通过一次导入到内存中计算完成；如果是在最后一个 feature maps 上展开后进行的全连接，这里若不进行展开，直接使用 <code>output_size</code> 的卷积核代替，则相当于是 <code>n∗1</code> 的全连接（这里的 <code>n</code> 就是 feature maps 展开的向量大小，也就是卷积核的大小 <code>kernel_size∗kernel_size</code>），使用 <code>m</code> 个卷积核，则可以相当于 <code>n∗m</code> 的全连接层。</p>
<p>但用卷积层代替全连接层的方式，其卷积核的计算是并行的，不需要同时读入内存中，所以使用卷积层的方式代替全连接层可以加开模型的优化。</p>
<h1 id="为什么不能用卷积层代替全连接层的方式，使得模型处理不同大小的输入？"><a href="#为什么不能用卷积层代替全连接层的方式，使得模型处理不同大小的输入？" class="headerlink" title="为什么不能用卷积层代替全连接层的方式，使得模型处理不同大小的输入？"></a>为什么不能用卷积层代替全连接层的方式，使得模型处理不同大小的输入？</h1><p>因为卷积层的运算是通过卷积核，说到底也就是点积运算，是需要事先设定参数的大小。 但如果这种操作用于处理不同 size 的输入，则实际上每次训练时，该层的参数 size 是不一样的，也就是模型不能得到训练。 虽然使用卷积层代替全连接层来处理不同大小输入的模型在整个流程上看起来没什么问题，但根本上，该模型是不能得到良好训练的（从代替的那层卷积层开始，其后的每一层得到的输入分布其实是一直在变化的，所以波动会比较大）。</p>
<h1 id="全卷积神经网络Fully-Convolutional-Network-FCN"><a href="#全卷积神经网络Fully-Convolutional-Network-FCN" class="headerlink" title="全卷积神经网络Fully Convolutional Network (FCN)"></a>全卷积神经网络Fully Convolutional Network (FCN)</h1><p>全卷积神经网络即把 CNN 网络最后的全连接层替换为卷积，这样带来的好处：</p>
<ul>
<li><p>卷积层和全连接层的区别：卷积层为局部连接；而全连接层则使用图像的全局信息。可以想象一下，最大的局部是不是就等于全局了？这首先说明全连接层使用卷积层来替代的可行性。</p>
</li>
<li><p>使用卷积层代替全连接层，可以让卷积网络在一张更大的输入图片上滑动，得到每个区域的输出（这样就突破了输入尺寸的限制）。解读如下：</p>
</li>
</ul>
<p>  <img src="https://upload-images.jianshu.io/upload_images/1351548-1e56e12d5cbf5aa3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="Yolo-v1"><a href="#Yolo-v1" class="headerlink" title="Yolo v1"></a>Yolo v1</h1><p>近几年来比较流行的目标检测算法可以分为两类：</p>
<ul>
<li><p>一类是基于 Region Proposal 的 RCNN 系算法（RCNN，Fast RCNN，Faster RCNN），它们是 two-stage 的，需要先使用启发式方法（selective search）或者 CNN 网络（RPN）产生 Region Proposal，然后再在 Region Proposal 上做分类与回归。</p>
</li>
<li><p>另一类是 Yolo，SSD 这类 one-stage 算法，其仅仅使用一个 CNN 网络直接预测不同目标的类别与位置。</p>
</li>
</ul>
<p>第一类方法是准确度高一些，但是速度慢，但是第二类算法是速度快，但是准确性要低一些。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-9e53d3c99cb9f789.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="1-滑动窗口与-CNN"><a href="#1-滑动窗口与-CNN" class="headerlink" title="1. 滑动窗口与 CNN"></a>1. 滑动窗口与 CNN</h2><p><strong>采用滑动窗口的目标检测算法思路非常简单，它将检测问题转化为了图像分类问题。</strong>其基本原理就是采用不同大小和比例（宽高比）的窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类，这样就可以实现对整张图片的检测了，如下图所示，如 DPM 就是采用这种思路。但是这个方法有致命的缺点，就是你并不知道要检测的目标大小是什么规模，所以你要设置不同大小和比例的窗口去滑动，而且还要选取合适的步长。但是这样会产生很多的子区域，并且都要经过分类器去做预测，这需要很大的计算量，所以你的分类器不能太复杂，因为要保证速度。解决思路之一就是减少要分类的子区域，这就是 RCNN 的一个改进策略，其采用了 selective search 方法来找到最有可能包含目标的子区域（Region Proposal），其实可以看成采用启发式方法过滤掉很多子区域，这会提升效率。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-057bedc91673a5a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如果你使用 CNN 分类器，那么滑动窗口是非常耗时的。但是结合卷积运算的特点，我们可以使用 CNN 实现更高效的滑动窗口方法。这里要介绍的是一种全卷积的方法，简单来说就是网络中用卷积层代替了全连接层，如下图所示。输入图片大小是 <code>16x16</code>，经过一系列卷积操作，提取了 <code>2x2</code> 的特征图，但是这个 <code>2x2</code> 的图上每个元素都是和原图是一一对应的，如图中蓝色的格子对应蓝色的区域，这不就是相当于在原图上做大小为 <code>14x14</code> 的窗口滑动，且步长为 <code>2</code>，共产生 <code>4</code> 个字区域。最终输出的通道数为 <code>4</code>，可以看成 <code>4</code> 个类别的预测概率值，这样一次 CNN 计算就可以实现窗口滑动的所有子区域的分类预测。这其实是 Overfeat 算法的思路。之所可以 CNN 可以实现这样的效果是因为卷积操作的特性，就是图片的空间位置信息的不变性，尽管卷积过程中图片大小减少，但是位置对应关系还是保存的。说点题外话，这个思路也被 RCNN 借鉴，从而诞生了 Fast RCNN 算法。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-a2e6c2ab8f7d6eeb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面尽管可以减少滑动窗口的计算量，但是只是针对一个固定大小与步长的窗口，这是远远不够的。Yolo 算法很好的解决了这个问题，它不再是窗口滑动了，而是直接将原始图片分割成互不重合的小方块，然后通过卷积最后生产这样大小的特征图，基于上面的分析，可以认为特征图的每个元素也是对应原始图片的一个小方块，然后用每个元素来可以预测那些中心点在该小方格内的目标，这就是 Yolo 算法的朴素思想。</p>
<h2 id="2-设计理念"><a href="#2-设计理念" class="headerlink" title="2.  设计理念"></a>2.  设计理念</h2><p>整体来看，Yolo 算法采用一个单独的 CNN 模型实现 end-to-end 的目标检测，整个系统如下图所示：首先将输入图片 resize 到 <code>448x448</code>，然后送入 CNN 网络，最后处理网络预测结果得到检测的目标。相比 RCNN 算法，其是一个统一的框架，其速度更快，而且 Yolo 的训练过程也是 end-to-end 的。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-1e8dc5ecca8eaca0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>具体来说，Yolo 的 CNN 网络将输入的图片分割成 <img src="https://www.zhihu.com/equation?tex=S%5Ctimes+S" alt="S\times S"> 网格，然后<strong>每个单元格负责去检测那些中心点落在该格子内的目标</strong>，如下图所示，可以看到狗这个目标的中心落在左下角一个单元格内，那么该单元格负责预测这个狗。每个单元格会预测 <img src="https://www.zhihu.com/equation?tex=B" alt="B"> 个边界框（bounding box）以及边界框的置信度（confidence score）。                      </p>
<p>所谓置信度其实包含两个方面：</p>
<ul>
<li><p>一是这个边界框含有目标的可能性大小。前者记为 <img src="https://www.zhihu.com/equation?tex=Pr%28object%29" alt="Pr(object)"> ，当该边界框是背景时（即不包含目标），此时 <img src="https://www.zhihu.com/equation?tex=Pr%28object%29%3D0" alt="Pr(object)=0"> 。而当该边界框包含目标时， <img src="https://www.zhihu.com/equation?tex=Pr%28object%29%3D1" alt="Pr(object)=1"> 。</p>
</li>
<li><p>二是这个边界框的准确度。边界框的准确度可以用预测框与实际框（ground truth）的 IoU 来表征，记为 <img src="https://www.zhihu.com/equation?tex=%5Ctext%7BIOU%7D%5E%7Btruth%7D_%7Bpred%7D" alt="\text{IOU}^{truth}_{pred}"> 。</p>
</li>
</ul>
<p>因此，置信度可以定义为 <img src="https://www.zhihu.com/equation?tex=Pr%28object%29%2A%5Ctext%7BIOU%7D%5E%7Btruth%7D_%7Bpred%7D" alt="Pr(object)*\text{IOU}^{truth}_{pred}"> 。</p>
<p>很多人可能将 Yolo 的置信度看成边界框是否含有目标的概率，但是其实它是两个因子的乘积，预测框的准确度也反映在里面。边界框的大小与位置可以用 <code>4</code> 个值来表征： <img src="https://www.zhihu.com/equation?tex=%28x%2C+y%2Cw%2Ch%29" alt="(x, y,w,h)"> ，其中 <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%29" alt="(x,y)"> 是边界框的中心坐标，而 <img src="https://www.zhihu.com/equation?tex=w" alt="w"> 和 <img src="https://www.zhihu.com/equation?tex=h" alt="h"> 是边界框的宽与高。</p>
<p>还有一点要注意，中心坐标的预测值 <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%29" alt="(x,y)"> 是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的，单元格的坐标定义如下图所示。而边界框的 <img src="https://www.zhihu.com/equation?tex=w" alt="w"> 和 <img src="https://www.zhihu.com/equation?tex=h" alt="h"> 预测值是相对于整个图片的宽与高的比例，这样理论上 <code>4</code> 个元素的大小应该在 <img src="https://www.zhihu.com/equation?tex=%5B0%2C1%5D" alt="[0,1]"> 范围。这样，每个边界框的预测值实际上包含 <code>5</code> 个元素： <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%2Cw%2Ch%2Cc%29" alt="(x,y,w,h,c)"> ，其中前 <code>4</code> 个表征边界框的大小与位置，而最后一个值是置信度。 </p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-e4b1c735185f8ea1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>还有分类问题，对于每一个单元格其还要给出预测出 <img src="https://www.zhihu.com/equation?tex=C" alt="C"> 个类别概率值，其表征的是由该单元格负责预测的边界框其目标属于各个类别的概率。但是这些概率值其实是在各个边界框置信度下的条件概率，即 <img src="https://www.zhihu.com/equation?tex=Pr%28class_%7Bi%7D%7Cobject%29" alt="Pr(class_{i}|object)"> 。值得注意的是，不管一个单元格预测多少个边界框，其只预测一组类别概率值，这是 Yolo 算法的一个缺点，在后来的改进版本中，Yolo9000 是把类别概率预测值与边界框是绑定在一起的。同时，我们可以计算出各个边界框类别置信度（class-specific confidence scores）：</p>
<p> <img src="https://www.zhihu.com/equation?tex=Pr%28class_%7Bi%7D%7Cobject%29%2APr%28object%29%2A%5Ctext%7BIOU%7D%5E%7Btruth%7D_%7Bpred%7D%3DPr%28class_%7Bi%7D%29%2A%5Ctext%7BIOU%7D%5E%7Btruth%7D_%7Bpred%7D" alt="Pr(class_{i}|object)*Pr(object)*\text{IOU}^{truth}_{pred}=Pr(class_{i})*\text{IOU}^{truth}_{pred}"> </p>
<p>边界框类别置信度表征的是该边界框中目标属于各个类别的可能性大小以及边界框匹配目标的好坏。后面会说，一般会根据类别置信度来过滤网络的预测框。</p>
<p>总结一下，每个单元格需要预测 <img src="https://www.zhihu.com/equation?tex=%28B%2A5%2BC%29" alt="(B*5+C)"> （每个单元格会预测 <img src="https://www.zhihu.com/equation?tex=B" alt="B"> 个边界框（bounding box）；对于每一个单元格其还要给出预测出 <img src="https://www.zhihu.com/equation?tex=C" alt="C"> 个类别概率值，其表征的是由该单元格负责预测的边界框其目标属于各个类别的概率）个值。如果将输入图片划分为 <img src="https://www.zhihu.com/equation?tex=S%5Ctimes+S" alt="S\times S"> 网格，那么最终预测值为 <img src="https://www.zhihu.com/equation?tex=S%5Ctimes+S%5Ctimes+%28B%2A5%2BC%29" alt="S\times S\times (B*5+C)"> 大小的张量。整个模型的预测值结构如下图所示。对于 PASCAL VOC 数据，其共有 <code>20</code> 个类别，如果使用 <img src="https://www.zhihu.com/equation?tex=S%3D7%2CB%3D2" alt="S=7,B=2"> ，那么最终的预测结果就是 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes+7%5Ctimes+30" alt="7\times 7\times 30"> 大小的张量。在下面的网络结构中我们会详细讲述每个单元格的预测值的分布位置。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-9b46d719c61170d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3-网络设计"><a href="#3-网络设计" class="headerlink" title="3.  网络设计"></a>3.  网络设计</h2><p>Yolo 采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考 GooLeNet 模型，包含 <code>24</code> 个卷积层和 <code>2</code> 个全连接层，如下图所示。对于卷积层，主要使用 <code>1x1</code> 卷积来做 channel reduction，然后紧跟 <code>3x3</code> 卷积。对于卷积层和全连接层，采用 Leaky ReLU 激活函数： <img src="https://www.zhihu.com/equation?tex=max%28x%2C+0.1x%29" alt="max(x, 0.1x)"> ，但是最后一层却采用线性激活函数。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-6834afa6247f6ccd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="DL-中的-GPU-和显存分析"><a href="#DL-中的-GPU-和显存分析" class="headerlink" title="DL 中的 GPU 和显存分析"></a>DL 中的 GPU 和显存分析</h1><h2 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1. 基础知识"></a>1. 基础知识</h2><p><code>nvidia-smi</code> 是 Nvidia 显卡命令行管理套件，基于 NVML 库，旨在管理和监控 Nvidia GPU 设备（推荐一个好用的小工具：<code>gpustat</code>，直接 <code>pip install gpustat</code>即可安装，<code>gpustat</code> 基于 <code>nvidia-smi</code>，可以提供更美观简洁的展示，结合 watch 命令，可以<strong>动态实时监控</strong> GPU 的使用情况。</p>
<pre><code class="python">watch --color -n1 gpustat -cpu 
</code></pre>
<p>输出为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-42d83bf4e6bd2559.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>显存可以看成是空间，类似于内存。</strong></p>
<ul>
<li>显存用于存放模型，数据</li>
<li>显存越大，所能运行的网络也就越大</li>
</ul>
<p>GPU 计算单元类似于 CPU 中的核，用来进行数值计算。衡量计算量的单位是 <code>flop</code>： <em>the number of floating-point multiplication-adds</em>，浮点数先乘后加算一个 <code>flop</code>。计算能力越强大，速度越快。衡量计算能力的单位是 <code>flops</code>： 每秒能执行的 <code>flop</code> 数量：</p>
<pre><code class="python">1*2 + 3                  1 flop
1*2 + 3*4 + 4*5          3 flop 
</code></pre>
<h2 id="2-显存分析"><a href="#2-显存分析" class="headerlink" title="2. 显存分析"></a>2. 显存分析</h2><h3 id="2-1-存储指标"><a href="#2-1-存储指标" class="headerlink" title="2.1 存储指标"></a>2.1 存储指标</h3><pre><code class="python3">1 Byte = 8 bit
1 K = 1024 Byte
1 M = 1024 K
1 G = 1024 M
1 T = 1024 G
</code></pre>
<p>除了 <code>K</code>、<code>M</code>，<code>G</code>，<code>T</code> 等之外，常用的还有 <code>KB</code> 、<code>MB</code>，<code>GB</code>，<code>TB</code> 。二者有细微的差别。</p>
<pre><code class="python">1 Byte = 8 bit
1 KB = 1000 Byte
1 MB = 1000 KB
1 GB = 1000 MB
1 TB = 1000 GB
</code></pre>
<p><code>K</code>、<code>M</code>，<code>G</code>，<code>T</code> 是以 <code>1024</code> 为底，而 <code>KB</code> 、<code>MB</code>，<code>GB</code>，<code>TB</code> 以 <code>1000</code> 为底。不过一般来说，在估算显存大小的时候，不需要严格的区分这二者。对于 OEM 厂商，在进行硬盘的设计时，便采用的是第二种方式，所以这就是为什么标注 <code>512G</code> 容量实际上只有大约 <code>476G</code> 左右的原因。</p>
<p>常用的数值类型如下图所示（ <code>int64</code> 准确的说应该是对应 <code>C</code> 中的 <code>long long</code> 类型，<code>long</code> 类型在 <code>32</code> 位机器上等效于 <code>int32</code>）：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-c06540b2f54c3d41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，<code>float32</code> 是在深度学习中最常用的数值类型，称为单精度浮点数，每一个单精度浮点数占用<code>4 Byte</code> 的显存。</p>
<p>举例来说：有一个 <code>1000x1000</code> 的矩阵，<code>float32</code> 类型，那么占用的显存差不多就是：</p>
<pre><code class="python">1000 x 1000 x 4 Byte = 4 MB
</code></pre>
<p><code>32x3x256x256</code> 的四维数组（<code>BxCxHxW</code>）占用显存为：<code>24 M</code></p>
<h3 id="2-2-神经网络显存占用"><a href="#2-2-神经网络显存占用" class="headerlink" title="2.2 神经网络显存占用"></a>2.2 神经网络显存占用</h3><p>神经网络模型占用的显存包括：</p>
<ul>
<li>模型自身的参数</li>
<li>模型的输出</li>
</ul>
<p>举例来说，对于如下图所示的一个全连接网络(不考虑偏置项 <code>b</code>）：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-9447d45ecb56a49f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>模型的显存占用包括：</p>
<ul>
<li>参数：二维数组 <code>w</code></li>
<li>模型的输出： 二维数组 <code>y</code></li>
</ul>
<p>输入 <code>X</code> 可以看成是上一层的输出，因此把它的显存占用归于上一层。这么看来显存占用就是 <code>w</code> 和 <code>y</code> 两个数组？并非如此！！！</p>
<h4 id="2-2-1-参数的显存占用"><a href="#2-2-1-参数的显存占用" class="headerlink" title="2.2.1 参数的显存占用"></a>2.2.1 参数的显存占用</h4><p>只有有参数的层，才会有显存占用。这部份的显存占用和<strong>输入无关</strong>，模型加载完成之后就会占用，主要包括：</p>
<ul>
<li><p><strong>有参数的层主要包括：</strong></p>
<ul>
<li>卷积</li>
<li>全连接 </li>
<li>BatchNorm</li>
<li>Embedding层</li>
<li>… …</li>
</ul>
</li>
<li><p><strong>无参数的层</strong>：</p>
<ul>
<li>多数的激活层（Sigmoid/ReLU）</li>
<li>池化层</li>
<li>Dropout</li>
<li>… …</li>
</ul>
</li>
</ul>
<p>更具体的来说，模型的参数数目（这里均不考虑偏置项 <code>b</code>）为：</p>
<ul>
<li><code>Linear(M-&gt;N)</code>：参数数目：<code>M×N</code></li>
<li><code>Conv2d(Cin, Cout, K)</code>：参数数目：<code>Cin × Cout × K × K</code></li>
<li><code>BatchNorm(N)</code>：参数数目：<code>2N</code></li>
<li><code>Embedding(N,W)</code>：参数数目： <code>N × W</code></li>
</ul>
<p><strong><code>参数占用显存 = 参数数目 × n</code></strong></p>
<ul>
<li><code>n = 4</code> ：<code>float32</code></li>
<li><code>n = 2</code> : <code>float16</code></li>
<li><code>n = 8</code> : <code>double64</code></li>
</ul>
<p>在 PyTorch 中，当执行完 <code>model=MyGreatModel().cuda()</code> 之后就会占用相应的显存，占用的显存大小基本与上述分析的显存差不多（会稍大一些，因为还有其它开销）。</p>
<h4 id="2-2-2-梯度与动量的显存占用"><a href="#2-2-2-梯度与动量的显存占用" class="headerlink" title="2.2.2 梯度与动量的显存占用"></a>2.2.2 梯度与动量的显存占用</h4><p>举例来说， 优化器如果是 SGD：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-aedfebda038f54bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看出来，除了保存 <code>w</code> 之外还要保存对应的梯度 <img src="https://upload-images.jianshu.io/upload_images/1351548-0ae20d4b59c5aa61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">。因此，<code>显存占用 = 参数占用的显存 x 2</code>。</p>
<p>如果是带 Momentum-SGD</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-53ae934e408eba20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这时候还需要保存动量， 因此，<code>显存 x 3</code>。如果是 Adam 优化器，动量占用的显存更多，<code>显存 x 4</code>。</p>
<p>总结一下，模型中<strong>与输入无关的显存占用</strong>包括：</p>
<ul>
<li>参数 <code>w</code></li>
<li>梯度 <code>dw</code>（一般与参数一样）</li>
<li>优化器的<strong>动量</strong>（普通 SGD 没有动量，momentum-SGD 动量与梯度一样，Adam 优化器动量的数量是梯度的两倍）</li>
</ul>
<h4 id="2-2-3-输入输出的显存占用"><a href="#2-2-3-输入输出的显存占用" class="headerlink" title="2.2.3 输入输出的显存占用"></a>2.2.3 输入输出的显存占用</h4><p>这部份的显存主要看输出的 feature map 的形状。</p>
<p><img src="https://pic2.zhimg.com/v2-f4d6bb11679be2251e12dad4f2cc56f9_b.jpg" alt="img"></p>
<p>比如卷积的输入输出满足以下关系：</p>
<p><img src="https://pic2.zhimg.com/v2-f05c13bb2fd8eecc0bd7b6e4d3d3148d_b.jpg" alt="img"></p>
<p>据此可以计算出每一层输出的 Tensor 的形状，然后就能计算出相应的显存占用。</p>
<p>模型输出的显存占用，总结如下：</p>
<ul>
<li>需要计算每一层的 feature map 的形状（多维数组的形状）</li>
<li>需要保存输出对应的梯度用以反向传播（链式法则）</li>
<li><strong>显存占用与 batch size 成正比</strong></li>
<li>模型输出不需要存储相应的动量信息。</li>
</ul>
<p>深度学习中神经网络的显存占用，可以得到如下公式：</p>
<pre><code class="python">显存占用 = 模型显存占用 + batch_size × 每个样本的显存占用
</code></pre>
<p>可以看出显存不是和 batch-size 简单的成正比，尤其是模型自身比较复杂的情况下：比如全连接很大，Embedding 层很大。</p>
<p>另外需要注意：</p>
<ul>
<li>输入（数据，图片）一般不需要计算梯度</li>
<li>神经网络的每一层输入输出都需要保存下来，用来反向传播，但是在某些特殊的情况下，我们可以不要保存输入。比如 ReLU，在 PyTorch 中，使用 <code>nn.ReLU(inplace = True)</code> 能将激活函数 ReLU 的输出直接覆盖保存于模型的输入之中，节省不少显存（<code>y=relu(x) -&gt; dx = dy.copy(); dx[y&lt;=0]=0</code>）。</li>
</ul>
<h3 id="2-3-节约显存的方法"><a href="#2-3-节约显存的方法" class="headerlink" title="2.3 节约显存的方法"></a>2.3 节约显存的方法</h3><p>在深度学习中，一般占用显存最多的是卷积等层的输出，模型参数占用的显存相对较少，而且不太好优化。</p>
<p>节省显存一般有如下方法：</p>
<ul>
<li>降低 batch-size</li>
<li>下采样（<code>NCHW -&gt; (1/4)*NCHW</code>）</li>
<li>减少全连接层（一般只留最后一层分类用的全连接层）</li>
</ul>
<h2 id="3-计算量分析"><a href="#3-计算量分析" class="headerlink" title="3. 计算量分析"></a>3. 计算量分析</h2><p>计算量越大，操作越费时，运行神经网络花费的时间越多。</p>
<h3 id="3-1-常用操作的计算量"><a href="#3-1-常用操作的计算量" class="headerlink" title="3.1 常用操作的计算量"></a>3.1 常用操作的计算量</h3><p>常用的操作计算量如下：</p>
<ul>
<li><p>全连接层：<code>BxMxN</code>，<code>B</code> 是 batch size，<code>M</code> 是输入形状，<code>N</code> 是输出形状。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-9078368da8db194d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
</li>
<li><p>卷积的计算量: <img src="https://www.zhihu.com/equation?tex=BHWC_%7Bout%7DC_%7Bin%7DK%5E2" alt="BHWC_{out}C_{in}K^2"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-385a1a730b1c85a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
</li>
<li><p>BatchNorm：计算量估算大概是 <img src="https://www.zhihu.com/equation?tex=BHWC%5Ctimes+%5C%7B4%2C5%2C6%5C%7D" alt="BHWC\times \{4,5,6\}"> </p>
</li>
<li><p>池化的计算量： <img src="https://www.zhihu.com/equation?tex=BHWCK%5E2" alt="BHWCK^2"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-849d0f53914c6b21.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
</li>
<li><p>ReLU 的计算量：<code>BHWC</code></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-16065f967f5e876f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
</li>
</ul>
<h3 id="3-2-AlexNet-的显存占用"><a href="#3-2-AlexNet-的显存占用" class="headerlink" title="3.2 AlexNet 的显存占用"></a>3.2 AlexNet 的显存占用</h3><p>AlexNet 的分析如下图，左边是每一层的参数数目（不是显存占用），右边是消耗的计算资源. 这里某些地方的计算结果可能和上面的公式对不上，这是因为原始的 AlexNet 实现有点特殊(在多块 GPU 上实现的)。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-e2eaace7df97fd53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看出：</p>
<ul>
<li>全连接层占据了绝大多数的参数</li>
<li>卷积层的计算量最大</li>
</ul>
<h3 id="3-3-减少卷积层的计算量"><a href="#3-3-减少卷积层的计算量" class="headerlink" title="3.3 减少卷积层的计算量"></a>3.3 减少卷积层的计算量</h3><p>谷歌提出的 MobileNet 利用了一种被称为 DepthWise Convolution 的技术，将神经网络运行速度提升许多，它的核心思想就是：把一个卷积操作拆分成两个相对简单的操作的组合。如图所示, 左边是原始卷积操作，右边是两个特殊而又简单的卷积操作的组合（上面类似于池化的操作，但是有权重，下面类似于全连接操作）。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-5978f3edac9e763d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这种操作使得：</p>
<ul>
<li>显存占用变多(每一步的输出都要保存)</li>
<li>计算量变少了许多，变成原来的（ <img src="https://www.zhihu.com/equation?tex=%7B1%5Cover+C_%7Bout%7D+%7D+%2B+%5Cfrac+1+%7Bk%5E2%7D" alt="{1\over C_{out} } + \frac 1 {k^2}"> ）（一般为原来的 <code>10－15%</code>）</li>
</ul>
<h3 id="3-4-常用模型的比较（显存-计算复杂度-准确率）"><a href="#3-4-常用模型的比较（显存-计算复杂度-准确率）" class="headerlink" title="3.4 常用模型的比较（显存/计算复杂度/准确率）"></a>3.4 常用模型的比较（显存/计算复杂度/准确率）</h3><p>论文《<a href="https://arxiv.org/abs/1605.07678" target="_blank" rel="noopener"><em>AN ANALYSIS OF DEEP NEURAL NETWORK MODELS<br>FOR PRACTICAL APPLICATIONS</em></a>》总结了当时常用模型的各项指标，横座标是计算复杂度（越往右越慢，越耗时），纵座标是准确率（越高越好），圆的面积是参数数量（不是显存占用），参数量越多，保存的模型文件越大。左上角我画了一个红色小圆，那是最理想的模型：快，准确率高，显存占用小。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-aac0ed9f47c94a9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><h3 id="4-1-建议"><a href="#4-1-建议" class="headerlink" title="4.1 建议"></a>4.1 建议</h3><ul>
<li>时间更宝贵，尽可能使模型变快（减少 <code>flop</code>）</li>
<li>显存占用不是和 batch size 简单成正比，模型自身的参数及其延伸出来的数据也要占据显存</li>
<li>batch size 越大，速度未必越快。在你充分利用计算资源的时候，加大 batch size 在速度上的提升很有限 </li>
</ul>
<p>尤其是 batch-size，假定 GPU 处理单元已经充分利用的情况下：</p>
<ul>
<li>增大 batch size 能增大速度，但是很有限（主要是并行计算的优化）</li>
<li>增大 batch size 能减缓梯度震荡，需要更少的迭代优化次数，收敛的更快，但是每次迭代耗时更长。</li>
<li>增大 batch size 使得一个 epoch 所能进行的优化次数变少，收敛可能变慢，从而需要更多时间才能收敛（比如 batch_size 变成全部样本数目）。</li>
</ul>
<h3 id="4-2-显卡的选购"><a href="#4-2-显卡的选购" class="headerlink" title="4.2 显卡的选购"></a>4.2 显卡的选购</h3><p>市面上常用的显卡指标如下（Base Core，不考虑 TensorCore 和 Boost 等)）：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1351548-6b30d43ffbfa2ea3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>显然 <strong>GTX 1080TI</strong> 性价比最高，速度超越 Titan X，价格却便宜很多，显存也只少了 <code>1G</code>。</p>
<ul>
<li>K80 性价比很低（速度慢，贵）</li>
<li>注意 GTX TITAN X 和 Nvidia TITAN X 的区别</li>
<li>TensorCore 的性能目前来看还无法全面发挥出来，这里不考虑。其它的 tesla 系列像 P100 这些企业级的显卡普通消费者不会买，而且性价比较低</li>
</ul>

        
    

    
</div>


                

                <!-- Post Comments -->
                
                    
                
            </div>

            <!-- Post Prev & Next Nav -->
            <nav class="material-nav mdl-color-text--grey-50 mdl-cell mdl-cell--12-col">
    <!-- Prev Nav -->
    
        <a href="/2018/11/17/KCF/" id="post_nav-newer" class="prev-content">
            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation">
                <i class="material-icons">arrow_back</i>
            </button>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            新篇
        </a>
    

    <!-- Section Spacer -->
    <div class="section-spacer"></div>

    <!-- Next Nav -->
    
        <a href="/2018/11/05/ORB/" id="post_nav-older" class="next-content">
            旧篇
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation">
                <i class="material-icons">arrow_forward</i>
            </button>
        </a>
    
</nav>

        </div>
    </div>



                    
                        <!-- Overlay For Active Sidebar -->
<div class="sidebar-overlay"></div>

<!-- Material sidebar -->
<aside id="sidebar" class="sidebar sidebar-colored sidebar-fixed-left" role="navigation">
    <div id="sidebar-main">
        <!-- Sidebar Header -->
        <div class="sidebar-header header-cover" style="background-image: url(/img/sidebar_header.png);">
    <!-- Top bar -->
    <div class="top-bar"></div>

    <!-- Sidebar toggle button -->
    <button type="button" class="sidebar-toggle mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon" style="display: initial;" data-upgraded=",MaterialButton,MaterialRipple">
        <i class="material-icons">clear_all</i>
        <span class="mdl-button__ripple-container">
            <span class="mdl-ripple">
            </span>
        </span>
    </button>

    <!-- Sidebar Avatar -->
    <div class="sidebar-image">
        <img src="/img/avatar.png" alt="Magicmanoooo's avatar">
    </div>

    <!-- Sidebar Email -->
    <a data-toggle="dropdown" class="sidebar-brand" href="#settings-dropdown">
        838713968@qq.com
        <b class="caret"></b>
    </a>
</div>


        <!-- Sidebar Navigation  -->
        <ul class="nav sidebar-nav">
    <!-- User dropdown  -->
    <li class="dropdown">
        <ul id="settings-dropdown" class="dropdown-menu">
            
                <li>
                    <a href="mailto: 838713968@qq.com" target="_blank" title="Email Me">
                        
                            <i class="material-icons sidebar-material-icons sidebar-indent-left1pc-element">email</i>
                        
                        Email Me
                    </a>
                </li>
            
        </ul>
    </li>

    <!-- Homepage -->
    
        <li id="sidebar-first-li">
            <a href="/">
                
                    <i class="material-icons sidebar-material-icons">home</i>
                
                主页
            </a>
        </li>
        
    

    <!-- Archives  -->
    
        <li class="dropdown">
            <a href="#" class="ripple-effect dropdown-toggle" data-toggle="dropdown">
                
                    <i class="material-icons sidebar-material-icons">inbox</i>
                
                    归档
                <b class="caret"></b>
            </a>
            <ul class="dropdown-menu">
            <li>
                <a class="sidebar_archives-link" href="/archives/2018/11/">十一月 2018<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/10/">十月 2018<span class="sidebar_archives-count">5</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/09/">九月 2018<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/08/">八月 2018<span class="sidebar_archives-count">4</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/07/">七月 2018<span class="sidebar_archives-count">5</span></a>
            </ul>
        </li>
        
    

    <!-- Categories  -->
    
        <li class="dropdown">
            <a href="#" class="ripple-effect dropdown-toggle" data-toggle="dropdown">
                
                    <i class="material-icons sidebar-material-icons">chrome_reader_mode</i>
                
                分类
                <b class="caret"></b>
            </a>
            <ul class="dropdown-menu">
                <li>
                
            </ul>
        </li>
        
    

    <!-- Pages  -->
    

    <!-- Article Number  -->
    
        <li>
            <a href="/archives">
                文章总数
                <span class="sidebar-badge">20</span>
            </a>
        </li>
        
    
</ul>


        <!-- Sidebar Footer -->
        <!--
I'm glad you use this theme, the development is no so easy, I hope you can keep the copyright, I will thank you so much.
If you still want to delete the copyrights, could you still retain the first one? Which namely "Theme Material"
It will not impact the appearance and can give developers a lot of support :)

很高兴您使用并喜欢该主题，开发不易 十分谢谢与希望您可以保留一下版权声明。
如果您仍然想删除的话 能否只保留第一项呢？即 "Theme Material"
它不会影响美观并可以给开发者很大的支持和动力。 :)
-->

<!-- Sidebar Divider -->

    <div class="sidebar-divider"></div>


<!-- Theme Material -->


<!-- Help & Support -->
<!--

    <a href="mailto:hiviosey@gmail.com" class="sidebar-footer-text-a">
        <div class="sidebar-text mdl-button mdl-js-button mdl-js-ripple-effect sidebar-footer-text-div" data-upgraded=",MaterialButton,MaterialRipple">
        sidebar.help
        <span class="mdl-button__ripple-container">
          <span class="mdl-ripple"></span>
        </span>
      </div>
    </a>

-->

<!-- Feedback -->
<!--

    <a href="https://github.com/viosey/hexo-theme-material/issues" target="_blank" class="sidebar-footer-text-a">
        <div class="sidebar-text mdl-button mdl-js-button mdl-js-ripple-effect sidebar-footer-text-div" data-upgraded=",MaterialButton,MaterialRipple">
        sidebar.feedback
        <span class="mdl-button__ripple-container"><span class="mdl-ripple"></span></span></div>
    </a>

-->

<!-- About Theme -->
<!--

    <a href="https://blog.viosey.com/index.php/Material.html" target="_blank" class="sidebar-footer-text-a">
        <div class="sidebar-text mdl-button mdl-js-button mdl-js-ripple-effect sidebar-footer-text-div" data-upgraded=",MaterialButton,MaterialRipple">
             sidebar.about_theme
            <span class="mdl-button__ripple-container"><span class="mdl-ripple"></span></span></div>
    </a>

-->

    </div>

    <!-- Sidebar Image -->
    

</aside>

                    

                    
                        <!-- Footer Top Button -->
                        <div id="back-to-top" class="toTop-wrap">
    <a href="#top" class="toTop">
        <i class="material-icons footer_top-i">expand_less</i>
    </a>
</div>

                    

                    <!--Footer-->
<footer class="mdl-mini-footer" id="bottom">
    
        <!-- Paradox Footer Left Section -->
        <div class="mdl-mini-footer--left-section sns-list">
    <!-- Twitter -->
    

    <!-- Facebook -->
    

    <!-- Google + -->
    

    <!-- Weibo -->
    
        <a href="https://weibo.com/5345088988/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo&amp;is_all=1" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-weibo">
                <span class="visuallyhidden">Weibo</span>
            </button><!--
     --></a>
    

    <!-- Instagram -->
    

    <!-- Tumblr -->
    

    <!-- Github -->
    
        <a href="https://github.com/Azurery" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-github">
                <span class="visuallyhidden">Github</span>
            </button><!--
     --></a>
    

    <!-- LinkedIn -->
    

    <!-- Zhihu -->
    
        <a href="https://www.zhihu.com/people/zhang-tao-60-41/activities" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-zhihu">
                <span class="visuallyhidden">Zhihu</span>
            </button><!--
     --></a>
    

    <!-- Bilibili -->
    
        <a href="https://space.bilibili.com/94222521/#/" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-bilibili">
                <span class="visuallyhidden">Bilibili</span>
            </button><!--
     --></a>
    

    <!-- Telegram -->
    
    
    <!-- V2EX -->
    
</div>


        <!--Copyright-->
        <div id="copyright">
            Copyright&nbsp;©&nbsp;2017&nbsp;-<script type="text/javascript">var fd = new Date();document.write("&nbsp;" + fd.getFullYear() + "&nbsp;");</script>Azurery
            
                <br>
                
                    只有永不遏止的奋斗，才能使青春之花，即使是凋谢，也是壮丽地凋谢
                
            
        </div>

        <!-- Paradox Footer Right Section -->

        <!--
        I am glad you use this theme, the development is no so easy, I hope you can keep the copyright.
        It will not impact the appearance and can give developers a lot of support :)

        很高兴您使用该主题，开发不易，希望您可以保留一下版权声明。
        它不会影响美观并可以给开发者很大的支持。 :)
        -->

        <div class="mdl-mini-footer--right-section">
            <div>
                <div class="footer-develop-div">Powered by <a href="https://hexo.io" target="_blank" class="footer-develop-a">Hexo</a></div>
                <div class="footer-develop-div">Theme - <a href="https://github.com/viosey/hexo-theme-material" target="_blank" class="footer-develop-a">Material</a></div>
            </div>
        </div>
    
</footer>


                    <!-- Import JS File -->

    <script>lsloader.load("lazyload_js","/js/lazyload.min.js?1BcfzuNXqV+ntF6gq+5X3Q==", true)</script>



    <script>lsloader.load("js_js","/js/js.min.js?V/53wGualMuiPM3xoetD5Q==", true)</script>



    <script>lsloader.load("np_js","/js/nprogress.js?pl3Qhb9lvqR1FlyLUna1Yw==", true)</script>


<script type="text/ls-javascript" id="NProgress-script">
    NProgress.configure({
        showSpinner: true
    });
    NProgress.start();
    $('#nprogress .bar').css({
        'background': '#29d'
    });
    $('#nprogress .peg').css({
        'box-shadow': '0 0 10px #29d, 0 0 15px #29d'
    });
    $('#nprogress .spinner-icon').css({
        'border-top-color': '#29d',
        'border-left-color': '#29d'
    });
    setTimeout(function() {
        NProgress.done();
        $('.fade').removeClass('out');
    }, 800);
</script>













<!-- UC Browser Compatible -->
<script>
	var agent = navigator.userAgent.toLowerCase();
	if(agent.indexOf('ucbrowser')>0) {
		document.write('<link rel="stylesheet" href="/css/uc.css">');
	   alert('由于 UC 浏览器使用极旧的内核，而本网站使用了一些新的特性。\n为了您能更好的浏览，推荐使用 Chrome 或 Firefox 浏览器。');
	}
</script>

<!-- Import prettify js  -->

    
        
            <script>lsloader.load("prettify_js","/js/prettify.min.js?WN07fivHQSMKWy7BmHBB6w==", true)</script>
        
    



<!-- Window Load -->
<!-- add class for prettify -->
<script type="text/ls-javascript" id="window-load">
    $(window).on('load', function() {
        // Post_Toc parent position fixed
        $('.post-toc-wrap').parent('.mdl-menu__container').css('position', 'fixed');
    });

    
        
            $(function() {
                $('pre').addClass('prettyprint linenums').attr('style', 'overflow:auto;');
                prettyPrint();
                })
        
    
    
</script>

<!-- MathJax Load-->


<!-- Bing Background -->


<script type="text/ls-javascript" id="lazy-load">
    // Offer LazyLoad
    queue.offer(function(){
        $('.lazy').lazyload({
            effect : 'show'
        });
    });

    // Start Queue
    $(document).ready(function(){
        setInterval(function(){
            queue.execNext();
        },200);
    });
</script>

<!-- Custom Footer -->



<script>
    (function(){
        var scriptList = document.querySelectorAll('script[type="text/ls-javascript"]')

        for (var i = 0; i < scriptList.length; ++i) {
            var item = scriptList[i];
            lsloader.runInlineScript(item.id,item.id);
        }
    })()
console.log('\n %c © Material Theme | Version: 1.5.0 | https://github.com/viosey/hexo-theme-material %c \n', 'color:#455a64;background:#e0e0e0;padding:5px 0;border-top-left-radius:5px;border-bottom-left-radius:5px;', 'color:#455a64;background:#e0e0e0;padding:5px 0;border-top-right-radius:5px;border-bottom-right-radius:5px;');
</script>

                </main>
            </div>
        </body>
    
</html>
